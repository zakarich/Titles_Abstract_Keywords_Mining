"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"ASR Normalization for Machine Translation","H. Huang; C. Feng; J. Wang; X. Zhang","Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol., Beijing, China; Res. Center of Comput. & Language Inf. Eng., CAS, Beijing, China; Res. Center of Comput. & Language Inf. Eng., CAS, Beijing, China","2010 Second International Conference on Intelligent Human-Machine Systems and Cybernetics","30 Sep 2010","2010","2","","91","94","In natural spoken language there are many meaningless modal particles and dittographes, furthermore ASR (automatic speech recognition) often has some recognition errors and the ASR results have no punctuations. Therefore, the translation would be rather poor if the ASR results are directly translated by MT (machine translation). In this paper, an ASR normalization approach was introduced for machine translation which based on maximum entropy sequential labeling model. Before translation, the meaningless modal particles and dittograph were deleted, and the recognition errors were corrected, and ASR results were also punctuated. Experiments show that the MT BLEU of 0.2465 is obtained, that improved by 17.3% over the MT baseline without normalization. The positive experimental results confirm that ASR normalization is effective for improvement of translation quality for spoken language machine translation.","","978-1-4244-7869-9","10.1109/IHMSC.2010.122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590723","Spoken language;machine translation;automatic speech recognition;maximum entropy model;normalization","Speech recognition;Entropy;Labeling;Computational modeling;Decoding;Acoustics;Natural language processing","language translation;maximum entropy methods;speech recognition","ASR normalization approach;automatic speech recognition;maximum entropy sequential labeling model;translation quality;spoken language machine translation","","","1","13","","30 Sep 2010","","","IEEE","IEEE Conferences"
"Using Example-Based Machine Translation Method For Automatic Image Annotation","Linsen Yu; Yongmei Liu; Tianwen Zhang","School of computer science and technology, Harbin Institute of Technology, Harbin, Heilongjiang province, China; Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol.; Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol.","2006 6th World Congress on Intelligent Control and Automation","23 Oct 2006","2006","2","","9809","9812","The paper proposes that the image annotation task can be thought of as similar to the machine translation problem and apply the example-based machine translation method to this problem. The method is based on the idea of performing automatic annotation by imitating annotation examples of images with similar visual scene. It can make full use of both correlation of annotation words and context of image regions in same image. Given an input image, the most visual similar images are retrieved from the annotated images. The annotation words of the retrieved images can be used as the annotation of the input image. From this view, we can say traditional techniques of content-based image retrieval (CBIR) are more apt to the task of automatic image annotation. As an example-based machine translation method, the judgment of visual similarity between images plays an import role. Earth mover's distance (EMD) is chosen as similarity measure for visual features. In order to make the EMD favor the similar regions between images, an enhanced EMD is presented. The approach does not rely on clustering and consequently does not suffer from the granularity issues. Experiment results show that the proposed mechanism outperforms the state-of-the-art techniques in annotating a large image collection using the same data set and same feature representations","","1-4244-0332-4","10.1109/WCICA.2006.1713911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1713911","image annotation;example-based machine translation;image retrieval","Image retrieval;Paper technology;Information retrieval;Computer science;Content based retrieval;Earth;Humans;Training data;Layout;Feature extraction","content-based retrieval;feature extraction;image matching;image retrieval;image segmentation;language translation;learning by example","example-based machine translation;automatic image annotation;image regions;content-based image retrieval;image visual similarity;Earth mover distance;similarity measure;visual features","","2","","11","","23 Oct 2006","","","IEEE","IEEE Conferences"
"Classification techniques for automatic speech recognition (ASR) algorithms used with real time speech translation","H. H. O. Nasereddin; A. A. R. Omari","Faculty of Information Technology, Middle East University (MEU), Amman, Jordan; iHorizons, Middle East University (MEU), Amman, Jordan","2017 Computing Conference","11 Jan 2018","2017","","","200","207","Speech processing is considered to be one of the most important application area of digital signal processing. Speech recognition and translation systems have consisted into two main systems, the first system represents an ASR system that contains two levels which are level one the feature extraction level As well as, level two the classification technique level using Data Time Wrapping (DTW), Hidden Markov Model (HMM), and Dynamic Bayesian Network (DBN). The second system is the Machine Translation (MT) system that mainly can be achieved by using three approaches which are (A) the statistical-based approach, (B) rule-approach, and (C) hybrid-based approach. In this study, we made a comparative study between classification techniques from ASR point of view, as well as, the translation approaches from MT point of view. The recognition rate was used in the ASR level and the error rate was used to evaluate the accuracy of the translated sentences. Furthermore, we classified the sample text audio files into four categories which were news, conversational, scientific phrases, and control categories.","","978-1-5090-5443-5","10.1109/SAI.2017.8252104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8252104","AF (Acoustic Feature);AM (Acoustic Model);ANN (Artificial Neural Network);ASR (Automatic Speech Recognition);BN (Bayesian Network);CNN (Convolutional Neural Networks);CVC (Consonant-Vowel-Consonant);DAG (Direct Acyclic Graph);DBN (Dynamic Bayesian Networks);DCT (Discrete Cosine Transformation);DNN (Deep Neural Network);DTW (Dynamic Time wrapping);HMM (Hidden Markov Model);JPD (Joint Probability Distribution);LM (Language Model);LPCC (Linear Predictive Cepstral Coefficients);MFCC (Mel Frequency Cepstral Coefficients);MT (Machine Translation);PD (Punctuation Dictionary);PLP (Perceptual Linear Prediction Coefficient);QV (Quantization Vector);RV (Random Variables);SLT (Spoken Language Translation);SVM (Support Vector Machine);WER (Word Error Rate);WRR (Word Recognition Rate)","Hidden Markov models;Speech;Bayes methods;Time series analysis;Speech recognition;Acoustics;Wrapping","Bayes methods;belief networks;feature extraction;hidden Markov models;language translation;natural language processing;pattern classification;speech processing;speech recognition","automatic speech recognition algorithms;speech processing;digital signal processing;ASR system;Data Time Wrapping;Hidden Markov Model;Dynamic Bayesian Network;Machine Translation system;statistical-based approach;translation approaches;recognition rate;ASR level;translated sentences;feature extraction level;classification technique level;rule-approach;real time speech translation","","2","","43","","11 Jan 2018","","","IEEE","IEEE Conferences"
"Syntax Rectification of Speech Using Neural Machine Translation","J. Singh; S. Pote; A. Karhe; A. Agrawal","Department of Computer Science, CDGI, Indore, India; Data Scientist, Big Data Infomatics, Indore, India; Department of Computer Science, CDGI, Indore, India; Department of Computer Science, CDGI, Indore, India","2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on","28 Feb 2019","2018","","","306","311","Speech Recognition is the identification of speech and conversion of the audio signals into textual data. The process of Speech Recognition has been accelerated with advancements in computing and progress in the development of Deep Learning. The mathematics behind Speech Recognition stems from Hidden Markov Models and has evolved into its more advanced form - End to End Speech Recognition. The aim of this paper is to focus on the syntax correction of speech in English language which will make use of Sequence to Sequence Models. The paper will delineate three major domains of grammatical correction of speech which will be achieved through real time interaction with the user.","","978-1-5386-1442-6","10.1109/I-SMAC.2018.8653676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653676","Automatic Speech Recognition;Neural Machine Translation;Sequence to Sequence;Deep Recurrent Neural Networks","Hidden Markov models;Speech recognition;Recurrent neural networks;Training;Conferences;Mathematical model;Probability distribution","audio signal processing;hidden Markov models;language translation;learning (artificial intelligence);neural nets;speech recognition","speech syntax rectification;neural machine translation;audio signal conversion;deep learning;hidden Markov models;end to end speech recognition;English language;sequence to sequence models;speech grammatical correction","","","","25","","28 Feb 2019","","","IEEE","IEEE Conferences"
"A Study of Myanmar (Burmese) to English Machine Translation Performance with Various Myanmar Translated Styles","H. Htun; Y. K. Thu; N. N. Oo; T. Supnithi","Yangon Technological University,Department of Computer Engineering and Information Technology,Yangon,Myanmar; National Electronics and Computer Technology Center (NECTEC),Pathum Thani,Thailand; Yangon Technological University,Department of Computer Engineering and Information Technology,Yangon,Myanmar; National Electronics and Computer Technology Center (NECTEC),Language and Semantic Technology Research Team(LST),Pathum Thani,Thailand","2020 IEEE Conference on Computer Applications(ICCA)","5 Mar 2020","2020","","","1","7","This paper contributes the first investigation of machine translation (MT) performance differences between Myanmar and English languages with the use of several possible Myanmar translations for the specific primary educational domain. We also developed both one to one and many Myanmar translations corpora (over 8K and 34K sentences) based on old and new English textbooks (including Grade 1 to 3) which are published by the Ministry of Education. Our developing parallel corpora were used for phrase-based statistical machine translation (PBSMT) which is the de facto standard of statistical machine translation. We measured machine translation performance differences among one-to-many English to Myanmar translation corpora. The differences range between 19.68 and 52.38 BLEU scores for English to Myanmar and between 50.17 and 75.12 BLEU scores for Myanmar to English translation. We expect this study can be applied in Myanmar-to-English automatic speech recognition (ASR) development for primary English textbooks. The main purpose is to translate primary English textbooks data correctly even if the children use in several Myanmar conversation styles.","","978-1-7281-5925-6","10.1109/ICCA49400.2020.9022851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022851","Phrase-based Statistical Machine Translation (PBSMT);One to Many Parallel Corpus;Myanmar-English Machine Translation;Primary English Textbooks of Myanmar;Word Error Rate (WER)","Training;Mathematical model;Buildings;Testing;Dictionaries;Information technology","computer aided instruction;language translation;natural language processing;speech recognition","English machine translation performance;machine translation performance differences;English languages;Myanmar translations corpora;phrase-based statistical machine translation;Myanmar translation corpora;English translation;Myanmar-to-English automatic speech recognition development;primary English textbooks data;Myanmar conversation styles;primary educational domain;parallel corpora","","","","23","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Integration of Statistical Models for Dictation of Document Translations in a Machine-Aided Human Translation Task","A. Reddy; R. C. Rose","Dept. of Electr. & Comput. Eng., McGill Univ., Montréal, QC, Canada; Dept. of Electr. & Comput. Eng., McGill Univ., Montréal, QC, Canada","IEEE Transactions on Audio, Speech, and Language Processing","7 Sep 2010","2010","18","8","2015","2027","This paper presents a model for machine-aided human translation (MAHT) that integrates source language text and target language acoustic information to produce the text translation of source language document. It is evaluated on a scenario where a human translator dictates a first draft target language translation of a source language document. Information obtained from the source language document, including translation probabilities derived from statistical machine translation (SMT) and named entity tags derived from named entity recognition (NER), is incorporated with acoustic phonetic information obtained from an automatic speech recognition (ASR) system. One advantage of the system combination used here is that words that are not included in the ASR vocabulary can be correctly decoded by the combined system. The MAHT model and system implementation is presented. It is shown that a relative decrease in word error rate of 29% can be obtained by this combined system relative to the baseline ASR performance on a French to English document translation task in the Hansard domain. In addition, it is shown that transcriptions obtained by using the combined system show a relative increase in NIST score of 34% compared to transcriptions obtained from the baseline ASR system.","1558-7924","","10.1109/TASL.2010.2040793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5393062","Machine-aided human translation (MAHT);machine translation;named entity recognition;speech recognition","Humans;Automatic speech recognition;Natural languages;Surface-mount technology;Decoding;Error analysis;Speech recognition;Acoustic noise;Probability;Vocabulary","language translation;speech recognition;statistical analysis;text analysis","statistical model integration;document translations;machine-aided human translation task;source language text integration;target language acoustic information;text translation;source language document;statistical machine translation;translation probability;named entity tags;named entity recognition;acoustic phonetic information;automatic speech recognition system;ASR vocabulary;MAHT model;word error rate;French to English document translation task;Hansard domain","","13","1","36","","19 Jan 2010","","","IEEE","IEEE Journals"
"Integration of Speech Recognition and Machine Translation in Computer-Assisted Translation","S. Khadivi; H. Ney","Dept. of Comput. Sci., RWTH Aachen Univ., Aachen; Dept. of Comput. Sci., RWTH Aachen Univ., Aachen","IEEE Transactions on Audio, Speech, and Language Processing","21 Oct 2008","2008","16","8","1551","1564","Parallel integration of automatic speech recognition (ASR) models and statistical machine translation (MT) models is an unexplored research area in comparison to the large amount of works done on integrating them in series, i.e., speech-to-speech translation. Parallel integration of these models is possible when we have access to the speech of a target language text and to its corresponding source language text, like a computer-assisted translation system. To our knowledge, only a few methods for integrating ASR models with MT models in parallel have been studied. In this paper, we systematically study a number of different translation models in the context of the N-best list rescoring. As an alternative to the N -best list rescoring, we use ASR word graphs in order to arrive at a tighter integration of ASR and MT models. The experiments are carried out on two tasks: English-to-German with an ASR vocabulary size of 17 K words, and Spanish-to-English with an ASR vocabulary of 58 K words. For the best method, the MT models reduce the ASR word error rate by a relative of 18% and 29% on the 17 K and the 58 K tasks, respectively.","1558-7924","","10.1109/TASL.2008.2004301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4648933","Computer-assisted translation (CAT);speech recognition;statistical machine translation (MT)","Speech recognition;Automatic speech recognition;Natural languages;Vocabulary;Humans;Concurrent computing;Context modeling;Error analysis;Engines","computational linguistics;graph theory;integration;language translation;speech recognition","machine translation;computer-assisted translation;parallel integration;automatic speech recognition;statistical machine translation models;speech-to-speech translation;source language text;word graphs;English-to-German;Spanish-to-English","","16","","33","","21 Oct 2008","","","IEEE","IEEE Journals"
"Sequential system combination for machine translation of speech","D. Karakos; S. Khudanpur","Center for Language and Speech Processing and Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD 212, USA; Center for Language and Speech Processing and Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD 212, USA","2008 IEEE Spoken Language Technology Workshop","6 Feb 2009","2008","","","257","260","System combination is a technique which has been shown to yield significant gains in speech recognition and machine translation. Most combination schemes perform an alignment between different system outputs in order to produce lattices (or confusion networks), from which a composite hypothesis is chosen, possibly with the help of a large language model. The benefit of this approach is two-fold: (i) whenever many systems agree with each other on a set of words, the combination output contains these words with high confidence; and (ii) whenever the systems disagree, the language model resolves the ambiguity based on the (probably correct) agreed upon context. The case of machine translation system combination is more challenging because of the different word orders of the translations: the alignment has to incorporate computationally expensive movements of word blocks. In this paper, we show how one can combine translation outputs efficiently, extending the incremental alignment procedure of (A-V.I. Rosti et al., 2008). A comparison between different system combination design choices is performed on an Arabic speech translation task.","","978-1-4244-3471-8","10.1109/SLT.2008.4777889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4777889","Machine Translation;System Combination;Confusion Networks;Alignments with reordering","Speech processing;Speech recognition;Automatic speech recognition;Natural languages;Lattices;Context modeling;Voting;Computer numerical control;Contracts;Sequences","language translation;natural language processing;speech recognition","sequential system combination;speech machine translation;speech recognition;large language model;machine translation system combination;incremental alignment procedure;Arabic speech translation","","2","","17","","6 Feb 2009","","","IEEE","IEEE Conferences"
"Image Semantic Extraction Using Latent Semantic Indexing on Image Retrieval Automatic-Annotation","Y. Herdiyeni; S. Nurdiati; I. A. Daud","Dept. of Comput. Sci., Bogor Agric. Univ., Bogor, Indonesia; Dept. of Comput. Sci., Bogor Agric. Univ., Bogor, Indonesia; Dept. of Comput. Sci., Bogor Agric. Univ., Bogor, Indonesia","2009 International Conference of Soft Computing and Pattern Recognition","31 Dec 2009","2009","","","283","288","This research proposed a new method Latent Semantic Indexing to overcome semantic problem on image retrieval based on automatic image annotation. Statistical machine translation used to automatically annotates the image. This approach considers image annotation as the translation of image regions to words, similar to the translation of text from one language to another. The ""lexicon"" for the translation is learned from large annotated image collections, which consist of images that are associated with text. Images are segmented into regions with grid segmentation. A pre-specified feature vector represents each region. The regions then clustered into a finite set of blobs. The correspondences between the blobs and the words are learned using Expectation Maximization algorithm. These correspondences are used to predict words associated with whole images (auto-annotation). The auto-annotation performance is evaluated using Normalized Score (ENS) algorithm. The experimental results show that the average precision of clause queries achieved best result than word queries, 0.544 and 0.251 for clause queries and word queries respectively. The proposed method of latent semantic indexing succeeds to exploit semantic value of automatic-annotation-based image retrieval.","","978-1-4244-5330-6","10.1109/SoCPaR.2009.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5370355","Content-based image retrieval;automatic annotation;statistical machine translation;latent semantic indexing","Indexing;Image retrieval;Design optimization;Printing;Integer linear programming;Constraint optimization;Testing;Containers;Pattern recognition;Computer applications","content-based retrieval;expectation-maximisation algorithm;image retrieval;image segmentation;information retrieval;language translation","image semantic extraction;latent semantic indexing;image retrieval;automatic image annotation;statistical machine translation;grid segmentation;feature vector;expectation maximization algorithm;normalized score algorithm","","","","6","","31 Dec 2009","","","IEEE","IEEE Conferences"
"Gamayun - Language Technology for Humanitarian Response","A. Öktem; M. A. Jaam; E. DeLuca; G. Tang",Translators without Borders; Translators without Borders; Translators without Borders; Translators without Borders,"2020 IEEE Global Humanitarian Technology Conference (GHTC)","8 Feb 2021","2020","","","1","4","Over half of the world's population do not have access to knowledge and information because it's not available in their language. Translators without Borders (TWB) wants to change this with Gamayun, an initiative to promote language equality. Gamayun uses advanced language technologies to improve communication with people who speak marginalized languages in humanitarian and development contexts. In this paper, we present the early implementation results of the project in building machine translation and automatic speech recognition systems for various marginalized languages.","2377-6919","978-1-7281-7388-7","10.1109/GHTC46280.2020.9342939","Microsoft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9342939","language technology;machine translation;automatic speech recognition;low-resource languages","Sociology;Tools;Machine translation;Task analysis;Statistics;Open source software;Automatic speech recognition","language translation;natural language processing;speech recognition","humanitarian response;language equality;language technologies;marginalized languages;humanitarian development contexts;machine translation;Gamayun;Translators without Borders;automatic speech recognition systems","","","","18","","8 Feb 2021","","","IEEE","IEEE Conferences"
"On statistical machine translation method for lexicon refinement in speech recognition","H. Xu; X. Xiao; E. Chng; H. Li","Temasek Laboratories, Nanyang Technological University, Singapore; Temasek Laboratories, Nanyang Technological University, Singapore; Temasek Laboratories, Nanyang Technological University, Singapore; Temasek Laboratories, Nanyang Technological University, Singapore","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","3 Sep 2015","2015","","","25","29","In low resource Automatic Speech Recognition (ASR), one usually resorts to the Statistical Machine Translation (SMT) technique to learn transform rules to refine grapheme lexicon. To do this, we face two challenges. One is to generate grapheme sequences from the training data as the targets, which is paired with the original transcripts to train SMT models; the other is to effectively prune the learned rules from the translation model. In this paper we further this study. First we propose a simple but effective pruning method; second, to see in which case we are able to learn better rules, different setups with various acoustic and language model combinations are investigated; finally, to examine if the rules in different setups are complementary, lexicons generated via different rule tables are merged in ASR experiments. We report a WER reduction of up to 6.2% with the proposed technique.","","978-1-4799-1948-2","10.1109/ChinaSIP.2015.7230355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230355","lexicon learning;grapheme lexicon;statistical machine translation;system fusion;automatic speech recognition","Hidden Markov models;Acoustics;Training data;Data models;Speech recognition;Speech;Accuracy","language translation;learning (artificial intelligence);speech recognition","statistical machine translation method;SMT;grapheme lexicon refinement;automatic speech recognition;ASR;pruning method","","","","20","","3 Sep 2015","","","IEEE","IEEE Conferences"
"Efficient integration of translation and speech models in dictation based machine aided human translation","L. Rodríguez; A. Reddy; R. Rose","Departamento de Sistemas Informáticos, University of Castilla La Mancha, Spain; Dept. of Electrical and Computer Engineering, McGill University, Montreal, Quebec, Canada; Dept. of Electrical and Computer Engineering, McGill University, Montreal, Quebec, Canada","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4949","4952","This paper is concerned with combining models for decoding an optimum translation for a dictation based machine aided human translation (MAHT) task. Statistical language model (SLM) probabilities in automatic speech recognition (ASR) are updated using statistical machine translation (SMT) model probabilities. The effect of this procedure is evaluated for utterances from human translators dictating translations of source language documents. It is shown that computational complexity is significantly reduced while at the same time word error rate is reduced by 30%.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289030","speech recognition;machine translation;speech input interfaces","Lattices;Decoding;Speech;Humans;Computational modeling;Probability;Vocabulary","language translation;speech processing;speech recognition","speech model;dictation based machine aided human translation;statistical language model probability;automatic speech recognition;statistical machine translation model probability;human translator;source language document;computational complexity;word error rate","","1","","13","","30 Aug 2012","","","IEEE","IEEE Conferences"
"An analysis of machine translation and speech synthesis in speech-to-speech translation system","K. Hashimoto; J. Yamagishi; W. Byrne; S. King; K. Tokuda","Nagoya Institute of Technology, Department of Computer Science and Engineering, Japan; University of Edinburgh, Centre for Speech Technology Research, United Kingdom; Cambridge University, Engineering Department, United Kingdom; University of Edinburgh, Centre for Speech Technology Research, United Kingdom; Nagoya Institute of Technology, Department of Computer Science and Engineering, Japan","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","5108","5111","This paper provides an analysis of the impacts of machine translation and speech synthesis on speech-to-speech translation systems. The speech-to-speech translation system consists of three components: speech recognition, machine translation and speech synthesis. Many techniques for integration of speech recognition and machine translation have been proposed. However, speech synthesis has not yet been considered. Therefore, in this paper, we focus on machine translation and speech synthesis, and report a subjective evaluation to analyze the impact of each component. The results of these analyses show that the naturalness and intelligibility of synthesized speech are strongly affected by the fluency of the translated sentences.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947506","speech synthesis;machine translation;speechto-speech translation;subjective evaluation","Speech;Speech synthesis;Correlation;Speech recognition;Natural languages;Hidden Markov models;Training data","speech recognition;speech synthesis","machine translation;speech synthesis;speech-to-speech translation system;speech recognition","","3","","21","","11 Jul 2011","","","IEEE","IEEE Conferences"
"WordNet Based Sign Language Machine Translation: from English Voice to ISL Gloss","K. Saija; S. Sangeetha; V. Shah","Radisys India Pvt. Ltd.,Bengaluru,India; National Institute of Technology,Trichy,India; Indian Institute of Technology,Guwahati,India","2019 IEEE 16th India Council International Conference (INDICON)","12 Mar 2020","2019","","","1","4","Communication is an essential part of human life. For the person with hearing and speaking disability, it is inconvenient to communicate with other people. In this paper, an end-to-end system to convert English voice to Indian Sign Language (ISL) gloss (written form of sign language) is proposed which will help deaf to communicate with others and vice versa. This system accepts English voice as an input and, converts it into the text using the speech recognition. From the recognized English text, ISL gloss is generated using the lexical database called WordNet. The focus of our work is to build a robust sign language machine translation system to convert the English text to ISL gloss using the linguistic database WordNet.","2325-9418","978-1-7281-2327-1","10.1109/INDICON47234.2019.9029074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9029074","Indian Sign Language;Sign language machine translation;WordNet;HMM;Natural Language Processing","Assistive technology;Gesture recognition;Dictionaries;Speech recognition;Hidden Markov models;Databases;Statistical analysis","handicapped aids;human computer interaction;language translation;natural language processing;sign language recognition;speech recognition;text analysis","ISL gloss;speaking disability;end-to-end system;Indian Sign Language gloss;linguistic database;English text recognition;English voice input;speech recognition;WordNet based sign language machine translation","","","","16","","12 Mar 2020","","","IEEE","IEEE Conferences"
"End-to-End Speech Translation With Transcoding by Multi-Task Learning for Distant Language Pairs","T. Kano; S. Sakti; S. Nakamura","Nara Institute of Science and Technology, Ikoma, Japan; Graduate School of Science and Technology, Nara Institute of Science and Technology, Ikoma, Japan; Data Science Center and Graduate School of Science and Technology, Nara Institute of Science and Technology, Ikoma, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","8 May 2020","2020","28","","1342","1355","Directly translating spoken utterances from a source language to a target language is challenging because it requires a fundamental transformation in both linguistic and para/non-linguistic features. Traditional speech-to-speech translation approaches concatenate automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech synthesizer (TTS) by text information. The current state-of-the-art models for ASR, MT, and TTS have mainly been built using deep neural networks, in particular, an attention-based encoder-decoder neural network with an attention mechanism. Recently, several works have constructed end-to-end direct speech-to-text translation by combining ASR and MT into a single model. However, the usefulness of these models has only been investigated on language pairs of similar syntax and word order (e.g., English-French or English-Spanish). For syntactically distant language pairs (e.g., English-Japanese), speech translation requires distant word reordering. Furthermore, parallel texts with corresponding speech utterances that are suitable for training end-to-end speech translation are generally unavailable. Collecting such corpora is usually time-consuming and expensive. This article proposes the first attempt to build an end-to-end direct speech-to-text translation system on syntactically distant language pairs that suffer from long-distance reordering. We train the model on English (subject-verb-object (SVO) word order) and Japanese (SOV word order) language pairs. To guide the attention-based encoder-decoder model on this difficult problem, we construct end-to-end speech translation with transcoding and utilize curriculum learning (CL) strategies that gradually train the network for end-to-end speech translation tasks by adapting the decoder or encoder parts. We use TTS for data augmentation to generate corresponding speech utterances from the existing parallel text data. Our experiment results show that the proposed approach provides significant improvements compared with conventional cascade models and the direct speech translation approach that uses a single model without transcoding and CL strategies.","2329-9304","","10.1109/TASLP.2020.2986886","JSPS KAKENHI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9072502","End-to-end speech-to-text translation;automatic speech recognition;machine translation (MT);multi-task learning","Task analysis;Decoding;Speech processing;Recurrent neural networks;Training;Adaptation models","computational linguistics;decoding;language translation;learning (artificial intelligence);natural language processing;neural nets;speech recognition;text analysis","source language;target language;ASR;text-to-text machine translation;MT;text-to-speech synthesizer;TTS;text information;attention-based encoder-decoder neural network;syntactically distant language pairs;speech utterances;training end-to-end speech translation;end-to-end direct speech-to-text translation system;Japanese language pairs;attention-based encoder-decoder model;direct speech translation approach;parallel text data;subject-verb-object word order;SOV word order;curriculum learning;data augmentation;speech translation","","2","","32","IEEE","20 Apr 2020","","","IEEE","IEEE Journals"
"Machine Translation of LATEX Based Mathematical Equations to Spoken Mathematics","M. M. Thinn; Y. K. Thu; H. M. Nwe; N. N. Yee; T. Myint; H. A. Thant; T. Supnithi","University of Technology (Yatanarpon Cyber City),Department of Information Science,Pyin Oo Lwin,Myanmar; National Electronic and Computer Technology Center (NECTEC),Language and Semantic Technology Research Team (LST),Pathum Thani,Thailand; University of Technology (Yatanarpon Cyber City),Department of Information Science,Pyin Oo Lwin,Myanmar; University of Technology (Yatanarpon Cyber City),Department of Information Science,Pyin Oo Lwin,Myanmar; University of Technology (Yatanarpon Cyber City),Department of Information Science,Pyin Oo Lwin,Myanmar; University of Technology (Yatanarpon Cyber City),Department of Information Science,Pyin Oo Lwin,Myanmar; National Electronic and Computer Technology Center (NECTEC),Language and Semantic Technology Research Team (LST),Pathum Thani,Thailand","2020 24th International Computer Science and Engineering Conference (ICSEC)","15 Mar 2021","2020","","","1","6","This paper describes the machine translation of LATEX encoded mathematical equations to spoken mathematical sentences. A LATEX- Spoken math parallel corpus (5,600 sentences) was developed. In this paper, the 10-fold cross-validation experiments were carried out by applying Phrase-based Statistical Machine Translation (PBSMT), Weighted Finite-State Transducers (WFST) and Ripple Down Rules (RDR) based tagging approaches. The BLEU, RIBES, F1 and WER evaluation scoring metrics are used for measuring translation performance. The experimental results show that the PBSMT approach achieved the highest translation performance for LATEX mathematical equations to spoken mathematical sentences translation. Moreover, we found that the translation performance of RDR approach is comparable with PBSMT.","","978-1-6654-0339-9","10.1109/ICSEC51790.2020.9375339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9375339","Phrase-based Statistical Machine Translation (PBSMT);Ripple Down Rules (RDR);Weighted Finite State Transducers (WFST)","Measurement;Computer science;Transducers;Error analysis;Tagging;Mathematics;Machine translation","language translation;natural language processing;speech recognition;statistical analysis","Spoken math parallel corpus;cross-validation experiments;applying Phrase-based Statistical Machine Translation;Weighted Finite-State Transducers;WER evaluation scoring metrics;PBSMT approach;highest translation performance;LATEX mathematical equations;spoken mathematical sentences translation;RDR approach;Spoken mathematics","","","","22","","15 Mar 2021","","","IEEE","IEEE Conferences"
"Automatic Pathology Annotation on Medical Images: A Statistical Machine Translation Framework","T. Gong; S. Li; C. L. Tan; B. C. Pang; C. C. T. Lim; C. K. Lee; Q. Tian; Z. Zhang","Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Nat. Neurosci. Inst., Tan Tock Seng Hosp., Singapore, Singapore; Nat. Neurosci. Inst., Tan Tock Seng Hosp., Singapore, Singapore; Nat. Neurosci. Inst., Tan Tock Seng Hosp., Singapore, Singapore; Inst. of Infocomm Res., Singapore, Singapore; Inst. of Infocomm Res., Singapore, Singapore","2010 20th International Conference on Pattern Recognition","7 Oct 2010","2010","","","2504","2507","Large number of medical images are produced daily in hospitals and medical institutions, the needs to efficiently process, index, search and retrieve these images are great. In this paper, we propose a pathology based medical image annotation framework using a statistical machine translation approach. After pathology terms and regions of interest (ROIs) are extracted from training text and images respectively, we use machine translation model IBM Model 1 to iteratively learn the alignment between the ROIs and the pathology terms and generate an ROI-to-pathology translation table. In testing phase, we annotate the ROI in the image with the pathology label of the highest probability in the translation table. The overall annotation results and the retrieval performance are promising to doctors and medical professionals.","1051-4651","978-1-4244-7541-4","10.1109/ICPR.2010.613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5595768","Computer aided detection and diagnosis;Multimedia analysis indexing and retrieval;Biomedical systems and applications","Biomedical imaging;Pathology;Training;Computed tomography;Image segmentation;Testing;Image retrieval","feature extraction;image retrieval;language translation;medical image processing;statistical analysis","statistical machine translation framework;medical institutions;image retrieval;pathology based medical image annotation framework;region of interest extraction;IBM model;ROI-to-pathology translation table","","3","","8","","7 Oct 2010","","","IEEE","IEEE Conferences"
"Transformer-based Machine Translation for Low-resourced Languages embedded with Language Identification","T. J. Sefara; S. G. Zwane; N. Gama; H. Sibisi; P. N. Senoamadi; V. Marivate","Council for Scientific and Industrial Research,Next Generation Enterprises and Institutions,Pretoria,South Africa; University of Zululand,Department of Computer Science,South Africa; University of the Witwatersrand,School of Computer Science and Applied Mathematics,South Africa; University of Johannesburg,Department of Computer Science,South Africa; University of Zululand,Department of Mathematics,South Africa; University of Pretoria,Department of Computer Science,South Africa","2021 Conference on Information Communications Technology and Society (ICTAS)","6 Apr 2021","2021","","","127","132","Recent research on the development of machine translation (MT) models has resulted in state-of-the-art performance for many resourced European languages. However, there has been a little focus on applying these MT services to low-resourced languages. This paper presents the development of neural machine translation (NMT) for low-resourced languages of South Africa. Two MT models, JoeyNMT and transformer NMT with self-attention are trained and evaluated using BLEU score. The transformer NMT with self-attention obtained state-of-the-art performance on isiNdebele, SiSwati, Setswana, Tshivenda, isiXhosa, and Sepedi while JoeyNMT performed well on isiZulu. The MT models are embedded with language identification (LID) model that presets the language for translation models. The LID models are trained using logistic regression and multinomial naive Bayes (MNB). MNB classifier obtained an accuracy of 99% outperforming logistic regression which obtained the lowest accuracy of 97%.","","978-1-7281-8081-6","10.1109/ICTAS50802.2021.9394996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9394996","machine translation;low-resourced languages;neural network;language identification","Buildings;Europe;Communications technology;Machine translation;Logistics","Bayes methods;language translation;learning (artificial intelligence);natural language processing;natural languages;pattern classification;regression analysis;speech recognition","transformer-based machine translation;low-resourced languages;machine translation models;resourced European languages;MT services;neural machine translation;MT models;transformer NMT;language identification model","","","","17","","6 Apr 2021","","","IEEE","IEEE Conferences"
"Rhonda: the architecture of a multilingual speech-to-speech translation pipeline","J. A. Louw; A. Moodley","Human Language Technologies Research Group, Meraka Institute, CSIR, Pretoria, South Africa; Human Language Technologies Research Group, Meraka Institute, CSIR, Pretoria, South Africa","2018 International Conference on Intelligent and Innovative Computing Applications (ICONIC)","6 Jan 2019","2018","","","1","7","Speech-to-speech translation can be described as converting a speech signal from a source language into a speech signal of the same meaning or intent into a target language. This process is achieved by the coordinated cooperation of individual Human Language Technology components, where the most important components to a speech translation system are automatic speech recognition, machine translation and text-to-speech. In this paper we present and discuss the design and architectural building blocks of the Rhonda speech-to-speech translation system, as well as their interactions with each other to facilitate speech-to-speech translation in a reliable, scalable and possibly distributed manner.","","978-1-5386-6477-3","10.1109/ICONIC.2018.8601204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8601204","speech-to-speech translation;automatic speech recognition;machine translation;text-to-speech;multilingual","Pipelines;Protocols;Computer architecture;Automatic speech recognition;Engines;Telecommunications","language translation;pipelines;speech recognition","speech-to-speech translation pipeline;speech signal;machine translation;architectural building;Rhonda speech-to-speech translation system;text-to-speech;automatic speech recognition;individual Human Language Technology components","","1","","36","","6 Jan 2019","","","IEEE","IEEE Conferences"
"Europarl-ST: A Multilingual Corpus for Speech Translation of Parliamentary Debates","J. Iranzo-Sánchez; J. A. Silvestre-Cerdà; J. Jorge; N. Roselló; A. Giménez; A. Sanchis; J. Civera; A. Juan","Universitat Politècnica de València,Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN),Spain; Universitat Politècnica de València,Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN),Spain; Universitat Politècnica de València,Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN),Spain; Universitat Politècnica de València,Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN),Spain; Universitat Politècnica de València,Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN),Spain; Universitat Politècnica de València,Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN),Spain; Universitat Politècnica de València,Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN),Spain; Universitat Politècnica de València,Machine Learning and Language Processing (MLLP) research group Valencian Research Institute for Artificial Intelligence (VRAIN),Spain","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","8229","8233","Current research into spoken language translation (SLT), or speech-to-text translation, is often hampered by the lack of specific data resources for this task, as currently available SLT datasets are restricted to a limited set of language pairs. In this paper we present Europarl-ST, a novel multilingual SLT corpus containing paired audio-text samples for SLT from and into 6 European languages, for a total of 30 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012. This paper describes the corpus creation process and presents a series of automatic speech recognition, machine translation and spoken language translation experiments that highlight the potential of this new resource. The corpus is released under a Creative Commons license and is freely accessible and downloadable.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054626","speech translation;spoken language translation;automatic speech recognition;machine translation;multilingual corpus","Training;Adaptation models;Filtering;Pipelines;Europe;Task analysis;Speech processing","natural language processing;speech recognition;text analysis","Europarl-ST;speech translation;parliamentary debates;speech-to-text translation;specific data resources;language pairs;audio-text samples;European languages;translation directions;European Parliament;corpus creation process;automatic speech recognition;machine translation;spoken language translation experiments;SLT datasets;multilingual SLT corpus","","1","","28","","9 Apr 2020","","","IEEE","IEEE Conferences"
"AwezaMed: A Multilingual, Multimodal Speech-To-Speech Translation Application for Maternal Health Care","L. Marais; J. A. Louw; J. Badenhorst; K. Calteaux; I. Wilken; N. van Niekerk; G. Stein","Aweza,Cape Town,South Africa; Aweza,Cape Town,South Africa; Aweza,Cape Town,South Africa; Aweza,Cape Town,South Africa; Aweza,Cape Town,South Africa; Aweza,Cape Town,South Africa; Aweza,Cape Town,South Africa","2020 IEEE 23rd International Conference on Information Fusion (FUSION)","10 Sep 2020","2020","","","1","8","The language contexts of multilingual developing countries such as South Africa are often characterised by communication challenges resulting from language differences. AwezaMed is a multilingual, multimodal speech-to-speech translation application for the health care domain, which was designed to assist in bridging communication barriers and mitigate the risks of miscommunication. The application focuses on the domain of maternal health care. It uses English as source language and Afrikaans, isiXhosa and isiZulu as target languages to enable health care providers to communicate with patients in their own language. It incorporates automatic speech recognition, machine translation and text-to-speech to deliver speech-to-speech translation functionality in a scalable way via a REST API to an Android mobile application. It is being piloted at various health care facilities across South Africa.","","978-0-578-64709-8","10.23919/FUSION45008.2020.9190240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190240","speech-to-speech translation;machine translation;automatic speech recognition;text-to-speech;mobile application","Reliability;Pipelines;Automatic speech recognition;Bridges;Cancer","Android (operating system);application program interfaces;developing countries;gender issues;health care;language translation;mobile computing;natural language processing;speech recognition","South Africa;language differences;AwezaMed;maternal health care;automatic speech recognition;text-to-speech;health care facilities;multilingual developing countries;multimodal speech-to-speech translation;multilingual speech-to-speech translation;English language;Afrikaans;isiXhosa;isiZulu;machine translation;REST API;Android mobile application","","","","17","","10 Sep 2020","","","IEEE","IEEE Conferences"
"Statistical Machine Translation for Speech: A Perspective on Structures, Learning, and Decoding","B. Zhou","IBM T. J. Watson Research Center, New York, NY, USA","Proceedings of the IEEE","17 Apr 2013","2013","101","5","1180","1202","In this paper, we survey and analyze state-of-the-art statistical machine translation (SMT) techniques for speech translation (ST). We review key learning problems, and investigate essential model structures in SMT, taking a unified perspective to reveal both connections and contrasts between automatic speech recognition (ASR) and SMT. We show that phrase-based SMT can be viewed as a sequence of finite-state transducer (FST) operations, similar in spirit to ASR. We further inspect the synchronous context-free grammar (SCFG)-based formalism that includes hierarchical phrase-based and many linguistically syntax-based models. Decoding for ASR, FST-based, and SCFG-based translation is also presented from a unified perspective as different realizations of the generic Viterbi algorithm on graphs or hypergraphs. These consolidated perspectives are helpful to catalyze tighter integrations for improved ST, and we discuss joint decoding and modeling toward coupling ASR and SMT.","1558-2256","","10.1109/JPROC.2013.2249491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497459","Discriminative training;finite-state transducer (FST);graph;hypergraph;speech translation (ST);statistical machine translation (SMT);synchronous context-free grammar (SCFG);Viterbi search","Speech processing;Information processing;Training;Statistical learning;Viterbi algorithm;Automata;Transducers;Context awareness","graph theory;language translation;learning (artificial intelligence);speech coding;speech recognition;statistical analysis;transducers;Viterbi decoding","statistical machine translation;structures;learning;decoding;SMT techniques;speech translation;automatic speech recognition;ASR;finite state transducer;FST;Viterbi algorithm;hypergraphs","","4","","100","","12 Apr 2013","","","IEEE","IEEE Journals"
"System Combination for Machine Translation of Spoken and Written Language","E. Matusov; G. Leusch; R. E. Banchs; N. Bertoldi; D. Dechelotte; M. Federico; M. Kolss; Y. -S. Lee; J. B. Marino; M. Paulik; S. Roukos; H. Schwenk; H. Ney","RWTH Aachen Univ., Aachen; RWTH Aachen Univ., Aachen; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Audio, Speech, and Language Processing","15 Aug 2008","2008","16","7","1222","1237","This paper describes an approach for computing a consensus translation from the outputs of multiple machine translation (MT) systems. The consensus translation is computed by weighted majority voting on a confusion network, similarly to the well-established ROVER approach of Fiscus for combining speech recognition hypotheses. To create the confusion network, pairwise word alignments of the original MT hypotheses are learned using an enhanced statistical alignment algorithm that explicitly models word reordering. The context of a whole corpus of automatic translations rather than a single sentence is taken into account in order to achieve high alignment quality. The confusion network is rescored with a special language model, and the consensus translation is extracted as the best path. The proposed system combination approach was evaluated in the framework of the TC-STAR speech translation project. Up to six state-of-the-art statistical phrase-based translation systems from different project partners were combined in the experiments. Significant improvements in translation quality from Spanish to English and from English to Spanish in comparison with the best of the individual MT systems were achieved under official evaluation conditions.","1558-7924","","10.1109/TASL.2008.914970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599393","machine translation;natural languages;speech processing;text processing","Natural languages;Voting;Automatic speech recognition;Computer networks;Lattices;Speech recognition;Speech analysis;Speech processing;Text processing;Iterative methods","language translation;natural languages;speech recognition","machine translation;consensus translation;ROVER approach;speech recognition hypotheses;statistical alignment algorithm;TC-STAR speech translation project;state-of-the-art statistical phrase-based translation system;natural language;spoken language;written language","","18","4","46","","15 Aug 2008","","","IEEE","IEEE Journals"
"English to Japanese spoken lecture translation system by using DNN-HMM and phrase-based SMT","N. Goto; K. Yamamoto; S. Nakagawa","Toyohashi University of Technology, Tenpaku-cho, Toyohashi, Aichi, 441-8580, Japan; Toyohashi University of Technology, Tenpaku-cho, Toyohashi, Aichi, 441-8580, Japan; Toyohashi University of Technology, Tenpaku-cho, Toyohashi, Aichi, 441-8580, Japan","2015 2nd International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)","23 Nov 2015","2015","","","1","6","This paper presents our scheme to translate spoken English lectures into Japanese that consists of an English automatic speech recognition system (ASR) that utilizes a deep neural network (DNN) and an English to Japanese phrase-based statistical machine translation system (SMT). We utilized an existing Wall Street Journal corpus for our acoustic model and adapted it with MIT OpenCourseWare lectures whose transcriptions we also utilized to create our language model. For the parallel corpus of our SMT system, we used TED Talks and Japanese News Article Alignment Data. Our ASR system achieved a word error rate (WER) of 21.0%, and our SMT system achieved a 3-gram base bilingual evaluation understudy (BLEU) of 16.8 for text input and 14.6 for speech input, respectively. These scores outperformed our previous system : WER = 32.1% and BLEU = 11.0.","","978-1-4673-8143-7","10.1109/ICAICTA.2015.7335357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335357","automatic speech recognition;statistical machine translation;classroom lecture;DNN-HMM;phrase-based translation","Hidden Markov models;Data models;Adaptation models;Acoustics;Speech;Speech recognition;Computational modeling","language translation;neural nets;speech recognition","English to Japanese spoken lecture translation;DNN-HMM;phrase-based SMT;English automatic speech recognition system;ASR;deep neural network;English to Japanese phrase-based statistical machine translation system;SMT;OpenCourseWare;word error rate;3-gram base bilingual evaluation understudy","","2","","21","","23 Nov 2015","","","IEEE","IEEE Conferences"
"Neural Networks, AI, Phone-based VR, Machine Learning, Computer Vision and the CUNAT Automated Translation App – not your father’s archaeological toolkit","D. H. Sanders","Learning Sites, Inc., Williamstown, MA, 01267, USA","2018 3rd Digital Heritage International Congress (DigitalHERITAGE) held jointly with 2018 24th International Conference on Virtual Systems & Multimedia (VSMM 2018)","26 Aug 2019","2018","","","1","5","Hundreds of thousands of ancient Mesopotamian legal treatises, beer recipes, religious rituals, international treaties, medical records, astronomical observations, and mathematical problems-texts that tell us exactly what happened in the words of the very people whose actions formed our shared cultural history-languish untranslated, because there are too many of them, translation takes too long, and there are too few linguistic experts. And thousands more tablets are uncovered every year. This paper describes our CUNAT (the CUNeiform Automated Translator) app and how it sets out to use unconventional (for archaeology) approaches to solving the translation problem. CUNAT will allow anyone to create virtual reality models of complex inscribed artifacts and automatically extract meaning from them more effectively and quickly than existing tools. The app uses photogrammetry, computer vision, AI, and neural networks to complete the tasks.","","978-1-7281-0292-4","10.1109/DigitalHeritage.2018.8810002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8810002","neural networks;AI;machine learning;photomodeling;computer vision;virtual heritage;cuneiform;translation;smartphone app;virtual reality","Three-dimensional displays;Solid modeling;Tools;Writing;Computer vision;Cultural differences;Virtual reality","archaeology;computer vision;history;language translation;learning (artificial intelligence);linguistics;photogrammetry;virtual reality","linguistic experts;Translator;archaeology;virtual reality models;computer vision;AI;neural networks;phone-based VR;machine learning;translation app;CUNAT automated translation app;CUNeiform Automated Translator app","","","","19","","26 Aug 2019","","","IEEE","IEEE Conferences"
"Speech Recognition System Combination for Machine Translation","M. J. F. Gales; X. Liu; R. Sinha; P. C. Woodland; K. Yu; S. Matsoukas; T. Ng; K. Nguyen; L. Nguyen; J. -L. Gauvain; L. Lamel; A. Messaoudi","Cambridge University, Trumpington St., Cambridge, UK; Cambridge University, Trumpington St., Cambridge, UK; Cambridge University, Trumpington St., Cambridge, UK; Cambridge University, Trumpington St., Cambridge, UK; Cambridge University, Trumpington St., Cambridge, UK; BBN Technologies, Cambridge, MA, USA; BBN Technologies, Cambridge, MA, USA; BBN Technologies, Cambridge, MA, USA; BBN Technologies, Cambridge, MA, USA; LIMSI-CNRS, Orsay Cedex, France; LIMSI-CNRS, Orsay Cedex, France; LIMSI-CNRS, Orsay Cedex, France","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1277","IV-1280","The majority of state-of-the-art speech recognition systems make use of system combination. The combination approaches adopted have traditionally been tuned to minimising word error rates (WERs). In recent years there has been a growing interest in taking the output from speech recognition systems in one language and translating it into another. This paper investigates the use of cross-site combination approaches in terms of both WER and impact on translation performance. In addition, the stages involved in modifying the output from a speech-to-text (STT) system to be suitable for translation are described. Two source languages, Mandarin and Arabic, are recognised and then translated using a phrase-based statistical machine translation system into English. Performance of individual systems and cross-site combination using cross-adaptation and ROVER are given. Results show that the best STT combination scheme in terms of WER is not necessarily the most appropriate when translating speech.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218341","Machine Translation;Speech Recognition","Speech recognition;Computer numerical control;Error analysis;Surface-mount technology;Voting;Natural languages;Appropriate technology;Costs;Diversity reception","error statistics;language translation;speech recognition","state-of-the-art speech recognition system;machine translation;system combination;word error rates;cross-site combination;speech-to-text system;Mandarin language;Arabic language;phrase-based statistical machine translation system;English language;ROVER","","4","","13","","4 Jun 2007","","","IEEE","IEEE Conferences"
"A new contextual version of Support Vector Machine based on hyperplane translation","R. G. Negri; S. J. S. Sant'Anna; L. V. Dutra","Instituto Nacional de Pesquisas Espaciais - INPE, São José dos Campos - SP, Brazil; Instituto Nacional de Pesquisas Espaciais - INPE, São José dos Campos - SP, Brazil; Instituto Nacional de Pesquisas Espaciais - INPE, São José dos Campos - SP, Brazil","2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS","27 Jan 2014","2013","","","3116","3119","Support Vector Machine (SVM) is a method widely used for image classification. The original formulation of this method does not incorporate contextual information. This study brings a new perspective regarding contextual SVM. The main idea of the presented proposal consists on translates, individually for each pixel using it contextual information, the separation hyperplane originally designed by SVM. A case study using ALOS PALSAR image shows that the proposed method produces better results than traditional SVM.","2153-7003","978-1-4799-1114-1","10.1109/IGARSS.2013.6723486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6723486","Image classification;Support Vector Machine;contextual information;hiperplane translation","Support vector machines;Accuracy;Remote sensing;Kernel;Reliability;Pattern recognition;Training","geophysical image processing;image classification;remote sensing;support vector machines","support vector machine;hyperplane translation;SVM;image classification;contextual information version;ALOS PALSAR imaging;remote sensing","","","","9","","27 Jan 2014","","","IEEE","IEEE Conferences"
"Could we create a training set for image captioning using automatic translation?","N. Samet; S. Hiçsönmez; P. Duygulu; E. Akbaş","Bilgisayar Mühendisliği, Orta Doggu Teknik Üniversitesi, Ankara, Türkiye; Bilgisayar Mühendisliği, Hacettepe Üniversitesi, Ankara, Türkiye; Bilgisayar Mühendisliği, Hacettepe Üniversitesi, Ankara, Türkiye; Bilgisayar Mühendisliği, Orta Doggu Teknik Üniversitesi, Ankara, Türkiye","2017 25th Signal Processing and Communications Applications Conference (SIU)","29 Jun 2017","2017","","","1","4","Automatic image captioning has received increasing attention in recent years. Although there are many English datasets developed for this problem, there is only one Turkish dataset and it is very small compared to its English counterparts. Creating a new dataset for image captioning is a very costly and time consuming task. This work is a first step towards transferring the available, large English datasets into Turkish. We translated English captioning datasets into Turkish by using an automated translation tool and we trained an image captioning model on the automatically obtained Turkish captions. Our experiments show that this model yields the best performance so far on Turkish captioning.","","978-1-5090-6494-6","10.1109/SIU.2017.7960638","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960638","Image captioning;computer vision;machine translation","Dogs;Google;Flickr;Computational modeling;Training;Electronic mail;Tools","computer vision;language translation;natural language processing","automatic translation;automatic image captioning;Turkish dataset;English captioning datasets;automated translation tool;image captioning model;computer vision","","1","","24","","29 Jun 2017","","","IEEE","IEEE Conferences"
"Design of Online Proofreading System for Business English Translation Based on Machine Vision","H. Xu","Lijiang College, Guangxi Normal University,Guilin,China","2020 IEEE International Conference on Industrial Application of Artificial Intelligence (IAAI)","1 Feb 2021","2020","","","155","160","The translation work of business English is becoming more and more complicated, and the accuracy of translation cannot be maintained at a high level. Therefore, it is necessary to use an online proofreading system to make up for the lack of English translation software. However, due to the traditional online proofreading system, the design of the translation logic is relatively fragmented, resulting in the constant changes in the proofreading node layout and affecting the proofreading results. Therefore, an online proofreading system for business English translation based on machine vision is designed. In terms of hardware, connect a new type of single-chip microcomputer in the system and redesign the control circuit. On the software side, it calculates vocabulary similarity by tracking business English vocabulary; calculating semantic similarity based on content characteristics, and designing the system's online proofreading logic based on machine vision theory, and then resetting the system's online intelligent proofreading method. Experimental results: Compared with the system under the traditional design, the proofreading node of the design system deployment and control has a certain regularity. After proofreading the English translation, the accuracy of the translated content has been greatly improved.","","978-1-6654-0471-6","10.1109/IAAI51705.2020.9332885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9332885","machine vision;business English;online proofreading;system design","Vocabulary;Machine vision;Semantics;Software;Artificial intelligence;Random forests;Business","business data processing;computer vision;language translation;microprocessor chips;vocabulary","translation logic;proofreading node layout;business English translation;machine vision;business English vocabulary;English translation software;online proofreading system;single-chip microcomputer;online intelligent proofreading","","","","14","","1 Feb 2021","","","IEEE","IEEE Conferences"
"Integrating Speech Recognition and Machine Translation","S. Matsoukas; I. Bulyko; B. Xiang; K. Nguyen; R. Schwartz; J. Makhoul","BBN Technologies, 10 Moulton St., Cambridge, MA 02138. smatsouk@bbn.com; BBN Technologies, 10 Moulton St., Cambridge, MA 02138; BBN Technologies, 10 Moulton St., Cambridge, MA 02138; Ph.D. student, College of Computer & Information Science, Northeastern University, Boston, MA.; BBN Technologies, 10 Moulton St., Cambridge, MA 02138; BBN Technologies, 10 Moulton St., Cambridge, MA 02138","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1281","IV-1284","This paper presents a set of experiments that we conducted in order to optimize the performance of an Arabic/English machine translation system on broadcast news and conversational speech data. Proper integration of speech-to-text (STT) and machine translation (MT) requires special attention to issues such as sentence boundary detection, punctuation, STT accuracy, tokenization, conversion of spoken numbers and dates to written form, optimization of MT decoding weights, and scoring. We discuss these issues, and show that a carefully tuned STT/MT integration can lead to significant translation accuracy improvements compared to simply feeding the regular STT output to a text MT system.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218342","Speech Recognition;Machine Translation;Sentence Boundary Detection","Speech recognition;Decoding;Pipelines;Broadcast technology;Broadcasting;Loudspeakers;Lattices;Educational institutions;Information science;Contracts","language translation;speech recognition","speech recognition;Arabic-English machine translation system;broadcast news;conversational speech data;speech-to-text;sentence boundary detection;MT decoding weights","","6","","5","","4 Jun 2007","","","IEEE","IEEE Conferences"
"Automatic evaluation methods of a speech translation system's capability","F. Sugaya; K. Yasuda; T. Takezawa; S. Yamamoto","ATR Spoken Language Translation Res. Labs., Kyoto, Japan; ATR Spoken Language Translation Res. Labs., Kyoto, Japan; ATR Spoken Language Translation Res. Labs., Kyoto, Japan; ATR Spoken Language Translation Res. Labs., Kyoto, Japan","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","359","362","The main goal of the paper is to propose automatic schemes for the translation paired comparison method, which was proposed by the authors to evaluate precisely a speech translation system's capability. In the method, the outputs of the speech translation system are subjectively compared with the results of native Japanese taking the Test of English for International Communication (TOEIC), which is used as a measure of a person's speech translation capability. Experiments are conducted on TDMT, which is a subsystem of the Japanese-to-English speech translation system ATR-MATRIX developed at ATR Interpreting Telecommunications Research Laboratories. The winning rate of TDMT shows a good correlation with the TOEIC scores of the examinees. A regression analysis on the subjective results shows that the translation capability of TDMT matches a person scoring around 700 on the TOEIC. The automatic evaluation methods use DP-based similarity, which is calculated by DP distances between a translation output and multiple translation answers. The answers are collected by two methods: paraphrasing and query from a parallel corpus. In both types of collection, the similarity shows the same good correlation with the TOEIC scores of the examinees as the subjective winning rate. Regression analysis using similarity shows that the system's matched point is around 750. We also show effects of paraphrased data.","","0-7803-7343-X","10.1109/ASRU.2001.1034661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034661","","Speech analysis;Natural languages;System testing;Regression analysis;Humans;Laboratories;Performance evaluation;Costs;Text recognition","language translation;natural language interfaces;statistical analysis;speech recognition","speech translation system;automatic evaluation;language translation;ATR-MATRIX;ATR Interpreting Telecommunications Research Laboratories;regression analysis;speech recognition","","2","","9","","7 Nov 2002","","","IEEE","IEEE Conferences"
"Language Model Bootstrapping Using Neural Machine Translation for Conversational Speech Recognition","S. Punjabi; H. Arsikere; S. Garimella","Alexa Machine Learning, Amazon,Bangalore,India; Alexa Machine Learning, Amazon,Bangalore,India; Alexa Machine Learning, Amazon,Bangalore,India","2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","20 Feb 2020","2019","","","487","493","Building conversational speech recognition systems for new languages is constrained by the availability of utterances capturing user-device interactions. Data collection is expensive and limited by speed of manual transcription. In order to address this, we advocate the use of neural machine translation as a data augmentation technique for bootstrapping language models. Machine translation (MT) offers a systematic way of incorporating collections from mature, resource-rich conversational systems that may be available for a different language. However, ingesting raw translations from a general purpose MT system may not be effective owing to the presence of named entities, intra sentential code-switching and the domain mismatch between the conversational data being translated and the parallel text used for MT training. To circumvent this, we explore following domain adaptation techniques: (a) sentence embedding based data selection for MT training, (b) model finetuning, and (c) rescoring and filtering translated hypotheses. Using Hindi language as the experimental testbed, we supplement transcribed collections with translated US English utterances. We observe a relative word error rate reduction of 7.8-15.6%, depending on the bootstrapping phase. Fine grained analysis reveals that translation particularly aids the interaction scenarios underrepresented in the transcribed data.","","978-1-7281-0306-8","10.1109/ASRU46091.2019.9003982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003982","speech recognition;neural machine translation;domain adaptation;code-switching","Training;Data models;Adaptation models;Decoding;Buildings;Speech recognition;Architecture","natural language processing;speech recognition;statistical analysis","language model bootstrapping;neural machine translation;conversational speech recognition systems;user-device interactions;data collection;data augmentation technique;language models;resource-rich conversational systems;general purpose MT system;conversational data;domain adaptation techniques;model finetuning;rescoring;filtering translated hypotheses;Hindi language;supplement transcribed collections;translated US English utterances;transcribed data;intrasentential code-switching","","","","20","","20 Feb 2020","","","IEEE","IEEE Conferences"
"Applications of Statistical Machine Translation Approaches to Spoken Language Understanding","K. Macherey; O. Bender; H. Ney","Google Inc., Mountain View, CA; Comput. Sci. Dept. 6, RWTH Aachen Univ., Aachen; Comput. Sci. Dept. 6, RWTH Aachen Univ., Aachen","IEEE Transactions on Audio, Speech, and Language Processing","27 Mar 2009","2009","17","4","803","818","In this paper, we investigate two statistical methods for spoken language understanding based on statistical machine translation. The first approach employs the source-channel paradigm, whereas the other uses the maximum entropy framework. Starting with an annotated corpus, we describe the problem of natural language understanding as a translation from a source sentence to a formal language target sentence. We analyze the quality of different alignment models and feature functions and show that the direct maximum entropy approach outperforms the source channel-based method. Furthermore, we investigate how both methods perform if the input sentences contain speech recognition errors. Finally, we investigate a new approach to combine speech recognition and spoken language understanding. For this purpose, we employ minimum error rate training which directly optimizes the final evaluation criterion. By combining all knowledge sources in a log-linear way, we show that we can decrease both the word error rate and the slot error rate. Experiments were carried out on two German inhouse corpora for spoken dialogue systems.","1558-7924","","10.1109/TASL.2009.2014262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4806285","Combined approach;machine translation;maximum entropy;minimum error rate training;speech recognition;spoken language understanding","Natural languages;Entropy;Error analysis;Automatic speech recognition;Speech recognition;Surface-mount technology;Computer science;Statistical analysis;Formal languages;Humans","entropy;formal languages;language translation;natural language processing;speech recognition;statistical analysis","statistical machine translation;spoken language understanding;source-channel paradigm;maximum entropy framework;natural language understanding;formal language target sentence;alignment models;direct maximum entropy approach;speech recognition errors;minimum error rate training;final evaluation criterion;German inhouse corpora;spoken dialogue systems","","11","3","28","","27 Mar 2009","","","IEEE","IEEE Journals"
"Translation russian cyrillic to latin alphabet using SVM (support vector machine)","D. F. Azid; B. Irawan; C. Setianingsih","School Of Electrical Engineering, Telkom University, Bandung, Jawa Barat, Indonesia; School Of Electrical Engineering, Telkom University, Bandung, Jawa Barat, Indonesia; School Of Electrical Engineering, Telkom University, Bandung, Jawa Barat, Indonesia","2017 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob)","8 Feb 2018","2017","","","59","65","Russian is a language that is widely used by people in the world for various purposes because it is ranked 6th in the world as the language with the most speakers. This is one of the foundations of this study in addition to the high interest of the world population to use and learn the language. Russian language does not use alphabets but uses Cyrillic script, where the font is different from the letters in general so there are some obstacles to learn, understand and pronounce it. Capture to translate is one of the media that built to be a solution of this problem, built on image processing, feature extraction process with edges detection findcontours method and artificial intelligence using Support Vector Machine (SVM) classification algorithm with android mobile application interface that utilizes camera device as its input. In this study, Capture to translate using the Support Vector Machine (SVM) classification algorithm is able to produce a level of word classification accuracy of 93.8% in three syllable based on test that has been done, which is related to preprocessing, feature extraction and classification.","","978-1-5386-2373-2","10.1109/APWiMob.2017.8284005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8284005","Russia;Cyrillic;Capture to Translate;Image Processing;edges detection;findcontours;Artificial Intelligence;Support Vector Machine (SVM);Android;Preprocessing","Support vector machines;Artificial intelligence;Feature extraction;Classification algorithms;Training;Image edge detection;Cameras","Android (operating system);edge detection;feature extraction;image classification;language translation;mobile computing;natural language processing;support vector machines","SVM;translation russian cyrillic;latin alphabet;russian language;Cyrillic script;feature extraction process;edges detection findcontours method;Support Vector Machine classification algorithm;artificial intelligence;android mobile application interface","","4","","16","","8 Feb 2018","","","IEEE","IEEE Conferences"
"Automatic Speech Translation System Selecting Target Language by Direction-of-Arrival Information","M. Tsujikawa; K. Okabe; K. Hanazawa; Y. Kajikawa","Department of Electrical, Electronic and Information Engineering, Faculty of Engineering Science, Kansai University, Biometrics Research Laboratoties, NEC Corporation, Osaka, Kanagawa, Japan; Biometrics Research Laboratoties, NEC Corporation, Kanagawa, Japan; Biometrics Research Laboratoties, NEC Corporation, Kanagawa, Japan; Department of Electrical, Electronic and Information Engineering, Faculty of Engineering Science, Kansai University, Biometrics Research Laboratoties, NEC Corporation, Osaka, Kanagawa, Japan","2018 26th European Signal Processing Conference (EUSIPCO)","2 Dec 2018","2018","","","2315","2319","In this paper, we propose an automatic speech translation system that selects its target language on the basis of the direction-of-arrival (DOA) information. The system uses two microphones to detect speech signals arriving from specific directions. The target language for speech recognition is selected on the basis of the DOA. Both the speech detection and target language selection relieves users from operations normally required for individual utterances, without serious increase in computational costs. In a speech-recognition evaluation of the proposed system, 80 % word accuracy was achieved for utterances recorded with two microphones that were 40cm distant from speaker positions. This accuracy is nearly equivalent to that in which the time frame and target language of a user's speech are given in advance.","2076-1465","978-9-0827-9701-5","10.23919/EUSIPCO.2018.8553139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8553139","automatic speech translation;speech recognition;language identification;direction of arrival;speech detection;microphone array","Speech recognition;Voice activity detection;Microphones;Interference;Noise measurement;Loudspeakers;Noise robustness","direction-of-arrival estimation;language translation;microphones;speaker recognition;speech recognition","automatic speech translation system;direction-of-arrival information;speech-recognition evaluation;speech detection;speech signals","","1","","9","","2 Dec 2018","","","IEEE","IEEE Conferences"
"Multi-Information Spatial–Temporal LSTM Fusion Continuous Sign Language Neural Machine Translation","Q. Xiao; X. Chang; X. Zhang; X. Liu","Department of Electronics Information and Engineering, Xi’an Technological University, Xi’an, China; Department of Electronics Information and Engineering, Xi’an Technological University, Xi’an, China; Department of Electronics Information and Engineering, Xi’an Technological University, Xi’an, China; Department of Electronics Information and Engineering, Xi’an Technological University, Xi’an, China","IEEE Access","9 Dec 2020","2020","8","","216718","216728","There are two basic problems in sign language recognition (SLR): (a) isolated word SLR and (b) continuous SLR. Most of the existing continuous SLR methods are extensions of the isolated word SLR methods. These methods use the isolated word SLR results as the basic module and obtain the sentence recognition results through sentence segmentation and word alignment. However, sentence segmentation and word alignment are often not accurate, resulting in a low sentence recognition accuracy. At the same time, continuous SLR usually requires strict sample labels, leading to the difficult task of manual labeling and limited training data availability. To address these challenges, this paper proposes a bidirectional spatial-temporal LSTM fusion attention network (Bi-ST-LSTM-A) for continuous SLR. This approach avoids problems such as sentence segmentation, word alignment, and tedious manual labeling. Our contributions are summarized as follows: (1) we proposed a sign language video feature representation method using a convolutional neural network (CNN) and spatial-temporal LSTM (ST-LSTM) information fusion technology; and (2) we constructed a uniform neural machine translation framework that can be used for complex continuous SLR and gesture recognition of nonspecific people in nonspecific environments. Experiments were carried out on some large continuous sign language datasets. The sign language recognition accuracy reached 81.22% on the 500 CSL dataset, 76.12% on the RWTH-PHOENIX-Weather dataset and 75.32% on the RWTH-PHOENIX-Weather-2014T dataset, thereby illustrating the effectiveness of the proposed framework.","2169-3536","","10.1109/ACCESS.2020.3039539","NSFC; Shaanxi Province Natural Science Foundation; National Natural Science Foundation of China; Natural Science Basic Research Plan of Shaanxi Province in China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9265176","Continuous SLR;attention;ST-LSTM;neural machine translation;CNN","Gesture recognition;Assistive technology;Hidden Markov models;Feature extraction;Task analysis;Convolution;Kernel","convolutional neural nets;feature extraction;image fusion;image representation;image segmentation;language translation;natural language processing;recurrent neural nets;sign language recognition;video signal processing","word alignment;bidirectional spatial-temporal LSTM fusion attention network;Bi-ST-LSTM-A;sentence segmentation;continuous sign language datasets;continuous SLR;sentence recognition;isolated word SLR;neural machine translation;multiinformation spatial-temporal LSTM fusion;sign language recognition;sign language video feature representation;convolutional neural network;spatial-temporal LSTM information fusion;gesture recognition;CSL dataset;RWTH-PHOENIX-Weather dataset;RWTH-PHOENIX-Weather-2014T dataset","","1","","53","CCBY","20 Nov 2020","","","IEEE","IEEE Journals"
"English to Japanese spoken language translation system for classroom lectures","V. Ferdiansyah; S. Nakagawa","Department of Computer Science and Engineering Toyohashi University of Technology Toyohashi, Japan; Department of Computer Science and Engineering Toyohashi University of Technology Toyohashi, Japan","2014 International Conference of Advanced Informatics: Concept, Theory and Application (ICAICTA)","12 Jan 2015","2014","","","34","38","This paper presents our attempt to create English automatic speech recognition (ASR) and English to Japanese statistical machine translation system (SMT). We used MIT OpenCourseWare lectures as our test lecture corpus. Wall Street Journal (WSJ) corpus adapted with MIT OpenCourseWare lectures was used as our acoustic model. MIT OpenCourseWare lecture transcriptions were utilized to create our language model. As for the parallel corpus, we used TED Talks and Japanese-English News Article Alignment Data (JENAAD). Our proposed ASR system can achieve 32.1% 0word error rate (WER) and our SMT system can achieve 10.95 BLEU.","","978-1-4799-5100-0","10.1109/ICAICTA.2014.7005911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005911","automatic speech recognition;machine translation;classroom lectures;MIT OCW","Adaptation models;Speech;Acoustics;Hidden Markov models;Data models;Speech recognition;Informatics","courseware;language translation;speech recognition","classroom lectures;English automatic speech recognition;English-to-Japanese statistical machine translation system;MIT OpenCourseWare lectures;Wall Street Journal corpus;WSJ corpus;acoustic model;lecture transcriptions;TED Talks;JapaneseEnglish News Article Alignment Data;JENAAD;ASR system;word error rate;WER;SMT system","","2","","30","","12 Jan 2015","","","IEEE","IEEE Conferences"
"Automatic detection of artifact in neonatal ECG","S. Gholinezhadasnefestani; W. Marnane; G. Lightbody; A. Temko; G. Boylan; N. Stevenson","Department of Electrical and Electronic Engineering, Neonatal Brain Research Group, Irish Centre for Fetal and Neonatal Translational Research, University College Cork, Cork, Ireland; Department of Electrical and Electronic Engineering, Neonatal Brain Research Group, Irish Centre for Fetal and Neonatal Translational Research, University College Cork, Cork, Ireland; Department of Electrical and Electronic Engineering, Neonatal Brain Research Group, Irish Centre for Fetal and Neonatal Translational Research, University College Cork, Cork, Ireland; Department of Electrical and Electronic Engineering, Neonatal Brain Research Group, Irish Centre for Fetal and Neonatal Translational Research, University College Cork, Cork, Ireland; Department of Paediatrics and Child Health, Neonatal Brain Research Group, Irish Centre for Fetal and Neonatal Translational Research, University College Cork, Cork, Ireland; Department of Paediatrics and Child Health, Neonatal Brain Research Group, Irish Centre for Fetal and Neonatal Translational Research, University College Cork, Cork, Ireland","2015 22nd Iranian Conference on Biomedical Engineering (ICBME)","25 Feb 2016","2015","","","184","188","Heart Rate Variability derived from the Neonatal Electrocardiogram has been found to be associated with the Electroencephalography grade of Hypoxic Ischemic Encephalopathy and neurodevelopmental outcome. This association has been established for clean or artifact free ECG. However, it was shown that the Electrocardiogram and subsequently Heart Rate Variability features can be heavily corrupted by artifacts which have to be manually removed. This work combines a set of statistical features to quantify the quality of the HR signal by automatically detecting the artifacts in neonatal ECG. The HRV signal is obtained by detecting R-Peaks using the adapted Pan-Tompkins algorithm. Four features are extracted from HR signal to discriminate normal and corrupted signal. The performance of these features in discrimination is then assessed using statistical tests. It has been shown that there is a significant difference of proposed features between artifact and normal signals (p<;0.001). The discrimination power is increased by combing the current features using Support Vector Machine. The median AUC was 0.9941 (IQR: 0.98-1.00).","","978-1-4673-9351-5","10.1109/ICBME.2015.7404139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404139","component;Heart rate variabilty;ECG;feature extraction;Support Vector Machine(SVM)","Support vector machines;Heart rate variability;Feature extraction;Pediatrics;Electrocardiography;Detectors","electrocardiography;medical signal processing;obstetrics","automatic artifact detection;neonatal ECG;Heart Rate Variability;Neonatal Electrocardiogram;Hypoxic Ischemic Encephalopathy;neurodevelopmental outcome;adapted Pan-Tompkins algorithm;Support Vector Machine","","","","16","","25 Feb 2016","","","IEEE","IEEE Conferences"
"Re-Translation Strategies for Long Form, Simultaneous, Spoken Language Translation","N. Arivazhagan; C. Cherry; I. Te; W. Macherey; P. Baljekar; G. Foster",Google Research; Google Research; Google Research; Google Research; Google Research; Google Research,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","7919","7923","We investigate the problem of simultaneous machine translation of long-form speech content. We target a continuous speech-to-text scenario, generating translated captions for a live audio feed, such as a lecture or play-by-play commentary. As this scenario allows for revisions to our incremental translations, we adopt a re-translation approach to simultaneous translation, where the source is repeatedly translated from scratch as it grows. This approach naturally exhibits very low latency and high final quality, but at the cost of incremental instability as the output is continuously refined. We experiment with a pipeline of industry-grade speech recognition and translation tools, augmented with simple inference heuristics to improve stability. We use TED Talks as a source of multilingual test data, developing our techniques on English-to-German spoken language translation. Our minimalist approach to simultaneous translation allows us to scale our final evaluation to several other target languages, dramatically improving incremental stability for all of them.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054585","Speech Recognition Neural Machine Translation","Signal processing algorithms;Speech recognition;Tools;Signal processing;Stability analysis;Machine translation;Speech processing","language translation;linguistics;speech recognition","Re-Translation Strategies;simultaneous machine translation;long-form speech content;speech-to-text scenario;translated captions;live audio feed;play-by-play commentary;incremental translations;simultaneous translation;industry-grade speech recognition;translation tools;English-to-German spoken language translation;target languages;incremental stability","","","","16","","9 Apr 2020","","","IEEE","IEEE Conferences"
"Multilingual speech to speech translation system in bluetooth environment","M. D. F. Ansari; R. S. Shaji; T. J. SivaKarthick; S. Vivek; A. Aravind","Department of Information Technology, Noorul Islam University, Kumaracoil; Department of Information Technology, Noorul Islam University, Kumaracoil; Department of Information Technology, Noorul Islam University, Kumaracoil; Department of Information Technology, Noorul Islam University, Kumaracoil; Department of Information Technology, Noorul Islam University, Kumaracoil","2014 International Conference on Control, Instrumentation, Communication and Computational Technologies (ICCICCT)","22 Dec 2014","2014","","","1055","1058","Voice Translator is speech to speech translation application for android mobile phone, which translates English speech to Hindi speech and vice versa. Voice Translator includes three modules, Voice Recognition, Machine Translation and Speech Synthesis. Voice Recognition module captures the voice or speech from the mobile user through speaker, identifies then converts the speech into text and then the text send to Machine Translation for further process. Machine Translation module does the process of translation i.e. this module consists of library for both language and when text is received by this module, it converts the text of one language to another as per user choice and thus it sends the translated text to last module. Speech Synthesis module acts as the text to speech translator i.e. when it gets the translated text. This module processes on translated text which converts it into speech and then makes it as user output. Thus, Voice Translation application works by integrating all these three modules and gives the user best output.","","978-1-4799-4190-2","10.1109/ICCICCT.2014.6993116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993116","Voice Recognition;Machine Translation;Multilingual speech","Speech;Speech recognition;Bluetooth;Speech synthesis;Accuracy;Text recognition","Bluetooth;language translation;natural language processing;smart phones;speech recognition;speech synthesis","multilingual speech to speech translation system;Bluetooth environment;voice translator;Android mobile phone;English speech-Hindi speech translation;voice recognition;machine translation;speech synthesis;text-speech translator","","","","9","","22 Dec 2014","","","IEEE","IEEE Conferences"
"A method of automatic recognition of attributive clauses in Chinese language","L. Wang; W. Qu; H. Wang; S. Yu","Key Lab of Computational Linguistics of Ministry of Education, School of Foreign Languages, Peking University, Beijing 100871, China; School of Computer Science, Nanjing Normal University, 210000, China; Institute of Computational Linguistics, Peking University, Beijing 100871, China; Institute of Computational Linguistics, Peking University, Beijing 100871, China","2016 International Conference on Asian Language Processing (IALP)","13 Mar 2017","2016","","","172","175","Influenced by the grammatical system of western languages, there are more and more syntactic structures in modern Chinese that can be translated into English attributive clauses. But for the great differences of the syntactic structures, parsing Chinese by western grammatical rules usually does not lead to satisfactory results, which will result in poor translation performance in complex syntactic structures. This paper first attempts to recognize the attributive clauses by using conditional random fields (“CRFs”) theory by selecting representative features combined with grammatical information unique in Chinese language. The experiment proves that this method will produce a better result than simply by statistical machine translation method.","","978-1-5090-0922-0","10.1109/IALP.2016.7875961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875961","Chinese language;Attributive Clause;Automatic Recognition","Syntactics;Hidden Markov models;Computational linguistics;Compounds;Feature extraction;Probabilistic logic;Semantics","language translation;natural language processing;speech recognition","automatic recognition;Chinese language;grammatical system;western languages;English attributive clauses;parsing Chinese;western grammatical rules;complex syntactic structures;conditional random fields;CRF theory;grammatical information;statistical machine translation","","","","7","","13 Mar 2017","","","IEEE","IEEE Conferences"
"Speech Translation by Confusion Network Decoding","N. Bertoldi; R. Zens; M. Federico","ITC-irst, Centro per la Ricerca, Scientifica e Tecnologica, I-38050 Povo (Trento), Italy. bertoldi@itc.it; Lehrstuhl für Informatik 6, Computer Science Department, RWTH Aachen University, D-52056 Aachen, Germany. zens@cs.rwth-aachen.de; ITC-irst, Centro per la Ricerca, Scientifica e Tecnologica, I-38050 Povo (Trento), Italy. federico@itc.it","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1297","IV-1300","This paper describes advances in the use of confusion networks as interface between automatic speech recognition and machine translation. In particular, it presents an implementation of a confusion network decoder which significantly improves both in efficiency and performance previous work along this direction. The confusion network decoder results as an extension of a state-of-the-art phrase-based text translation system. Experimental results in terms of decoding speed and translation accuracy are reported on a real-data task, namely the translation of plenary speeches at the European Parliament from Spanish to English.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218346","Machine Translation;Speech Translation;Natural Language Processing","Decoding;Speech recognition;Speech processing;Natural languages;Speech analysis;Computer science;Automatic speech recognition;Natural language processing;Information processing;Information analysis","decoding;language translation;speech coding;speech recognition","speech translation;confusion network decoding;automatic speech recognition;machine translation;state-of-the-art phrase-based text translation system;plenary speeches;European Parliament","","17","2","11","","4 Jun 2007","","","IEEE","IEEE Conferences"
"Classification of EEG Multiple Imagination Tasks Based on Independent Component Analysis and Relevant Vector Machines","S. Zhang; R. Xu; A. N. Belkacem; D. Shin; K. Wang; Z. Wang; L. Yu; Z. Qiao; C. Wang; C. Chen","Tianjin University of Technology, Key Laboratory of Complex System Control Theory and Application, Tianjin, 300384, China; Tianjin University, Academy of medical Engineering and Translational medicine, Tianjin, 300072, China; Department of computer and network engineering, College of Information Technology, UAE University, Al Ain, 15551, UAE; Department of Electronics and Mechatronics, Tokyo Polytechnic University, Atsugi, 243-0297, Japan; Tianjin University, Academy of medical Engineering and Translational medicine, Tianjin, 300072, China; Tianjin University, Academy of medical Engineering and Translational medicine, Tianjin, 300072, China; Tianjin University of Technology, Zhonghuan Information College, Tianjin, 300380, China; Tianjin University of Technology, Key Laboratory of Complex System Control Theory and Application, Tianjin, 300384, China; Beijing Anding Hospital, Beijing Key Laboratory of Mental Disorders, Capital Medical University, Beijing, 100088, China; Tianjin University, Academy of medical Engineering and Translational medicine, Tianjin, 300072, China","2019 IEEE MTT-S International Microwave Biomedical Conference (IMBioC)","29 Jul 2019","2019","1","","1","4","To solve the problem of feature extraction in braincomputer interface (BCI), the position, size and direction of dipole are located by using dipole localization method, so as to locate the active part of advanced nerve activity and remove a series of physiological and electrical artifacts such as electro-ophthalmogram. The common space pattern and correlation vector machine are used to extract the effective components of EEG signals and classify multiple motor imagery tasks. The results show that the combination of EEG dipole localization and common spatial pattern can effectively improve the signal-to-noise ratio of EEG signals and extract more obvious features. The correlation vector machine provides better classification results and is an effective method to complete the classification and recognition of motor imagery signals.","","978-1-5386-7395-9","10.1109/IMBIOC.2019.8777887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8777887","Brain computer interface;Motor imagery;Brainwave dipole Cospatial pattern;Relevance vector machine","Electroencephalography;Feature extraction;Brain-computer interfaces;Support vector machine classification;Signal processing algorithms;Independent component analysis","brain-computer interfaces;electroencephalography;feature extraction;independent component analysis;medical signal processing;signal classification;support vector machines","common space pattern;electro-ophthalmogram;electrical artifacts;physiological artifacts;advanced nerve activity;dipole localization method;BCI;braincomputer interface;feature extraction;relevant vector machines;independent component analysis;EEG multiple imagination tasks;motor imagery signals;obvious features;signal-to-noise ratio;common spatial pattern;EEG dipole localization;multiple motor imagery tasks;EEG signals;effective components;correlation vector machine","","1","","17","","29 Jul 2019","","","IEEE","IEEE Conferences"
"Research of Language Recognition Methods Based on Machine Learning","D. L. Khabarov; V. V. Bazanov; A. V. Kuchebo; M. Zavgorodnii; A. Y. Rybakova","National Research Nuclear University MEPhI,(Moscow Engineering Physics Institute),Moscow,Russian Federation; National Research Nuclear University MEPhI,(Moscow Engineering Physics Institute),Moscow,Russian Federation; National Research Nuclear University MEPhI,(Moscow Engineering Physics Institute),Moscow,Russian Federation; National Research Nuclear University MEPhI,(Moscow Engineering Physics Institute),Moscow,Russian Federation; Kazan National Research Technological University,Kazan,Russian Federation","2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (ElConRus)","9 Apr 2021","2021","","","438","442","This article is focused on speech recognition methods research and analysis of their efficiency. It presents a review of the most known ways to convert a person's speech into text, their effectiveness, and complexity in exploitation. The article scribes the application of machine learning in speech recognition tasks. A program was developed with a neural network for the research realization - a new technique of speech translation automation. The experiment provides information about comparing methods of speech recognition and analyzing the results of using them. Conclusions about the further realization of the research were drawn based on these results.","2376-6565","978-1-6654-0476-1","10.1109/ElConRus51938.2021.9396491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9396491","machine learning;speech recognition;neural networks;machine hearing;language determination","Automation;Conferences;Neural networks;Speech recognition;Machine learning;Complexity theory;Task analysis","image recognition;language translation;learning (artificial intelligence);natural language processing;speech recognition","language recognition methods;machine learning;speech recognition methods research;known ways;person;speech recognition tasks;research realization;speech translation automation","","","","10","","9 Apr 2021","","","IEEE","IEEE Conferences"
"Detection of Alphabets for Machine Translation of Sign Language Using Deep Neural Net","P. T. Krishnan; P. Balasubramanian","OMR, St. Joseph's College of Engg., Chennai, India; OMR, St. Joseph's College of Engg., Chennai, India","2019 International Conference on Data Science and Communication (IconDSC)","29 Aug 2019","2019","","","1","3","Recognition of sign language by hand gestures is one of the classical problems in computer vision. Conventional tools used for sign language translation involves application of linear classifiers such as kNN and Support Vector Machine (SVM)to perform the classification of hand gestures images. However, these methods require sophisticated features for classification. To automate the feature extraction and feature selection procedure, a Deep Neural Network (DNN)based machine translation is proposed in this work. Here, the images of English Sign Language (ESL)are identified using Deep Learning (DL)approach. A custom DNN with three convolution layers and three Max-Pooling layers are designed for this purpose. A top validation accuracy of 82% was obtained for the DNN structure proposed in this paper.","","978-1-5386-9319-3","10.1109/IconDSC.2019.8816988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816988","Deep Learning;Sign Language Translation;Deep Neural Net;Gesture Recognition;Convolution Net","Training;Gesture recognition;Assistive technology;Deep learning;Convolution;Neural networks;Feature extraction","computer vision;feature extraction;gesture recognition;image classification;language translation;learning (artificial intelligence);neural nets;support vector machines","computer vision;sign language translation;linear classifiers;hand gestures images;feature extraction;feature selection procedure;alphabet detection;deep neural network based machine translation;English sign language;deep learning approach;DNN;ESL;support vector machine;kNN;SVM","","1","","8","","29 Aug 2019","","","IEEE","IEEE Conferences"
"Development and application of multilingual speech translation","S. Nakamura","Spoken Language Communication Research Group Project, National Institute of Information and Communications Technology, Japan","2009 Oriental COCOSDA International Conference on Speech Database and Assessments","2 Oct 2009","2009","","","9","12","This paper describes the latest version of handheld speech-to-speech translation system developed by National Institute of Information and Communications Technology, NICT. As the entire speech-to-speech translation functions are implemented into one terminal, it realizes real-time and location free speech-to-speech translation service for many language pairs. A new noise-suppression technique notably improves speech recognition performance. Corpus-based approaches of recognition, translation, and synthesis enabled wide range coverage of topic varieties and portability to other languages. Currently, we mainly focus on translation between Japanese, English and Chinese.","","978-1-4244-4400-7","10.1109/ICSDA.2009.5278383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5278383","speech-to-speech translation;speech recognition;speech synthesis;machine translation;large-scale corpus","Natural languages;Speech recognition;Surface-mount technology;Speech synthesis;Large-scale systems;Working environment noise;Communications technology;Training data;Loudspeakers;Adaptation model","language translation;speech recognition","multilingual speech translation;handheld speech-to-speech translation system;noise-suppression technique;speech recognition","","1","","13","","2 Oct 2009","","","IEEE","IEEE Conferences"
"YesilcamGAN: Automatic face translation to Yesilcam artists","S. Özkan; G. B. Akar","Multimedia Lab. Ortadoğu Teknik Üniversitesi, Ankara, Türkiye; Multimedia Lab. Ortadoğu Teknik Üniversitesi, Ankara, Türkiye","2018 26th Signal Processing and Communications Applications Conference (SIU)","9 Jul 2018","2018","","","1","4","In this paper, we propose a method that can automatically translate regular human faces to Yesilcam artists. Throughout the paper, two important contributions are presented. First, since a classifier based approach is used, the proposed method automatically selects the most similar artist and makes the face translation. Second, so as to obtain the latent information about faces, a pre-trained model is utilized at the encoder part. Experiments show that visual results which are perceptually promising, Yesilcam artist-like yet preserving facial features of a person, can be synthesized.","","978-1-5386-1501-0","10.1109/SIU.2018.8404362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404362","Cycle-Generative Adversarial Networks;Image-to-image Translation","Face;Dogs;Art;Image generation;Multimedia communication;Visualization;Facial features","art;face recognition;image classification;language translation","Yesilcam artist-like;YesilcamGAN;automatic face translation;Yesilcam artists","","","","","","9 Jul 2018","","","IEEE","IEEE Conferences"
"A survey of voice translation methodologies — Acoustic dialect decoder","H. Krupakar; K. Rajvel; B. Bharathi; S. A. Deborah; V. Krishnamurthy","Dept. of Computer Science and Engineering, SSN College Of Engineering; Dept. of Computer Science and Engineering, SSN College Of Engineering; Dept. of Computer Science and Engineering, SSN College Of Engineering; Dept. of Computer Science and Engineering, SSN College Of Engineering; Dept. of Computer Science and Engineering, SSN College Of Engineering","2016 International Conference on Information Communication and Embedded Systems (ICICES)","25 Jul 2016","2016","","","1","9","Language Translation has always been about inputting source as text/audio and waiting for system to give translated output in desired form. In this paper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice earpiece translation device. We introduce and survey the recent advances made in the field of Speech Engineering, to employ in the ADD, particularly focusing on the three major processing steps of Recognition, Translation and Synthesis. We tackle the problem of machine understanding of natural language by designing a recognition unit for source audio to text, a translation unit for source language text to target language text, and a synthesis unit for target language text to target language speech. Speech from the surroundings will be recorded by the recognition unit present on the ear-piece and translation will start as soon as one sentence is successfully read. This way, we hope to give translated output as and when input is being read. The recognition unit will use Hidden Markov Models (HMMs) Based Tool-Kit (HTK), RNNs with LSTM cells, and the synthesis unit, HMM based speech synthesis system HTS. This system will initially be built as an English to Tamil translation device.","","978-1-5090-2552-7","10.1109/ICICES.2016.7518940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518940","Voice Translator;Speech Recognition;Machine Translation;Speech Synthesis;Deep learning;RNN;LSTM;HTK;HTS;HMMs","Hidden Markov models;Speech;Speech recognition;Decoding;Feature extraction;Neural networks;Computer science","acoustic signal processing;hidden Markov models;language translation;recurrent neural nets;speech recognition;speech synthesis;text analysis","voice translation methodologies;acoustic dialect decoder;language translation;ADD;voice-to-voice earpiece translation device;speech engineering;speech recognition;speech translation;machine understanding problem;natural language;recognition unit;source audio;translation unit;source language text;target language text;synthesis unit;target language speech;hidden Markov models;HMM-based tool-kit;HTK;RNN;LSTM cells;HMM based speech synthesis system;HTS;English-to-Tamil translation device","","","","39","","25 Jul 2016","","","IEEE","IEEE Conferences"
"Automatic registration of images with simulated rotation and translation estimation using HAIRIS","K. Priyadharshini","M.E Communication System, Department of Electronics and Communication Engineering, PGP College of Engineering and Technology, India","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","30 Jan 2014","2013","","","1","8","Image registration is the most important fundamental phenomenon in the image processing system. In which Automatic image registration is a challenging aspect. Although enormous methods for automatic image registration have been developed and implemented in ancient days, it is still a broad use in plenty applications, such as in remote sensing. In my work, I have proposed a method for automatic image registration through histogram-based image segmentation (HAIRIS). This new approach is designed by combining several segmentations of the pair of images to be registered, according to a relaxation parameter based on the delineating histogram modes, following the characterization of the objects extracted - via the objects area, axis ratio, perimeter and fractal dimension - and a statistical procedure for objects matching is applied to each object. Finally, the simulated rotation and translation are illustrated for this proposed methodology. The first and foremost dataset consists of a photograph and a rotated and shifted version of this photograph is developed, with different levels of added Gaussian white noise. This can also be applied to satellite images which are in pair, with different spectral content and simulated translation and rotation is estimated, and also for various remote sensing applications comprising of different viewing angles, different acquisition dates and different sensors. Histogram-based image segmentation allows the registration of pairs of multitemporal and multisensor images with their differences in rotation and translation parameters, with small spectral content differences whereas, leading to sub pixel accuracy.","","978-1-4799-3926-8","10.1109/ICCCNT.2013.6726537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726537","Histogram;Image registration;Automatic Image registration;image segmentation;matching;wiener filter;second-order Butterworth LPF","Image segmentation;Histograms;Remote sensing;Image registration;Wiener filters;Estimation;Noise","feature extraction;Gaussian noise;image matching;image registration;image segmentation;image sensors;spectral analysis;white noise","rotation estimation;translation estimation;image processing system;automatic image registration;remote sensing;histogram-based image segmentation;HAIRIS;relaxation parameter;histogram modes;objects extraction;objects area;axis ratio;object perimeter;fractal dimension;statistical procedure;objects matching;photograph;added Gaussian white noise;satellite images;spectral content;viewing angles;acquisition dates;multitemporal images;multisensor images;rotation parameters;translation parameters;subpixel accuracy","","","","10","","30 Jan 2014","","","IEEE","IEEE Conferences"
"Computing consensus translation from multiple machine translation systems","B. Bangalore; G. Bordel; G. Riccardi","AT&T Labs.-Res., USA; NA; NA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","351","354","We address the problem of computing a consensus translation given the outputs from a set of machine translation (MT) systems. The translations from the MT systems are aligned with a multiple string alignment algorithm and the consensus translation is then computed. We describe the multiple string alignment algorithm and the consensus MT hypothesis computation. We report on the subjective and objective performance of the multilingual acquisition approach on a limited domain spoken language application. We evaluate five domain-independent off-the-shelf MT systems and show that the consensus-based translation performance is equal to or better than any of the given MT systems, in terms of both objective and subjective measures.","","0-7803-7343-X","10.1109/ASRU.2001.1034659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034659","","Natural languages;Tagging;Text categorization;Performance evaluation;Optical wavelength conversion;Impedance matching;Robustness;Speech recognition;Stochastic processes;Automatic speech recognition","language translation;natural language interfaces;speech recognition","machine translation systems;multiple string alignment algorithm;consensus hypothesis computation;multilingual acquisition approach;spoken language;speech recognition","","23","7","9","","7 Nov 2002","","","IEEE","IEEE Conferences"
"Language model adaptation for ASR of spoken translations using phrase-based translation models and named entity models","J. Pelemans; T. Vanallemeersch; K. Demuynck; L. Verwimp; H. Van hamme; P. Wambacq","ESAT, KU Leuven, Belgium; Centre for Computational Linguistics, KU Leuven, Belgium; ELIS, Ghent University, Belgium; ESAT, KU Leuven, Belgium; ESAT, KU Leuven, Belgium; ESAT, KU Leuven, Belgium","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 May 2016","2016","","","5985","5989","Language model adaptation based on Machine Translation (MT) is a recently proposed approach to improve the Automatic Speech Recognition (ASR) of spoken translations that does not suffer from a common problem in approaches based on rescoring i.e. errors made during recognition cannot be recovered by the MT system. In previous work we presented an efficient implementation for MT-based language model adaptation using a word-based translation model. By omitting renormalization and employing weighted updates, the implementation exhibited virtually no adaptation overhead, enabling its use in a real-time setting. In this paper we investigate whether we can improve recognition accuracy without sacrificing the achieved efficiency. More precisely, we investigate the effect of both state-of-the-art phrase-based translation models and named entity probability estimation. We report relative WER reductions of 6.2% over a word-based LM adaptation technique and 25.3% over an unadapted 3-gram baseline on an English-to-Dutch dataset.","2379-190X","978-1-4799-9988-0","10.1109/ICASSP.2016.7472826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472826","speech recognition;spoken translations;language model adaptation;phrase-based machine translation;named entities","Adaptation models;Computational modeling;Speech recognition;Context;Estimation;Speech;Computational complexity","estimation theory;probability;speech recognition","language model adaptation;ASR;spoken translation;machine translation system;automatic speech recognition;MT system;word-based translation model;phrase-based translation model;named entity probability estimation;WER;word-based LM adaptation technique;unadapted 3-gram baseline;English-to-Dutch dataset","","2","","20","","19 May 2016","","","IEEE","IEEE Conferences"
"Improving language models for ASR using translated in-domain data","S. Kombrink; T. Mikolov; M. Karafiát; L. Burget","Brno University of Technology, Czech; Brno University of Technology, Czech; Brno University of Technology, Czech; Brno University of Technology, Czech","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4405","4408","Acquisition of in-domain training data to build speech recognition systems for under-resourced languages can be a costly, time-demanding and tedious process. In this work, we propose the use of machine translation to translate English transcripts of telephone speech into Czech language in order to improve a Czech CTS speech recognition system. The translated transcripts are used as additional language model training data in a scenario where the baseline language model is trained on off- and close-domain data only. We report perplexities, OOV and word error rates and examine different data sets and translators on their suitability for the described task.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6288896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6288896","Low Resource ASR;Language Modeling;Machine Translation","Data models;Speech;Dictionaries;Google;Speech recognition;Acoustics;Decoding","language translation;natural language processing;speech recognition","language models;ASR;in-domain training data;speech recognition systems;under-resourced languages;machine translation;English transcripts;telephone speech;Czech language","","","","6","","30 Aug 2012","","","IEEE","IEEE Conferences"
"Speech Translation Enhanced ASR for European Parliament Speeches - On the Influence of ASR Performance on Speech Translation","S. Stuker; M. Paulik; M. Kolss; C. Fugen; A. Waibel","Institut für Theoretische Informatik, Universiät Karlsruhe (TH), Karlsruhe, Germany. stueker@ira.uka.de; Institut für Theoretische Informatik, Universiät Karlsruhe (TH), Karlsruhe, Germany; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, USA. paulik@ira.uka.de; Institut für Theoretische Informatik, Universiät Karlsruhe (TH), Karlsruhe, Germany. kolss@ira.uka.de; Institut für Theoretische Informatik, Universiät Karlsruhe (TH), Karlsruhe, Germany; Institut für Theoretische Informatik, Universiät Karlsruhe (TH), Karlsruhe, Germany; Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, USA. waibel@ira.uka.de","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1293","IV-1296","In this paper we describe our work in coupling automatic speech recognition (ASR) and machine translation (MT) in a speech translation enhanced automatic speech recognition (STE-ASR) framework for transcribing and translating European parliament speeches. We demonstrate the influence of the quality of the ASR component on the MT performance, by comparing a series of WERs with the corresponding automatic translation scores. By porting an STE-ASR framework to the task at hand, we show how the word errors for transcribing English and Spanish speeches can be lowered by 3.0% and 4.8% relative, respectively.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218345","Speech Recognition;Machine Translation;European Parliamentary Plenary Sessions;TC-STAR;STE-ASR","Speech enhancement;Automatic speech recognition;Natural languages;Speech recognition;Speech synthesis;Interactive systems;Laboratories;Broadcasting;TV;Europe","language translation;speech recognition","speech translation enhanced ASR;European parliament speeches;automatic speech recognition;machine translation;automatic translation scores;word errors;Spanish speeches","","3","","15","","4 Jun 2007","","","IEEE","IEEE Conferences"
"Use of statistical N-gram models in natural language generation for machine translation","Fu-Hua Liu; Liang Gu; Yuqing Gao; M. Picheny","IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA","2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).","21 May 2003","2003","1","","I","I","Various language modeling issues in a speech-to-speech translation system are described in this paper. First, the language models for the speech recognizer need to be adapted to the specific domain to improve the recognition performance for in-domain utterances, while keeping the domain coverage as broad as possible. Second, when a maximum entropy based statistical natural language generation model is used to generate target language sentence as the translation output, serious inflection and synonym issues arise, because the compromised solution is used in semantic representation to avoid the data sparseness problem. We use N-gram models as a postprocessing step to enhance the generation performance. When an interpolated language model is applied to a Chinese-to-English translation task, the translation performance, measured by an objective metric of BLEU, improves substantially to 0.514 from 0.318 when we use the correct transcription as input. Similarly, the BLEU score is improved to 0.300 from 0.194 for the same task when the input is speech data.","1520-6149","0-7803-7663-3","10.1109/ICASSP.2003.1198861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1198861","","Natural languages;Speech recognition;Speech synthesis;Entropy;Data mining;Internet;Globalization;Switches;Scheduling;Speech processing","speech recognition;language translation;statistical analysis;natural language interfaces;maximum entropy methods;speech processing","statistical N-gram models;natural language generation;machine translation;language modeling;speech-to-speech translation system;language models;speech recognizer;recognition performance;in-domain utterances;maximum entropy;target language sentence;postprocessing step;interpolated language model;Chinese-to-English translation task;BLEU;inflection;synonym;semantic representation","","2","","11","","21 May 2003","","","IEEE","IEEE Conferences"
"System based on machine vision for translation of fingerspelling alphabet to latin alphabet","D. Lancheros-Cuesta; C. S. Schlenker; L. F. Morales","Universidad de La Salle, Ingeniería en Automatización, Bogotá, Colombia; Universidad de La Salle, Ingeniería en Automatización, Bogotá, Colombia; Universidad de La Salle, Ingeniería en Automatización, Bogotá, Colombia","2015 10th Iberian Conference on Information Systems and Technologies (CISTI)","30 Jul 2015","2015","","","1","5","On the nowadays society exist a lot of communication problems, particularly when the persons has sensory disabilities as deafness or blindness. This problem take place at the moment of interpreting the sign language. The present paper shows the development of a current research project that integrates an intelligent system in the recognition of images and its reproduction in hardware interpretations ends. For those purposes, a system of image acquisition with a webcam and an interface was implemented in Matlab, through which the video was displayed in real time, the image of the point gained, together with the translation of Colombian sign language. This system was trained to recognize 4-letter alphabet, obtaining an average error of 2%, concluding that such application was effective for translating letters acquired both the right hand or the left; similarly concluded that the type of radial neural network proves to be very useful for this type of operation and the higher classification have this training, the results will be more accurate cast. Finally it is important to note that the system integrates hardware with Arduino system that displays real-time translation in this system was trained to recognize 4 letters of the alphabet, giving an average error of 2%, the same way the PNN neural network proves to be very useful for this type of sorting operations.","2166-0727","978-9-8998-4345-5","10.1109/CISTI.2015.7170450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170450","Neuronal Networks;Artificial Vision;Smart Recognition;Image Treatment","MATLAB;Biological neural networks;Training;Assistive technology;Image recognition;Gesture recognition","computer vision;handicapped aids;image sensors;language translation;neural nets;sign language recognition","machine vision;fingerspelling alphabet translation;Latin alphabet;communication problems;sensory disabilities;deafness;blindness;images recognition;hardware interpretations;image acquisition;webcam;Matlab;Colombian sign language translation;4-letter alphabet;radial neural network;Arduino system;real-time translation;PNN neural network;sorting operations","","","","6","","30 Jul 2015","","","IEEE","IEEE Conferences"
"Automatic Stopword Detection Using Term Ranking between Written and Machine Speech Recognition Transcribed Reviews","J. J. Hind; M. Mahyoub; D. Woods; C. Wong; A. Hussain; D. Al-Jumeily",LivingLens; LivingLens; LivingLens; LivingLens; Liverpool John Moores University; Liverpool John Moores University,"2019 12th International Conference on Developments in eSystems Engineering (DeSE)","23 Apr 2020","2019","","","301","308","Video feedback and machine speech recognition are fast-becoming a popular choice for companies to gain insight into their products. In conjunction with this, text analytics can be used to extract insight from these video translations. Currently, there is little work in the area to analyse and compare techniques for natural language processing, information retrieval and information extraction. A commonly practiced technique in text analytics is the extraction of stop words; words whose presence do not contribute context or information to a document. In this paper, we explore statistical techniques for the automated extraction of stop words, comparing 4 datasets from written and translated reviews. Using statistical variations of the successful technique `term ranking', we evaluate their performance using a common list of stop words. Results suggest that variation, TFnormIDFnorm, was the most successful with a best performing precision rate of 46.7% and a recall rate of 86.6%. The best results were seen in the largest dataset using written reviews, however comparison of the remaining 3 datasets revealed that spoken text performed 0.4% better in precision than the next best dataset and 2.6% better in recall. Initial results show marginally better performance in machine speech recognition transcribed texts from videos in comparison to comparably size datasets of written reviews.","2161-1351","978-1-7281-3021-7","10.1109/DeSE.2019.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9073508","Term Ranking;TFIDF;Machine Speech Recognition (MSR);stop words;marketing;reviews.","Ranking (statistics);Speech recognition;Measurement;Text mining;Dictionaries","information retrieval;natural language processing;speech recognition;statistical analysis;text analysis","written translated reviews;statistical variations;spoken text;automatic Stopword detection;transcribed reviews;video feedback;text analytics;video translations;natural language processing;information retrieval;information extraction;statistical techniques;term ranking;machine speech recognition transcribed reviews","","","","20","","23 Apr 2020","","","IEEE","IEEE Conferences"
"Measuring human readability of machine generated text: three case studies in speech recognition and machine translation","D. Jones; E. Gibson; W. Shen; N. Granoien; M. Herzog; D. Reynolds; C. Weinstein","Lincoln Lab., MIT, Lexington, MA, USA; NA; NA; NA; NA; NA; NA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","5","","v/1009","v/1012 Vol. 5","We present highlights from three experiments that test the readability of current state-of-the art system output from: (1) an automated English speech-to-text (SST) system; (2) a text-based Arabic-to-English machine translation (MT) system; and (3) an audio-based Arabic-to-English MT process. We measure readability in terms of reaction time and passage comprehension in each case, applying standard psycholinguistic testing procedures and a modified version of the standard defense language proficiency test for Arabic called the DLPT*. We learned that: (1) subjects are slowed down by about 25% when reading system STT output; (2) text-based MT systems enable an English speaker to pass Arabic Level 2 on the DLPT*; and (3) audio-based MT systems do not enable English speakers to pass Arabic Level 2. We intend for these generic measures of readability to predict performance of more application-specific tasks.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1416477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1416477","","Anthropometry;Humans;Speech recognition;Measurement standards;Automatic testing;System testing;Art;Time measurement;Psychology;Natural languages","speech recognition;language translation;linguistics","defense language proficiency test;machine generated text human readability measurement;speech recognition;automated English speech-to-text system;text-based Arabic-to-English machine translation;audio-based Arabic-to-English MT process;reaction time;passage comprehension;psycholinguistic testing procedures","","5","","7","","9 May 2005","","","IEEE","IEEE Conferences"
"Automatic text recognition in natural scene and its translation into user defined language","D. C. Bijalwan; A. Aggarwal","TehriGarhwal, Uttrakhand; Deptt. of CSE, JPIET, Meerut, India","2014 International Conference on Parallel, Distributed and Grid Computing","5 Feb 2015","2014","","","324","329","In recent year's availability of economical image capturing devices in low cost products like mobile phones has led a significant attention of researchers to the problem of recognizing text in images. Recognition of scene text is a challenging problem compared to the recognition of printed documents. In this work a novel approach is proposed to recognize text in complex background natural scene, word formation from recognized text, spelling checking and word translation into user defined language and finally overlay translated word onto the image. The proposed approach is robust to different kinds of text appearances, including font size, font style, color, and background. Combining the respective strengths of different complementary techniques and overcoming their shortcomings, the proposed method uses efficient character detection and localization technique and multiclass classifier to recognize the text accurately. The proposed approach successfully recognizes text on natural scene images and does not depend on a particular alphabet, text background. It works with a wide variety in size of characters and can handle up to 20 degree skewness efficiently.","","978-1-4799-7683-6","10.1109/PDGC.2014.7030764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030764","text detection;text localization;binarization;segmentation;fuzzy logic","Text recognition;Feature extraction;Character recognition;Image recognition;Support vector machines;Vectors;Noise","image classification;language translation;text detection","multiclass classifier;localization technique;character detection;word translation;spelling checking;complex background natural scene;scene text recognition;mobile phones;low cost products;economical image capturing devices;user defined language translation;automatic text recognition","","1","1","13","","5 Feb 2015","","","IEEE","IEEE Conferences"
"Automatic Lecture Subtitle Generation and How It Helps","X. Che; S. Luo; H. Yang; C. Meinel","Hasso Plattner Inst., Potsdam, Germany; Hasso Plattner Inst., Potsdam, Germany; Hasso Plattner Inst., Potsdam, Germany; Hasso Plattner Inst., Potsdam, Germany","2017 IEEE 17th International Conference on Advanced Learning Technologies (ICALT)","7 Aug 2017","2017","","","34","38","In this paper we propose an integrated framework of automatic bilingual subtitle generation for lecture videos, especially for MOOCs. The framework consists of Automatic Speech Recognition (ASR), Sentence Boundary Detection (SBD), and Machine Translation (MT). Then we quantitatively evaluate the auto-generated subtitles, the manually produced subtitles from scratch, and the auto-generated subtitles with manual modification in term of accuracy and time expenditure, in both original and target languages. The result shows that the auto-generated subtitles in the original language (English) are fairly accurate already. By using them as the draft, human subtitle producers can save 54% of the working time and simultaneously reduce the error rate by 54.3%, which is a significant improvement. However, the effectiveness of machine translated subtitles (English to Chinese) is limited. In the end, if the proposed framework is applied, the total working time in preparing bilingual subtitles can be shortened by approximately 1/3, with no decline in quality.","2161-377X","978-1-5386-3870-5","10.1109/ICALT.2017.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001709","Automatic Subtitling;Sentence Boundary Detection;Lecture Videos;MOOC","Videos;Manuals;Acoustics;Mathematical model;Data models;Automatic speech recognition;Electronic learning","computer aided instruction;language translation;natural language processing;speech recognition","automatic lecture subtitle generation;integrated framework;automatic bilingual subtitle generation;lecture videos;MOOC;automatic speech recognition;ASR;sentence boundary detection;SBD;machine translation;MT;autogenerated subtitle generation;original languages;target languages;human subtitle producers;English-Chinese translation","","3","","24","","7 Aug 2017","","","IEEE","IEEE Conferences"
"From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation","C. Chen; T. Su; G. Meng; Z. Xing; Y. Liu","Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore; Res. Sch. of Comput. Sciecne, Australian Nat. Univ., Canberra, ACT, Australia; Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore","2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)","2 Sep 2018","2018","","","665","676","A GUI skeleton is the starting point for implementing a UI design image. To obtain a GUI skeleton from a UI design image, developers have to visually understand UI elements and their spatial layout in the image, and then translate this understanding into proper GUI components and their compositions. Automating this visual understanding and translation would be beneficial for bootstraping mobile GUI implementation, but it is a challenging task due to the diversity of UI designs and the complexity of GUI skeletons to generate. Existing tools are rigid as they depend on heuristically-designed visual understanding and GUI generation rules. In this paper, we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a UI design image into a GUI skeleton. Our translator learns to extract visual features in UI images, encode these features' spatial layouts, and generate GUI skeletons in a unified neural network framework, without requiring manual rule development. For training our translator, we develop an automated GUI exploration method to automatically collect large-scale UI data from real-world applications. We carry out extensive experiments to evaluate the accuracy, generality and usefulness of our approach.","1558-1225","978-1-4503-5638-1","10.1145/3180155.3180240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453135","User interface;reverse engineering;deep learning","Graphical user interfaces;Skeleton;Layout;Visualization;Feature extraction;Tools;Task analysis","computer bootstrapping;computer vision;graphical user interfaces;language translation;mobile computing;neural nets","large-scale UI data;GUI exploration method;GUI components;computer vision;GUI generation rules;heuristically-designed visual understanding;bootstrap mobile GUI implementation;neural machine translator;GUI skeleton;UI design image","","19","","","","2 Sep 2018","","","IEEE","IEEE Conferences"
"Pattern matching for automatic sign language translation system using LabVIEW","A. Domingo; R. Akmeliawati; K. Y. Chow","School of Electrical and Computer Systems Engineering, Monash University Malaysia, No. 2, Jalan Kolej, Bandar Sunway, 46150 Petaling Jaya, Selangor D.E., Malaysia; School of Electrical and Computer Systems Engineering, Monash University Malaysia, No. 2, Jalan Kolej, Bandar Sunway, 46150 Petaling Jaya, Selangor D.E., Malaysia; School of Electrical and Computer Systems Engineering, Monash University Malaysia, No. 2, Jalan Kolej, Bandar Sunway, 46150 Petaling Jaya, Selangor D.E., Malaysia","2007 International Conference on Intelligent and Advanced Systems","24 Oct 2008","2007","","","660","665","This paper presents an automatic sign language translator, which is able to translate Malaysian sign language using pattern-matching algorithm. The sign language translator is a vision-based system where the image of the sign is captured by a camera, processed and translated into English by the computer. This sign language translator is able to recognize alphabets (A-Z), numbers (0-9), finger spelling, words (13 words) and sentences. Alphabets, numbers and fingers are categorized under static signs while words and sentences are known as dynamic signs where the signs involve motion. Static signs are recognised by matching positions of each fingertip with the database while the recognition of dynamic signs is performed by comparing the trajectory of the motion. The accuracy for static sign is 97.79% while the accuracy for dynamic sign is 80.38%.","","978-1-4244-1355-3","10.1109/ICIAS.2007.4658470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658470","Image processing;LabVIEW;pattern matching;sign language","Image color analysis;Handicapped aids;Feature extraction;Fingers;Image processing;Trajectory;Databases","computer vision;data acquisition;image matching;language translation;natural language processing;virtual instrumentation","automatic Malaysian sign language translation system;LabView;pattern matching algorithm;vision-based system;camera","","4","","6","","24 Oct 2008","","","IEEE","IEEE Conferences"
"Speech-to-speech translation humanoid robot in doctor's office","S. Shin; E. T. Matson; Jinok Park; Bowon Yang; Juhee Lee; Jin-Woo Jung","M2M Lab, Computer and Information Technology, Purdue University, West Lafayette, IN, USA; M2M Lab, Computer and Information Technology, Purdue University, West Lafayette, IN, USA; Computer Science and Engineering, Dongguk University, Seoul, Republic of Korea; Computer Science and Engineering, Dongguk University, Seoul, Republic of Korea; Computer Science and Engineering, Dongguk University, Seoul, Republic of Korea; Computer Science and Engineering, Dongguk University, Seoul, Republic of Korea","2015 6th International Conference on Automation, Robotics and Applications (ICARA)","9 Apr 2015","2015","","","484","489","This paper illustrates the implementation of a speech-to-speech translation humanoid robot in the domain of medical care. At this stage, the proposed system is a one-way translation that is designed to help English speaking patients describe their symptoms to Korean doctors or nurses. A humanoid robot is useful because it can be extended to reach out to people in need first and may substitute the role of human workers, unlike laptops or tablets. The system consists of three main parts - speech recognition, English-Korean translation, and Korean speech generation. It utilizes CMU Sphinx-4 as a speech recognition tool. English-Korean translation in this system is based on the rule-based translation. The success rate of the translation shows reliable results from an experiment with a closed scenario.","","978-1-4799-6466-6","10.1109/ICARA.2015.7081196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081196","Speech-to-Speech Translation;Humanoid Robot;Medical Service Robot;Machine Translation","Speech recognition;Speech;Grammar;Humanoid robots;Hospitals","health care;humanoid robots;language translation;mobile robots;speech recognition","speech-to-speech translation humanoid robot;medical care;English speaking patients;Korean doctors;nurses;English-Korean translation;Korean speech generation;CMU Sphinx-4;speech recognition tool;rule-based translation","","2","","17","","9 Apr 2015","","","IEEE","IEEE Conferences"
"Why word error rate is not a good metric for speech recognizer training for the speech translation task?","X. He; L. Deng; A. Acero","Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","5632","5635","Speech translation (ST) is an enabling technology for cross-lingual oral communication. A ST system consists of two major components: an automatic speech recognizer (ASR) and a machine translator (MT). Nowadays, most ASR systems are trained and tuned by minimizing word error rate (WER). However, WER counts word errors at the surface level. It does not consider the contextual and syntactic roles of a word, which are often critical for MT. In the end-to-end ST scenarios, whether WER is a good metric for the ASR component of the full ST system is an open issue and lacks systematic studies. In this paper, we report our recent investigation on this issue, focusing on the interactions of ASR and MT in a ST system. We show that BLEU-oriented global optimization of ASR system parameters improves the translation quality by an absolute 1.5% BLEU score, while sacrificing WER over the conventional, WER-optimized ASR system. We also conducted an in-depth study on the impact of ASR errors on the final ST output. Our findings suggest that the speech recognizer component of the full ST system should be optimized by translation metrics instead of the traditional WER.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947637","Speech translation;speech recognition;machine translation;translation metric;word error rate;BLEU score optimization;log-linear model","Speech recognition;Speech;Hidden Markov models;Measurement;Computational modeling;Training;Optimization","language translation;optimisation;speech recognition;training","word error rate;speech recognizer training;speech translation;crosslingual oral communication;ST system;machine translator;end-to-end ST scenario;ASR component;BLEU oriented global optimization;WER optimized ASR system","","16","","20","","11 Jul 2011","","","IEEE","IEEE Conferences"
"A Single-class Support Vector Machine Translation Algorithm To Compensate For Non-stationary Data In Heterogeneous Vision-based Sensor Networks","J. Rhinelander; P. X. Liu","Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, Ontario, Canada, K1S 5B6. Email: jasonr@sce.carleton.ca; Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, Ontario, Canada, K1S 5B6. Email: xpliu@sce.carleton.ca","2008 IEEE Instrumentation and Measurement Technology Conference","20 Jun 2008","2008","","","1102","1106","This paper develops a translation algorithm that adapts an existing support vector machine (SVM) to observations that have a different probability distribution than originally trained with. The primary advantage of this algorithm is that the re-training can be avoided. The support vector translation algorithm can be used in a fully distributed vision-based sensor network for target classification and tracking. Preliminary results are discussed and planned future work is briefly outlined.","1091-5281","978-1-4244-1540-3","10.1109/IMTC.2008.4547203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4547203","Support vector machine;pattern recognition;machine-learning;heterogeneous vision-based sensor network","Support vector machines;Support vector machine classification;Cameras;Quadratic programming;Intelligent sensors;Equations;Sensor systems;Distributed computing;Layout;Computational intelligence","image classification;image sensors;probability;support vector machines","single-class support vector machine translation algorithm;nonstationary data;heterogeneous vision-based sensor networks;probability distribution;pattern recognition;machine learning","","2","1","8","","20 Jun 2008","","","IEEE","IEEE Conferences"
"Deep Learning Methods for Image Segmentation Containing Translucent Overlapped Objects","T. L. Mahyari; R. M. Dansereau","Carleton University,Department of Systems and Computer Engineering,Ottawa,Canada; Carleton University,Department of Systems and Computer Engineering,Ottawa,Canada","2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","28 Jan 2020","2019","","","1","5","Convolutional neural networks(CNN) are a subset of deep learning methods recently used widely for image segmentation. SegNet network [4] has shown interesting results for semantic segmentation, but it is designed to segment images with non-overlapped objects. However in some data translucent regions partially overlap. Having overlapped regions will cause methods not designed for overlapped objects to perform poorly or not work at all. To our knowledge no CNN has been designed yet to segment partially overlapped translucent objects.In this paper, we have designed a CNN to segment partially overlapped translucent regions. We used SegNet [4] as transfer learning for our overlapped image segmentation method. We also designed a new CNN with a simpler network for our data. Results on synthetic images give more than 95% segmentation accuracy for both methods.","","978-1-7281-2723-1","10.1109/GlobalSIP45357.2019.8969558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8969558","Machine learning;deep learning;image segmentation;translucent overlapped images.","","convolutional neural nets;image segmentation;learning (artificial intelligence)","deep learning methods;SegNet network;semantic segmentation;nonoverlapped objects;data translucent regions;overlapped regions;translucent objects;transfer learning;overlapped image segmentation method;synthetic images;translucent overlapped objects;convolutional neural networks;CNN;partially overlapped translucent region segmentation","","1","","23","","28 Jan 2020","","","IEEE","IEEE Conferences"
"Consolidation based speech translation","Chiori Hori; Bing Zhao; S. Vogel; A. Waibel","NiCT-ATR 2-2-2 Hikaridai, Soraku-gun, Kyoto, 619-0288 Japan; Carnegie Mellon University, 407 S. Craig St., Pittsburgh, PA 15213, USA; Carnegie Mellon University, 407 S. Craig St., Pittsburgh, PA 15213, USA; Carnegie Mellon University, 407 S. Craig St., Pittsburgh, PA 15213, USA","2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)","14 Jan 2008","2007","","","380","385","To alleviate the degradation of the performance of speech translation, this paper proposes a new approach to translate ASR results through consolidation which extracts meaningful phrases and remove redundant and irrelevant information caused by speaker's disfluency and recognition errors. The speech translation results via consolidation are partial translation and can not be directly compared with gold standards in which all words are translated. We would like to propose a new evaluation framework for partial translation by comparing with the most similar set of words extracted from a word network created by merging gradual summarizations of the gold standard translation. Chinese broadcast news speech in RT04 were recognized, consolidated and then translated. The performance of MT results was evaluated using BLEU. We propose information preservation accuracy (IPAccy) and meaning preservation accuracy (MPAccy) for consolidation and consolidation-based MT.","","978-1-4244-1745-2","10.1109/ASRU.2007.4430142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4430142","Speech Consolidation;Machine translation;Chinese broadcast news speech;Chinese-English translation","Automatic speech recognition;Data mining;Speech recognition;Natural languages;Degradation;Gold;Broadcasting;Databases;Text recognition;Speech analysis","language translation;natural languages;speech processing;speech recognition","speech translation;ASR results;phrases extraction;speaker disfluency;recognition errors;partial translation;words extraction;word network;gradual summarizations;Chinese broadcast news speech;information preservation accuracy;meaning preservation accuracy","","","","6","","14 Jan 2008","","","IEEE","IEEE Conferences"
"On Efficient Coupling of ASR and SMT for Speech Translation","B. Zhou; L. Besacier; Y. Gao","IBM T. J. Watson Research Center, Yorktown Heights, NY 10598; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-101","IV-104","This paper presents an efficient tightly integrated approach for improved speech translation performance. The proposed approach combines the automatic speech recognition (ASR) and statistical machine translation (SMT) components in a bi-directional fashion. First, our SMT decoder takes the speech recognition lattice to perform an integrated search for the optimal translation by combining various ASR scores and translation models. Our approach is implemented within the recently proposed Folsom SMT framework that employs a multilayer search algorithm to conduct efficient operations on multiple graphs, which not only achieves memory efficiency and fast speed that is critical for real time speech translation applications, but also provides significant accuracy improvements. Secondly, we also report our experiments where the ASR is customized by reinforcing the language model to favor downstream translation component. We evaluated our approach on a large vocabulary speech translation task, and we obtain more than 2 point BLEU improvement over standard cascaded 1-best speech translation.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218047","Integrated speech translation;coupling;speech recognition;machine translation;efficient search","Automatic speech recognition;Surface-mount technology;Lattices;Vocabulary;Speech analysis;Natural languages;Decoding;Speech recognition;Bidirectional control;Nonhomogeneous media","decoding;language translation;search problems;speech coding;speech recognition","integrated speech translation;SMT;ASR;automatic speech recognition;statistical machine translation;SMT decoder;multilayer search algorithm;BLEU","","11","","10","","4 Jun 2007","","","IEEE","IEEE Conferences"
"Computer-assisted translation using speech recognition","E. Vidal; F. Casacuberta; L. Rodriguez; J. Civera; C. D. M. Hinarejos","Dept. de Sistemas Informaticos y Computacion, Univ. Politecnica de Valencia, Spain; Dept. de Sistemas Informaticos y Computacion, Univ. Politecnica de Valencia, Spain; Dept. de Sistemas Informaticos y Computacion, Univ. Politecnica de Valencia, Spain; Dept. de Sistemas Informaticos y Computacion, Univ. Politecnica de Valencia, Spain; Dept. de Sistemas Informaticos y Computacion, Univ. Politecnica de Valencia, Spain","IEEE Transactions on Audio, Speech, and Language Processing","18 Apr 2006","2006","14","3","941","951","Current machine translation systems are far from being perfect. However, such systems can be used in computer-assisted translation to increase the productivity of the (human) translation process. The idea is to use a text-to-text translation system to produce portions of target language text that can be accepted or amended by a human translator using text or speech. These user-validated portions are then used by the text-to-text translation system to produce further, hopefully improved suggestions. There are different alternatives of using speech in a computer-assisted translation system: From pure dictated translation to simple determination of acceptable partial translations by reading parts of the suggestions made by the system. In all the cases, information from the text to be translated can be used to constrain the speech decoding search space. While pure dictation seems to be among the most attractive settings, unfortunately perfect speech decoding does not seem possible with the current speech processing technology and human error-correcting would still be required. Therefore, approaches that allow for higher speech recognition accuracy by using increasingly constrained models in the speech recognition process are explored here. All these approaches are presented under the statistical framework. Empirical results support the potential usefulness of using speech within the computer-assisted translation paradigm.","1558-7924","","10.1109/TSA.2005.857788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1621206","Computer-assisted translation (CAT);speech recognition;statistical machine translation","Speech recognition;Humans;Speech processing;Productivity;Natural languages;Decoding;Space technology;Computer errors;Error correction","language translation;speech recognition","computer-assisted translation;speech recognition;text-to-text translation;statistical framework","","14","1","37","","18 Apr 2006","","","IEEE","IEEE Journals"
"Multimodal fusion of brain structural and functional imaging with a deep neural machine translation approach","M. F. Amin; S. M. Plis; E. Damaraju; D. Hjelm; K. Cho; V. D. Calhoun","The Mind Research Network, 1101 Yale Blvd, Albuquerque, NM 87106, USA; The Mind Research Network, 1101 Yale Blvd, Albuquerque, NM 87106, USA; The Mind Research Network, 1101 Yale Blvd, Albuquerque, NM 87106, USA; The Mind Research Network, 1101 Yale Blvd, Albuquerque, NM 87106, USA; Courant Institute & Center for Data Science, New York University, 10012, USA; The Mind Research Network, 1101 Yale Blvd, Albuquerque, NM 87106, USA","2016 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)","28 Apr 2016","2016","","","1","4","In this work, we study a novel approach of deep neural machine translation to find linkage between multimodal brain imaging data, such as structural MRI (sMRI) and functional MRI (fMRI). The idea is to consider two different imaging views of the same brain like two different languages conveying some common concepts or facts. An important aspect of the translation model is an attention network module that learns alignment between features from fMRI and sMRI. We use independent component analysis (ICA) based features for the translation model. Our study shows significant group differences between healthy controls and patients with schizophrenia in the learned alignments. Furthermore, this novel approach reveals a group differential relation between a cognitive score (attention and vigilance) and alignments that could not be found when individual modality of data were considered.","","978-1-4673-9919-7","10.1109/SSIAI.2016.7459160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459160","","Correlation;Brain modeling;Magnetic resonance imaging;Feedforward neural networks;Loading","biomedical MRI;image fusion;independent component analysis;language translation;medical image processing;neural nets","multimodal fusion;brain structural imaging;brain functional imaging;deep neural machine translation approach;sMRI;fMRI;attention network module;independent component analysis;ICA based features;patients with schizophrenia;healthy patients;cognitive score;data modality;magnetic resonance imaging","","","","10","","28 Apr 2016","","","IEEE","IEEE Conferences"
"Automatic speech translation based on the semantic structure","J. Muller; H. Stahl; M. Lang","Inst. for Human-Machine-Commun., Munchen Univ. of Technol., Germany; NA; NA","Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP '96","6 Aug 2002","1996","2","","658","661 vol.2","Describes a system for the semantic-based translation of spoken or written limited-domain utterances. The semantic structure, as the output of a semantic decoder, serves as the interlingua level. A word-chain generator combined with a linguistic post-processor produces the corresponding word chain in the target language. Both the semantic decoder and the word chain generator work with pure stochastic and trainable knowledge bases. The grammatical features of certain words can be easily extracted with the aid of both the word chain and the semantic structure.","","0-7803-3555-4","10.1109/ICSLP.1996.607447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=607447","","Decoding;Natural languages;Speech synthesis;Tin;Stochastic processes;Logic;Graphics","language translation;speech recognition;natural languages;decoding;linguistics","automatic speech translation;semantic structure;spoken utterances;written utterances;limited-domain utterances;semantic decoder;interlingua level;word-chain generator;linguistic post-processor;stochastic knowledge bases;trainable knowledge bases;grammatical feature extraction;speech understanding;language production;semantic model;syntactic model;inflectional model","","2","1","15","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Time Series Neural Networks for Real Time Sign Language Translation","S. S Kumar; T. Wangyal; V. Saboo; R. Srinath","Comput. Sci. & Eng. Dept., PES Inst. of Technol., Bangalore, India; Comput. Sci. & Eng. Dept., PES Inst. of Technol., Bangalore, India; Comput. Sci. & Eng. Dept., PES Inst. of Technol., Bangalore, India; Comput. Sci. & Eng. Dept., PES Inst. of Technol., Bangalore, India","2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)","17 Jan 2019","2018","","","243","248","Sign language is the primary mode of communication for the hearing and speech impaired and there is a need for systems to translate sign languages to spoken languages. Prior research has been focused on providing glove based solutions which are intrusive and expensive. We propose a sign language translation system based solely on visual cues and deep learning for accurate translation. Our system applies Computer Vision and Neural Machine Translation for American Sign Language (ASL) gloss recognition and translation respectively. In this paper, we show that an end to end neural network system is not only capable of recognition of individual ASL glosses but also translation of continuous sign language videos into complete English sentences, making it an effective and practical tool for sign language communication.","","978-1-5386-6805-4","10.1109/ICMLA.2018.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614068","Sign Language Translation, Computer Vision, Long Short Term Memory Networks, Neural Machine Translation, Attention Neural Networks, Deep Neural Networks, American Sign Language","Assistive technology;Gesture recognition;Videos;Recurrent neural networks;Time series analysis;Shape","computer vision;handicapped aids;language translation;learning (artificial intelligence);natural language processing;neural nets;real-time systems;sign language recognition;time series;video signal processing","time series neural networks;Neural Machine Translation;American Sign Language gloss recognition;hearing impaired;speech impaired;end to end neural network system;real time sign language translation system;ASL gloss recognition;computer vision;sign language communication;continuous sign language videos","","1","","23","","17 Jan 2019","","","IEEE","IEEE Conferences"
"Some insights from translating conversational telephone speech","G. Kumar; M. Post; D. Povey; S. Khudanpur","Center for Language and Speech Processing & Human Language Technology Center of Excellence The Johns Hopkins University, Baltimore, MD 21218, USA; Center for Language and Speech Processing & Human Language Technology Center of Excellence The Johns Hopkins University, Baltimore, MD 21218, USA; Center for Language and Speech Processing & Human Language Technology Center of Excellence The Johns Hopkins University, Baltimore, MD 21218, USA; Center for Language and Speech Processing & Human Language Technology Center of Excellence The Johns Hopkins University, Baltimore, MD 21218, USA","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","14 Jul 2014","2014","","","3231","3235","We report insights from translating Spanish conversational telephone speech into English text by cascading an automatic speech recognition (ASR) system with a statistical machine translation (SMT) system. The key new insight is that the informal register of conversational speech is a greater challenge for ASR than for SMT: the BLEU score for translating the reference transcript is 64%, but drops to 32% for translating automatic transcripts, whose word error rate (WER) is 40%. Several strategies are examined to mitigate the impact of ASR errors on the SMT output: (i) providing the ASR lattice, instead of the 1-best output, as input to the SMT system, (ii) training the SMT system on Spanish ASR output paired with English text, instead of Spanish reference transcripts, and (iii) improving the core ASR system. Each leads to consistent and complementary improvements in the SMT output. Compared to translating the 1-best output of an ASR system with 40% WER using an SMT system trained on Spanish reference transcripts, translating the output lattice of a better ASR system with 35% WER using an SMT system trained on ASR output improves BLEU from 32% to 38%.","2379-190X","978-1-4799-2893-4","10.1109/ICASSP.2014.6854197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6854197","Speech Recognition;Natural Language Processing;Machine Translation;Human Language Technology;Spoken Language Translation","Speech;Lattices;Training;Speech recognition;Conferences;Acoustics;Speech processing","error analysis;language translation;speech recognition","Spanish conversational telephone speech translation;English text;automatic speech recognition system;statistical machine translation system;SMT system;informal register;BLEU score;automatic transcripts;word error rate;WER;ASR errors;SMT output;ASR lattice;Spanish reference transcripts;core ASR system","","3","","20","","14 Jul 2014","","","IEEE","IEEE Conferences"
"Arabic ASR and MT Integration for GALE","Y. Al-Onaizan; L. Mangu","IBM T.J. Watson Research Center, 1101 Kitchawan Road, Route 134, Yorktown Heights, NY 10523; IBM T.J. Watson Research Center, 1101 Kitchawan Road, Route 134, Yorktown Heights, NY 10523","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1285","IV-1288","In this paper we describe our work in machine translation of Arabic speech into English. This work was done within the context of the GALE research program. We describe several integration techniques between our ASR and MT system. Our initial results suggest that tighter coupling between ASR and MT system improves the translation quality of speech input. We explore the effect of each integration technique on the overall system.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218343","Speech Translation;Statistical Machine Translation;Arabic Spoken Language Translation;GALE Translation System","Automatic speech recognition;Surface-mount technology;Natural languages;Vocabulary;Speech recognition;Broadcasting;Gaussian processes;Decoding;Cost function;Europe","language translation;speech recognition","Arabic ASR-MT integration;GALE;machine translation;Arabic speech;translation quality;automatic speech recognition","","4","","6","","4 Jun 2007","","","IEEE","IEEE Conferences"
"Feedforward sequential memory networks based encoder-decoder model for machine translation","J. Hou; S. Zhang; L. Dai; H. Jiang","National Engineering Laboratory for Speech and Language Information Processing University of Science and Technology of China, Hefei, Anhui, P. R. China; National Engineering Laboratory for Speech and Language Information Processing University of Science and Technology of China, Hefei, Anhui, P. R. China; National Engineering Laboratory for Speech and Language Information Processing University of Science and Technology of China, Hefei, Anhui, P. R. China; Department of Electrical Engineering and Computer Science, York University, Toronto, Ontario, Canada","2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","8 Feb 2018","2017","","","622","625","Recently recurrent neural networks based encoder-decoder model is a popular approach to sequence to sequence mapping problems, such as machine translation. However, it is time-consuming to train the model since symbols in a sequence can not be processed parallelly by recurrent neural networks because of the temporal dependency restriction. In this paper we present a sequence to sequence model by replacing the recurrent neural networks with feedforward sequential memory networks in both encoder and decoder, which enables the new architecture to encode the entire source sentence simultaneously. We also modify the attention module to make the decoder generate outputs simultaneously during training. We achieve comparable results in WMT'14 English-to-French translation task with 1.4 to 2 times faster during training because of temporal independency in feedforward sequential memory networks based encoder and decoder.","","978-1-5386-1542-3","10.1109/APSIPA.2017.8282100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8282100","","Decoding;Mathematical model;Training;Feedforward neural networks;Computational modeling;Recurrent neural networks;Task analysis","decoding;encoding;language translation;learning (artificial intelligence);natural language processing;recurrent neural nets;speech recognition","feedforward sequential memory networks;encoder-decoder model;machine translation;recurrent neural networks;sequence mapping problems;sequence model","","","","19","","8 Feb 2018","","","IEEE","IEEE Conferences"
"Trends and challenges in language modeling for speech recognition and machine translation","H. Schwenk","University of Le Mans, France","2009 IEEE Workshop on Automatic Speech Recognition & Understanding","8 Jan 2010","2009","","","23","23","Summary form only given. Language models play an important role in large vocabulary continuous speech recognition (LVCSR) systems and statistical approaches to machine translation (SMT), in particular when modeling morphologically rich languages. Despite intensive research over more than 20 years, state-of-the-art LVCSR and SMT systems seem to use only one dominant approach: n-gram back-off language models. This talk first reviews the most important approaches to language modeling. I then discuss some of the recent trends and challenges for the future. An interesting alternative to the back-off n-gram approach are the so-called continuous space methods. The basic idea is to perform the probability estimation in a continuous space. By these means better probability estimations of unseen word sequences can be expected. There is also a relative large body of works on adaptive language models. The adaptation can aim to tailor a language model to a particular task or domain, or it can be performed over time. Another very active research area are discriminative language models. Finally, I will review the challenges and benefits of language models trained an very large amounts of training material.","","978-1-4244-5478-5","10.1109/ASRU.2009.5373531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5373531","","Natural languages;Speech recognition;Surface-mount technology;Vocabulary","estimation theory;language translation;speech recognition","large vocabulary continuous speech recognition;machine translation statistical approach;morphologically rich languages;continuous space methods;discriminative language models;adaptive language models;n-gram back-off language models;probability estimation;word sequences","","","","","","8 Jan 2010","","","IEEE","IEEE Conferences"
"Automatic detection of translucency using a deep learning method from patches of clinical basal cell carcinoma images","H. Huang; P. Kharazmi; D. I. McLean; H. Lui; Z. J. Wang; T. K. Lee","Dept. of Elec. & Comp. Engineering, Univ. of British Columbia, Vancouver, Canada; Dept. of Elec. & Comp. Engineering, Univ. of British Columbia, Vancouver, Canada; Dept. of Dermatology & Skin Science, Univ. of British Columbia, Vancouver, Canada; Dept. of Dermatology & Skin Science, Univ. of British Columbia, Vancouver, Canada; Dept. of Elec. & Comp. Engineering, Univ. of British Columbia, Vancouver, Canada; Cancer Control Research Program, BC Cancer Agency, Vancouver, Canada","2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","7 Mar 2019","2018","","","685","688","Translucency, defined as a jelly-like appearance, is a common clinical feature of basal cell carcinoma, the most common skin cancer. The feature plays an important role in diagnosing basal cell carcinoma in an early stage because the feature can be observed readily in clinical examinations with a high specificity of 93%. Therefore, translucency detection is a critical component of computer aided systems which aim at early detection of basal cell carcinoma. To address this problem, we proposed an automated method for analyzing patches of clinical basal cell carcinoma images using stacked sparse autoencoder (SSAE). SSAE learns high-level features in unsupervised manner and all learned features are fed into a softmax classifier for translucency detection. Across the 4401 patches generated from 32 clinical images, the proposed method achieved a 93% detection accuracy from a five-fold cross-validation. The preliminary result suggested that the proposed method could detect translucency from skin images.","2640-0103","978-9-8814-7685-2","10.23919/APSIPA.2018.8659685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8659685","Translucency;deep learning;stacked sparse autoencoder;basal cell carcinoma","Skin cancer;Skin;Dermatology;Neurons;Image color analysis;Lesions","biomedical optical imaging;cancer;cellular biophysics;feature extraction;image classification;learning (artificial intelligence);medical image processing;skin","automatic detection;deep learning method;clinical basal cell carcinoma images;common clinical feature;common skin cancer;clinical examinations;translucency detection;high-level features;learned features;clinical images;detection accuracy;SSAE learning","","","","15","","7 Mar 2019","","","IEEE","IEEE Conferences"
"Text mining in radiology reports by statistical machine translation approach","A. Bodile; M. Kshirsagar","Department of Computer Technology, Yeshwantrao Chavan College of Engineering, Nagpur, India; Department of Computer Technology, Yeshwantrao Chavan College of Engineering, Nagpur, India","2015 Global Conference on Communication Technologies (GCCT)","3 Dec 2015","2015","","","238","241","Medical text mining has gained increasing popularity in recent years. Now a days, large amount of medical text data are daily generated in health institutions, but never refer again as it is very time consuming task. In Radiology domain, most of the reports are in free text format and usually unprocessed, hence it is difficult to access the valuable information for medical professional unless proper text mining is not applied. There are some systems existing for radiology report information retrieval like MedLEE, NeuRadIR, CBIR but very few of them make use of text associated with image. This paper proposes a text mining system to deals with this problem by using statistical machine translation approach. The System stores the text and image features to find the match report. The SVM classifier is use in SMT approach to check whether entered report present in database or not. The system will return the similar report match with the entered report from the database.","","978-1-4799-8553-1","10.1109/GCCT.2015.7342797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7342797","Text mining;Radiology report;Image feature extractor;report retriever","Feature extraction;Biomedical imaging;Radiology;Text mining;Image edge detection;Support vector machines;Image color analysis","data mining;feature extraction;image classification;image retrieval;language translation;medical image processing;medical information systems;radiology;support vector machines;text analysis","text mining system;statistical machine translation;SVM classifier;image feature extractor;radiology report information retrieval","","1","","11","","3 Dec 2015","","","IEEE","IEEE Conferences"
"Speech into Sign Language Statistical Translation System for Deaf People","R. San Segundo; B. Gallo; J. M. Lucas; R. Barra-Chicote; L. F. D'Haro; F. Fernandez","Dept. de Ing. Electron., Univ. Politec. de Madrid, Madrid, Spain; Dept. de Ing. Electron., Univ. Politec. de Madrid, Madrid, Spain; Dept. de Ing. Electron., Univ. Politec. de Madrid, Madrid, Spain; Dept. de Ing. Electron., Univ. Politec. de Madrid, Madrid, Spain; Dept. de Ing. Electron., Univ. Politec. de Madrid, Madrid, Spain; Dept. de Ing. Electron., Univ. Politec. de Madrid, Madrid, Spain","IEEE Latin America Transactions","17 Nov 2009","2009","7","3","400","404","This paper presents a set of experiments used to develop a statistical system from translating speech to sign language for deaf people. This system is composed of an Automatic Speech Recognition (ASR) system, followed by a statistical translation module and an animated agent that represents the different signs. Two different approaches have been used to perform the translations: a phrase-based system and a finite state transducer. For the evaluation, the followings figures have been considered: WER (Word Error Rate), BLEU and NIST. The paper presents translation results of reference sentences and sentences from the automatic speech recognizer. Also three different configurations have been evaluated for the speech recognizer. The best results were obtained with the finite state transducer, with a word error rate of 28.21% for the reference text, and 29.27% using the ASR output.","1548-0992","","10.1109/TLA.2009.5336641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5336641","alignment;Finite State Transducer;Language Model;phrase;Sign Language;Statistical Machine Translation;Translation Model;word error rate","Telecommunications;Handicapped aids;Deafness;Automatic speech recognition;Transducers;Error analysis;Animation;NIST;Speech analysis;Speech recognition","language translation;speech recognition","statistical machine translation;automatic speech recognition;deaf people;sign language;language model;translation model;word error rate","","3","1","7","","17 Nov 2009","","","IEEE","IEEE Journals"
"Automatic pronunciation prediction for text-to-speech synthesis of dialectal arabic in a speech-to-speech translation system","S. Ananthakrishnan; S. Tsakalidis; R. Prasad; P. Natarajan; A. Namandi Vembu","Speech, Language and Multimedia Unit, Raytheon BBN Technologies, Cambridge, MA 02138, U.S.A.; Speech, Language and Multimedia Unit, Raytheon BBN Technologies, Cambridge, MA 02138, U.S.A.; Speech, Language and Multimedia Unit, Raytheon BBN Technologies, Cambridge, MA 02138, U.S.A.; Speech, Language and Multimedia Unit, Raytheon BBN Technologies, Cambridge, MA 02138, U.S.A.; Ming-Hsieh Dept. of Electrical Eng., University of Southern California, Los Angeles, 90089, U.S.A.","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4957","4960","Text-to-speech synthesis (TTS) is the final stage in the speech-tospeech (S2S) translation pipeline, producing an audible rendition of translated text in the target language. TTS systems typically rely on a lexicon to look up pronunciations for each word in the input text. This is problematic when the target language is dialectal Arabic, because the statistical machine translation (SMT) system usually produces undiacritized text output. Many words in the latter possess multiple pronunciations; the correct choice must be inferred from context. In this paper, we present a weakly supervised pronunciation prediction approach for undiacritized dialectal Arabic in S2S systems that leverages automatic speech recognition (ASR) to obtain parallel training data for pronunciation prediction. Additionally, we show that incorporating source language features derived from SMT-generated automatic word alignment further improves automatic pronunciation prediction accuracy.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289032","speech translation;dialectal arabic;pronunciation;speech synthesis","Predictive models;Hidden Markov models;Training;Speech;Mathematical model;Accuracy;Error analysis","language translation;prediction theory;speech recognition;speech synthesis","automatic pronunciation prediction approach;text-to-speech synthesis;dialectal Arabic;speech-to-speech translation system;TTS;S2S translation system;lexicon-look up pronunciation;statistical machine translation system;SMT system;automatic speech recognition;ASR;source language feature;SMT-generated automatic word alignment","","","","7","","30 Aug 2012","","","IEEE","IEEE Conferences"
"End-to-End Automatic Speech Translation of Audiobooks","A. Bérard; L. Besacier; A. C. Kocabiyikoglu; O. Pietquin","LIG - Univ. Grenoble-Alpes, France; CRIStAL - Univ. Lille, France; CRIStAL - Univ. Lille, France; LIG - Univ. Grenoble-Alpes, France","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","6224","6228","We investigate end-to-end speech-to-text translation on a corpus of audiobooks specifically augmented for this task. Previous works investigated the extreme case where source language transcription is not available during learning nor decoding, but we also study a midway case where source language transcription is available at training time only. In this case, a single model is trained to decode source speech into target text in a single pass. Experimental results show that it is possible to train compact and efficient end-to-end speech translation models in this setup. We also distribute the corpus and hope that our speech translation baseline on this corpus will be challenged in the future.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8461690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461690","End-to-end models;Speech Translation;LibriSpeech","Task analysis;Training;Decoding;Google;Vocabulary;Shape;Computational modeling","language translation;learning (artificial intelligence);speech recognition;text analysis","audiobooks;source language transcription;target text;compact end-to-end speech translation models;efficient end-to-end speech translation models;end-to-end speech-to-text translation;source speech decoding","","20","","20","","13 Sep 2018","","","IEEE","IEEE Conferences"
"Advances in syntax-based Malay-English speech translation","B. Xiang; B. Zhou; M. Cmejrek","IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","4801","4804","In this paper, we present advanced techniques that improved the performance of IBM Malay-English speech translation system significantly. During this work, we generated linguistics-driven hierarchical rules to enhance the formal syntax-based translation model; designed an active learning approach with bi-directional translations that outperformed unsupervised training; utilized translation direction information in parallel training corpus to build direction-specific interpolated language models for machine translation. There is 20% relative improvement achieved in the translation performance through all these techniques. A state-of-the-art Malay speech recognition system was also established as one of the crucial modules in the rapidly developed Malay-English speech translation.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960705","Machine Translation;Speech Recognition;Active Learning","Speech recognition;Natural languages;Machine learning;Bidirectional control;Automatic speech recognition;Data mining;Tagging;Training data;Semisupervised learning;Humans","language translation;speech recognition;unsupervised learning","syntax-based Malay-English speech translation;IBM Malay-English speech translation system;linguistics-driven hierarchical rules;formal syntax-based translation model;active learning;bi-directional translations;unsupervised training;Malay speech recognition system","","1","","14","","26 May 2009","","","IEEE","IEEE Conferences"
"Adaptation of lecture speech recognition system with machine translation output","R. W. M. Ng; T. Hain; T. Cohn","Department of Computer Science, The University of Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, United Kingdom","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","8401","8405","In spoken language translation, integration of the ASR and MT components is critical for good performance. In this paper, we consider the recognition setting where a text translation of each utterance is also available. We present experiments with different ASR system adaptation techniques to exploit MT system outputs. In particular, N-best MT outputs are represented as an utterance-specific language model, which are then used to rescore ASR lattices. We show that this method improves significantly over ASR alone, resulting in an absolute WER reduction of more than 6% for both indomain and out-of-domain acoustic models.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6639304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639304","TED talks;speech translation;language model adaptation","Speech;Adaptation models;Training;Interpolation;Speech recognition;Acoustics;Data models","language translation;speech recognition","lecture speech recognition system adaptation;machine translation output;spoken language translation;ASR components;MT components;recognition setting;text translation;ASR system adaptation techniques;MT system;N-best MT outputs;utterance-specific language model;ASR lattices;WER reduction;out-of-domain acoustic models;in-domain acoustic models","","1","3","31","","21 Oct 2013","","","IEEE","IEEE Conferences"
"Machine translation of conversation on the digitized battlefield","J. Moody; E. Steinbrecher; R. Frederking; A. Black; R. Brown","Lockheed Martin Syst. Integration, Owego, NY, USA; Lockheed Martin Syst. Integration, Owego, NY, USA; NA; NA; NA","2001 MILCOM Proceedings Communications for Network-Centric Operations: Creating the Information Force (Cat. No.01CH37277)","6 Aug 2002","2001","1","","635","639 vol.1","The technology of the information age and the digitized battlefield apply not only to machine-to-machine communications but support an unprecedented advancement in communication across human language barriers. Modern military operations, including ""peace-keeping, "" coordination with joint and multinational forces, and operations other than war, often involve the interaction of troops with indigenous peoples where language issues can have a serious impact on mission success. The Audio Voice Translation Guide System (Tongues) is a mobile, networked, speech-to-speech machine translation system that both benefits from and contributes to the digitized battlefield. The Tongues prototype runs on a commercial off-the-shelf (COTS) mobile computing platform, capable of fitting in a cargo pocket, and running a Microsoft Windows operating system. It supports real time interactive translation of bilingual conversation, within and without the selected target domain, with the highest quality appearing within the target domain. Tongues has a modular architecture, allowing for easy upgrades to the speech and language engines and multiple conversational domains and languages. The prototype system supports English/Croatian conversation for deployed US Army chaplains, a domain that includes religious support, humanitarian aid, and operations other than war. The system supports text-entry as well as speech input and provides feedback for verification of translation accuracy. The Tongues system can automatically capture and store textual and audio conversation transcripts in formats that comply with the Joint Technical Architecture-Army (JTA-A). It supports a wide variety of JTA-A compliant means for transferring that information to other information systems. Tongues can run in a stand-alone mode, or it can make a wireless network connection to remote machine translation servers, allowing for increased performance and flexibility.","","0-7803-7225-5","10.1109/MILCOM.2001.985914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=985914","","Prototypes;Speech;Natural languages;Peace technology;Mobile communication;Humans;Mobile computing;Military computing;Operating systems;Computer architecture","language translation;natural languages;speech processing;military communication;real-time systems;interactive systems;speech recognition","machine conversation translation;digitized battlefield;military operations;peace-keeping;multinational forces;joint multinational forces;Audio Voice Translation Guide System;Tongues;mobile machine translation system;COTS mobile computing platform;commercial off-the-shelf platform;Microsoft Windows operating system;real time interactive translation;bilingual conversation;modular architecture;speech engine;English/Croatian conversation;language engine;translation accuracy verification;US Army;humanitarian aid;religion;Joint Technical Architecture-Army;JTA-A;voice recognition engine","","1","","7","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Text to speech synthesis system for English to Malayalam translation","A. Anto; K. K. Nisha","Department of Computer Science and Engineering, Rajiv Gandhi Institute of Technology, Kottayam, India; Department of Computer Science and Engineering, Rajiv Gandhi Institute of Technology, Kottayam, India","2016 International Conference on Emerging Technological Trends (ICETT)","9 Mar 2017","2016","","","1","6","Speech recognition and Speech synthesis are the two emerging technologies in the communication field. Speech recognition is a system which generates text for the given speech, while a speech synthesizer is a system that should be able to read any text aloud. Research is conducting all over the world, to develop an efficient text to speech synthesis (TTS) system for minority languages. India has the largest democracy system in the world, also known as land of unity in diversity and has more than 22 official languages. It becomes difficult to understand the English and the other European languages to common people. This works aims to help people to translate English text to their own language and implement a TTS for the minority language, Malayalam. It is achieved by combining both Machine Translation and TTS. When an English text is given, it is translated to Malayalam with the help of a parser, using grammatical rules, applying morphology and a bilingual dictionary. From each of the translated Malayalam text, syllables are separated. A good number of syllables are recorded and stored in the syllable corpus. Syllables are concatenated to generate a synthesized Malayalam speech. Machine translation of English to Malayalam text is tested and achieved 73 percentage accuracy. For the TTS system, accuracy is verified by checking the naturalness and intelligibility. 87 percentages of the sentences are uttered correctly.","","978-1-5090-3751-3","10.1109/ICETT.2016.7873642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873642","Machine Translation System;Text to Speech synthesis;Speech recognition;Articulatory synthesis;Formant synthesis;Concatenation","Speech;Dictionaries;Speech recognition;Speech synthesis;Syntactics;Market research;Morphology","language translation;speech recognition;speech synthesis","text to speech synthesis system;English translation;Malayalam translation;speech recognition;speech synthesizer;text aloud;TTS system;minority languages;democracy system;European languages;machine translation;English text;parser;grammatical rules;bilingual dictionary;Malayalam text;syllable corpus;Malayalam speech;intelligibility","","","","19","","9 Mar 2017","","","IEEE","IEEE Conferences"
"Back-Translation-Style Data Augmentation for end-to-end ASR","T. Hayashi; S. Watanabe; Y. Zhang; T. Toda; T. Hori; R. Astudillo; K. Takeda","Nagoya University, Nagoya, JAPAN; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Google, Inc., USA; Nagoya University, Nagoya, JAPAN; Mitsubishi Electric Research Laboratories (MERL), Cambridge MA, USA; Spoken Language Systems Lab, INESC-ID Lisboa, Portugal; Nagoya University, Nagoya, JAPAN","2018 IEEE Spoken Language Technology Workshop (SLT)","14 Feb 2019","2018","","","426","433","In this paper we propose a novel data augmentation method for attention-based end-to-end automatic speech recognition (E2E-ASR), utilizing a large amount of text which is not paired with speech signals. Inspired by the back-translation technique proposed in the field of machine translation, we build a neural text-to-encoder model which predicts a sequence of hidden states extracted by a pre-trained E2E-ASR encoder from a sequence of characters. By using hidden states as a target instead of acoustic features, it is possible to achieve faster attention learning and reduce computational cost, thanks to sub-sampling in E2E-ASR encoder, also the use of the hidden states can avoid to model speaker dependencies unlike acoustic features. After training, the text-to-encoder model generates the hidden states from a large amount of unpaired text, then E2E-ASR decoder is retrained using the generated hidden states as additional training data. Experimental evaluation using LibriSpeech dataset demonstrates that our proposed method achieves improvement of ASR performance and reduces the number of unknown words without the need for paired data.","","978-1-5386-4334-1","10.1109/SLT.2018.8639619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8639619","automatic speech recognition;end-to-end;data augmentation;back-translation","Data models;Acoustics;Decoding;Feature extraction;Training data;Hidden Markov models;Training","acoustic signal processing;language translation;learning (artificial intelligence);neural nets;speech coding;speech recognition","attention-based end-to-end automatic speech recognition;speech signals;back-translation technique;machine translation;pre-trained E2E-ASR encoder;acoustic features;E2E-ASR decoder;back-translation-style data augmentation;data augmentation method;neural text-to-encoder model;attention learning","","18","","32","","14 Feb 2019","","","IEEE","IEEE Conferences"
"Lattice-Based ASR-MT Interface for Speech Translation","E. Matusov; H. Ney","AppTek (Application Technologies, Inc.), RWTH Aachen University, Aachen, Germany; RWTH Aachen University, Aachen, Germany","IEEE Transactions on Audio, Speech, and Language Processing","10 Feb 2011","2011","19","4","721","732","The usual approach to improve the interface between automatic speech recognition (ASR) and machine translation (MT) is to use ASR word lattices for translation. In comparison with the previous research along this line, this paper presents an efficient algorithm for lattice-based search in MT. This algorithm utilizes confusion network information to enable phrase-level reordering, and is also able to process general lattices. The proposed search is not constrained to be monotonic; thus, it is able to perform the same type of reordering given lattice input as any statistical phrase-based search algorithm with a single sentence input. Using the concept described in this paper, we are able to significantly improve speech translation results on several small and large vocabulary tasks. The improvements of the MT quality as measured by BLEU are as high as 5% relative. We also show that the proposed lattice-based translation can outperform state-of-the-art translation of confusion networks and has advantages in terms of translation speed. Furthermore, we propose and evaluate a novel approach that shares the benefits of lattice-based translation with those translation systems which are not designed to process word lattices.","1558-7924","","10.1109/TASL.2010.2060483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5518378","Machine translation (MT);natural languages;SLP-SSMT;speech processing","Lattices;Automatic speech recognition;Natural languages;Speech recognition;Speech processing;Permission;Vocabulary;Art;Process design;Face recognition","language translation;search problems;speech recognition;vocabulary","lattice-based ASR-MT interface;speech translation;automatic speech recognition;machine translation;ASR word lattices;lattice-based search;confusion network information;phrase-level reordering;statistical phrase-based search algorithm;large vocabulary task;MT quality","","5","","32","","23 Jul 2010","","","IEEE","IEEE Journals"
"Speech translation enhanced automatic speech recognition","M. Paulik; S. Stuker; C. Fugen; T. Schultz; T. Schaaf; A. Waibel","Karlsruhe Univ., Germany; Karlsruhe Univ., Germany; Karlsruhe Univ., Germany; NA; NA; NA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","121","126","Nowadays official documents have to be made available in many languages, like for example in the EU with its 20 official languages. Therefore, the need for effective tools to aid the multitude of human translators in their work becomes easily apparent. An ASR system, enabling the human translator to speak his translation in an unrestricted manner, instead of typing it, constitutes such a tool. In this work we improve the recognition performance of such an ASR system on the target language of the human translator by taking advantage of an either written or spoken source language representation. To do so, machine translation techniques are used to translate between the different languages and then the involved ASR systems are biased towards the gained knowledge. We present an iterative approach for ASR improvement and outperform our baseline system by a relative word error rate reduction of 35.8%/29.9% in the case of a written/spoken source language representation. Further, we show how multiple target languages, as for example provided by different simultaneous translators during European Parliament debates, can be incorporated into our system design for an improvement of all involved ASR systems","","0-7803-9478-X","10.1109/ASRU.2005.1566488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566488","","Speech enhancement;Automatic speech recognition;Humans;Natural languages;Iterative methods;Streaming media;Interactive systems;Laboratories;Target recognition;Error analysis","language translation;natural languages;speech recognition","speech translation;automatic speech recognition;human translator;source language representation;machine translation;multiple target languages","","14","","7","","3 Jan 2006","","","IEEE","IEEE Conferences"
"Unsupervised training for farsi-english speech-to-speech translation","Bing Xiang; Yonggang Deng; Yuqing Gao","IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA","2008 IEEE International Conference on Acoustics, Speech and Signal Processing","12 May 2008","2008","","","4977","4980","Speech-to-speech translation has evolved into an attractive area in recent years with significant progress made by various research groups. However, the translation engines usually suffer from the lack of bilingual training data, especially for low-resource languages. In this paper we present an unsupervised training technique to alleviate this problem by taking advantage of available source language data. Different approaches are proposed and compared through extensive experiments conducted on a speech-to-speech translation task between Farsi and English. The translation performance is significantly improved in both directions with the enhanced translation model. A state-of-the-art Farsi automatic speech recognition system is also established in this work.","2379-190X","978-1-4244-1483-3","10.1109/ICASSP.2008.4518775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4518775","Speech Recognition;Machine Translation;Unsupervised Training","Natural languages;Speech recognition;Automatic speech recognition;Hidden Markov models;Probability;Training data;Iterative decoding;Testing;Frequency estimation;Engines","language translation;natural language processing;speech processing;speech recognition;unsupervised learning","unsupervised training;Farsi-English speech-to-speech translation;source language data;automatic speech recognition;bilingual training data;statistical machine translation","","3","","14","","12 May 2008","","","IEEE","IEEE Conferences"
"ASR error detection in a conversational spoken language translation system","W. Chen; S. Ananthakrishnan; R. Kumar; R. Prasad; P. Natarajan","Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA 02138, U.S.A; Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA 02138, U.S.A; Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA 02138, U.S.A; Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA 02138, U.S.A; Speech, Language, and Multimedia Processing Unit, Raytheon BBN Technologies, Cambridge, MA 02138, U.S.A","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","7418","7422","Detection of automatic speech recognition (ASR) errors is crucial to preventing their further propagation through statistical machine translation (SMT) in conversational spoken language translation (CSLT) systems. In this paper, we venture beyond traditional features obtained from the ASR decoder and hypothesized word sequence, and explore additional information streams provided by an error-robust CSLT system, including SMT confidence estimates and posteriors from named entity detection (NED). Another significant novelty of this work is the use of an automated word boundary detector based on acoustic-prosodic features to verify the existence of ASR-hypothesized word boundaries, which further improves ASR error detection. Offline evaluation on a test set designed to invoke ASR errors showed that at 10% false alarm rate, the proposed features provide 2.8% absolute (4.2% relative) improvement in detection rate over a state-of-the-art baseline error detector that uses a rich set of features traditionally employed in the existing literature.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6639104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639104","automatic speech recognition;ASR error detection;conversational speech translation;SMT confidence estimation;word boundary detection","Feature extraction;Speech;Decoding;Detectors;Training;Acoustics;Speech recognition","language translation;speech recognition;statistical analysis","conversational spoken language translation system;automatic speech recognition error detection;statistical machine translation;ASR decoder;hypothesized word sequence;information streams;error-robust CSLT system;SMT confidence;named entity detection;NED;automated word boundary detector;acoustic-prosodic features;false alarm rate;detection rate improvement;baseline error detector","","12","","23","","21 Oct 2013","","","IEEE","IEEE Conferences"
"Applying log linear model based context dependent machine translation techniques to grapheme-to-phoneme conversion","R. Zhang; B. Zhou","IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","28 Jun 2010","2010","","","4634","4637","Grapheme-to-Phoneme conversion is a challenging task for speech recognition and text-to-speech systems for which the functionality of automatically predicting pronunciations for OOV words is highly desirable. In this paper, Grapheme-to-Phoneme conversion is viewed as a special case of sequence translation problem and we propose to tackle it with phrase based log-linear translation model. We improve standard machine translation method by utilizing context dependent units which lead to a better many-to-many alignment between chunks of graphemes and phonemes. Furthermore, hypotheses combination technique is applied to combine outputs generated by multiple translation models trained with different alignment units. Our proposed approach was evaluated on NetTalk and CMUDict datasets. Significant improvements on conversion accuracy are observed on both sets compared to conventional translation method: phoneme level error rates are reduced relatively by 18.4% and 22.5%, respectively. Our approach also performs better than or as good as previously published data driven methods examined on the same tasks.","2379-190X","978-1-4244-4295-9","10.1109/ICASSP.2010.5495551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495551","Grapheme-to-Phone conversion","Context modeling;Predictive models;Hidden Markov models;Speech recognition;Prediction methods;Speech synthesis;Error analysis;Dictionaries;Machine learning algorithms;Classification tree analysis","language translation;speech recognition;speech synthesis","log linear model;context dependent machine translation techniques;grapheme-to-phoneme conversion;speech recognition;text-to-speech systems;OOV words;sequence translation problem;multiple translation models;NetTalk datasets;CMUDict datasets","","1","","16","","28 Jun 2010","","","IEEE","IEEE Conferences"
"Sign Language Translation","R. Harini; R. Janani; S. Keerthana; S. Madhubala; S. Venkatasubramanian","Saranathan college of engineering,Department of Computer Science and Engineering,Trichy,India; Saranathan college of engineering,Department of Computer Science and Engineering,Trichy,India; Saranathan college of engineering,Department of Computer Science and Engineering,Trichy,India; Saranathan college of engineering,Department of Computer Science and Engineering,Trichy,India; Saranathan college of engineering,Department of Computer Science and Engineering,Trichy,India","2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS)","23 Apr 2020","2020","","","883","886","Sign language is the way of communication for hearing impaired people. There is a challenge for common people to communicate with deaf people which makes this system helpful in assisting them. This project aims at implementing computer vision which can take the sign from the users and convert them into text in real time. The proposed system contains four modules such as: image capturing, preprocessing classification and prediction. By using image processing the segmentation can be done. Sign gestures are captured and processed using OpenCV python library. The captured gesture is resized, converted to grey scale image and the noise is filtered to achieve prediction with high accuracy. The classification and predication are done using convolution neural network.","2575-7288","978-1-7281-5197-7","10.1109/ICACCS48705.2020.9074370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9074370","Machine learning;Gesture recognition;Sign language;Human Computer Interaction;Computer Vision","Gesture recognition;Assistive technology;Feature extraction;Training;Computer science;Cameras;Computer vision","computer vision;feature extraction;gesture recognition;handicapped aids;image filtering;image segmentation;language translation;neural nets;sign language recognition","computer vision;image capturing;sign gestures;sign language translation;impaired people;deaf people;OpenCV Python library","","","","9","","23 Apr 2020","","","IEEE","IEEE Conferences"
"The Impact of ASR on Speech-to-Speech Translation Performance","R. Sarikaya; B. Zhou; D. Povey; M. Afify; Y. Gao","IBM T.J. Watson Research Center, Yorktown Heights, NY 10598. sarikaya@us.ibm.com; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598. bzhou@us.ibm.com; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598. dpovey@us.ibm.com; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598. maafify@us.ibm.com; IBM T.J. Watson Research Center, Yorktown Heights, NY 10598. yuqing@us.ibm.com","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-1289","IV-1292","This paper reports on experiments to quantify the impact of automatic speech recognition (ASR) in general and discriminatively trained ASR in particular on the machine translation (MT) performance. The minimum phone error (MPE) training method is employed for building the discriminative ASR acoustic models and a weighted finite state transducer (WEST) based method is used for MT. The experiments are performed on a two-way English/dialectal-Arabic speech-to-speech (S2S) translation task in the military/medical domain. We demonstrate the relationship between ASR and MT performance measured by BLEU and human judgment for both directions of the translation. Moreover, we question the use of BLEU metric for assessing the MT quality, present our observations and draw some conclusions.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218344","Speech Recognition;ASR;Machine Translation;MT;Performance Metric","Automatic speech recognition;Humans;Hidden Markov models;Biomedical acoustics;Biomedical transducers;Acoustic transducers;Acoustic measurements;Speech recognition;Error analysis;Laboratories","language translation;speech recognition","ASR;automatic speech recognition;machine translation;minimum phone error training method;discriminative ASR acoustic models;weighted finite state transducer;two-way English-dialectal-Arabic speech-to-speech translation;BLEU","","2","","13","","4 Jun 2007","","","IEEE","IEEE Conferences"
"Improving Automatic Speech Recognition and Speech Translation via Word Embedding Prediction","S. -P. Chuang; A. H. Liu; T. -W. Sung; H. -y. Lee","Graduate Institute of Communication, National Taiwan University, Taipei, Taiwan; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Computer Science and Engineering, University of California, San Diego, U.S.; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","9 Dec 2020","2021","29","","93","105","In this article, we target speech translation (ST). We propose lightweight approaches that generally improve either ASR or end-to-end ST models. We leverage continuous representations of words, known as word embeddings, to improve ASR in cascaded systems as well as end-to-end ST models. The benefit of using word embedding is that word embedding can be obtained easily by training on pure textual data, which alleviates data scarcity issue. Also, word embedding provides additional contextual information to speech models. We motivate to distill the knowledge from word embedding into speech models. In ASR, we use word embeddings as a regularizer to reduce the WER, and further propose a novel decoding method to fuse the semantic relations among words for further improvement. In the end-to-end ST model, we propose leveraging word embeddings as an intermediate representation to enhance translation performance. Our analysis shows that it is possible to map speech signals to semantic space, which motivates future work on applying the proposed methods in spoken language processing tasks.","2329-9304","","10.1109/TASLP.2020.3037543","Ministry of Science and Technology, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9257188","Speech recognition;speech translation","Training;Semantics;Predictive models;Multitasking;Decoding;Speech processing;Task analysis","decoding;language translation;natural language processing;sensor fusion;speech processing;speech recognition","automatic speech recognition;word embedding prediction;speech translation;end-to-end ST model;speech models","","","","77","IEEE","12 Nov 2020","","","IEEE","IEEE Journals"
"Automatic Real-Time Embedded QRS Complex Detection for a Novel Patch-Type Electrocardiogram Recorder","D. B. Saadi; G. Tanev; M. Flintrup; A. Osmanagic; K. Egstrup; K. Hoppe; P. Jennum; J. L. Jeppesen; H. K. Iversen; H. B. D. Sorensen","Department of Electrical Engineering, Technical University of Denmark, Kongens Lyngby, Denmark; DELTA Danish Electronics, Light and Acoustics, Hørsholm, Denmark; DELTA Danish Electronics, Light and Acoustics, Hørsholm, Denmark; Department of Medical Research, Svendborg Hospital, Odense University Hospital, Svendborg, Denmark; Department of Medical Research, Svendborg Hospital, Odense University Hospital, Svendborg, Denmark; DELTA Danish Electronics, Light and Acoustics, Hørsholm, Denmark; Department of Clinical Neurophysiology, Glostrup HospitalDanish Center for Sleep Medicine, University of Copenhagen, Glostrup, Denmark; Department of Medicine, Glostrup Hospital, Glostrup, Denmark; Department of Neurology, Glostrup Hospital, Glostrup, Denmark; Department of Electrical Engineering, Technical University of Denmark, Kongens Lyngby, Denmark","IEEE Journal of Translational Engineering in Health and Medicine","20 May 2017","2015","3","","1","12","Cardiovascular diseases are projected to remain the single leading cause of death globally. Timely diagnosis and treatment of these diseases are crucial to prevent death and dangerous complications. One of the important tools in early diagnosis of arrhythmias is analysis of electrocardiograms (ECGs) obtained from ambulatory long-term recordings. The design of novel patch-type ECG recorders has increased the accessibility of these long-term recordings. In many applications, it is furthermore an advantage for these devices that the recorded ECGs can be analyzed automatically in real time. The purpose of this study was therefore to design a novel algorithm for automatic heart beat detection, and embed the algorithm in the CE marked ePatch heart monitor. The algorithm is based on a novel cascade of computationally efficient filters, optimized adaptive thresholding, and a refined search back mechanism. The design and optimization of the algorithm was performed on two different databases: The MIT-BIH arrhythmia database (Se = 99.90%, P+ = 99.87) and a private ePatch training database (Se = 99.88%, P+ = 99.37%). The offline validation was conducted on the European ST-T database (Se = 99.84%, P+ = 99.71%). Finally, a double-blinded validation of the embedded algorithm was conducted on a private ePatch validation database (Se = 99.91%, P+ = 99.79%). The algorithm was thus validated with high clinical performance on more than 300 ECG records from 189 different subjects with a high number of different abnormal beat morphologies. This demonstrates the strengths of the algorithm, and the potential for this embedded algorithm to improve the possibilities of early diagnosis and treatment of cardiovascular diseases.","2168-2372","","10.1109/JTEHM.2015.2421901","DELTA Performance Contract 2010-2012: Intelligent Welfare Technologies and Single Use Devices; Danish Council for Technology and Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7084104","automatic QRS complex detection;embedded ECG analysis;ePatch ECG recorder;patch type ECG recorder;real-time ECG analysis;Automatic QRS complex detection;embedded ECG analysis;ePatch ECG recorder;patch type ECG recorder;real-time ECG analysis","Electrocardiography;Databases;Algorithm design and analysis;Finite impulse response filters;Back;Feature extraction","cardiovascular system;diseases;electrocardiography;filtering theory;medical signal detection;medical signal processing","cardiovascular diseases treatment;cardiovascular diseases diagnosis;abnormal beat morphology;European ST-T database;private ePatch training database;MIT-BIH arrhythmia database;computational efficient filters;CE marked ePatch heart monitor;automatic heart beat detection;patch-type ECG recordings;arrhythmias diagnosis;patch-type electrocardiogram recorder;automatic real-time embedded QRS complex detection","","18","","22","","10 Apr 2015","","","IEEE","IEEE Journals"
"Experiments in word-reordering and morphological preprocessing for transducer-based statistical machine translation","A. de Gispert; J. B. Marino","TALP Res. Center, Univ. Politecnica de Catalunya, Barcelona, Spain; TALP Res. Center, Univ. Politecnica de Catalunya, Barcelona, Spain","2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)","2 Aug 2004","2003","","","634","639","Statistical speech translation can be achieved by an integrated search procedure that produces speech recognition and translation at the same time. Based on finite-state transducers and Viterbi search, the approach forces the rewriting of the target language in a set of modified units (with zero, one or more original words) to preserve the monotonicity of the search over the speech signal without changing the word order in the target language. In this paper, we analyse the effect of non-monotonic word alignments in two different Spanish to English translation tasks (speech-aimed small-vocabulary Verbmobil task and text-aimed large-vocabulary European Parliament task), revealing the most frequent cross patterns and experimenting with reordering strategies to improve the transducer probabilities. In addition, some preliminary results are presented on introducing POS-tagging and lemmatization, as well as some preprocessing such as categorization, to help improving the training of the system.","","0-7803-7980-2","10.1109/ASRU.2003.1318514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318514","","Natural languages;Equations;Speech recognition;Acoustic transducers;Speech processing;Text recognition;Viterbi algorithm;Pattern analysis;Speech analysis;Vocabulary","language translation;speech recognition","word-reordering;morphological preprocessing;transducer-based statistical machine translation;integrated search procedure;integrated speech recognition/translation;finite-state transducers;Viterbi search;search monotonicity;nonmonotonic word alignments;Spanish/English translation;speech-aimed small vocabulary Verbmobil task;text-aimed large-vocabulary European Parliament task;cross patterns;POS-tagging;lemmatization;categorization","","","","14","","2 Aug 2004","","","IEEE","IEEE Conferences"
"Effect of linguistic information in neural machine translation","N. Nakamura; H. Isahara","Department of Computer Science and Engineering, Toyohashi University of Technology, Toyohashi, Japan; Information and Media Center, Toyohashi University of Technology, Toyohashi, Japan","2017 International Conference on Advanced Informatics, Concepts, Theory, and Applications (ICAICTA)","2 Nov 2017","2017","","","1","6","Deep Neural Networks(DNNs) outperform previous works in many fields such as in natural language processing. Neural Machine Translation(NMT) also outperforms Statistical Machine Translation(SMT) which has complex features and rules. However, NMT requires a large corpus and a long calculation time. In order to suppress calculation cost, recent researches replaced low frequency words with symbols. However, the symbols make sentences ambiguous and deteriorates translation accuracy. To solve this problem, sub-word units such as Byte Pair Encoding(BPE) and Wordpiece Model(WPM) creating vocabularies in a prespecified vocabulary size has been proposed. Nevertheless, these tokenize methods break words and treat them as symbols. Words as symbols are compatible with neural networks and NMT performance has increased. This result shows that linguistic correctness is not necessarily important in NMT. If that is the case, we wonder to what extent linguistic correctness contributes to NMT accuracy. In this research, we experiment to incorporate linguistic information into sub-word units. Experimentally, we demonstrate that morpheme as linguistic information is a helpful factor for sub-word units.","","978-1-5386-3001-3","10.1109/ICAICTA.2017.8090975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8090975","","Pragmatics;Vocabulary;Decoding;Computer architecture;Mathematical model;Telescopes","language translation;learning (artificial intelligence);linguistics;natural language processing;neural nets;speech recognition;statistical analysis;text analysis","calculation cost;low frequency words;symbols;translation accuracy;sub-word units;prespecified vocabulary size;NMT performance;NMT accuracy;linguistic information;natural language processing;Byte Pair Encoding;BPE;Wordpiece Model;WPM;linguistic correctness;Deep Neural Networks;DNN;Neural Machine Translation;NMT;Statistical Machine Translation;SMT","","","","16","","2 Nov 2017","","","IEEE","IEEE Conferences"
"Converting Written Language to Spoken Language with Neural Machine Translation for Language Modeling","S. Ando; M. Suzuki; N. Itoh; G. Kurata; N. Minematsu","The University of Tokyo,Graduate School of Engineering; IBM Research AI; IBM Research AI; IBM Research AI; The University of Tokyo,Graduate School of Engineering","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","8124","8128","When building a language model (LM) for spontaneous speech, the ideal situation is to have a large amount of spoken, in-domain training data. Having such abundant data, however, is not realistic. We address this problem by generating texts in spoken language from those in written language by using a neural machine translation (NMT) model. We collected faithful transcripts of fully spontaneous speech and corresponding written versions and used them as a parallel corpus to train the NMT model. We used top-k random sampling, which generates a large variety of texts of higher quality as compared to other generation methods for NMT. We indicate that the NMT model is capable of converting written texts in a certain domain to spoken texts, and that the converted texts are effective for training LMs. Our experimental results show significant improvement of speech recognition accuracy with the LMs.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053226","spontaneous speech;parallel corpus;Transformer;domain adaptation","Training;Adaptation models;Training data;Speech recognition;Data models;Machine translation;Task analysis","language translation;natural language processing;speech recognition;text analysis","NMT model;written texts;spoken texts;converted texts;written language;spoken language;language modeling;neural machine translation model;top-k random sampling;speech recognition","","","","21","","9 Apr 2020","","","IEEE","IEEE Conferences"
"An automatic speech translation system on PDAs for travel conversation","R. Isotani; K. Yamabana; S. Ando; K. Hanazawa; S. Ishikawa; T. Emori; K. Iso; H. Hattori; A. Okumura; T. Watanabe","Multimedia Res. Labs., NEC Corp., Japan; Multimedia Res. Labs., NEC Corp., Japan; Multimedia Res. Labs., NEC Corp., Japan; Multimedia Res. Labs., NEC Corp., Japan; Multimedia Res. Labs., NEC Corp., Japan; Multimedia Res. Labs., NEC Corp., Japan; Multimedia Res. Labs., NEC Corp., Japan; Multimedia Res. Labs., NEC Corp., Japan; Multimedia Res. Labs., NEC Corp., Japan; Multimedia Res. Labs., NEC Corp., Japan","Proceedings. Fourth IEEE International Conference on Multimodal Interfaces","22 Jan 2003","2002","","","211","216","We present an automatic speech-to-speech translation system for personal digital assistants (PDAs) that helps oral communication between Japanese and English speakers in various situations while traveling. Our original compact large vocabulary continuous speech recognition engine, compact translation engine based on a lexicalized grammar, and compact Japanese speech synthesis engine lead to the development of a Japanese/English bi-directional speech translation system that works with limited computational resources.","","0-7695-1834-6","10.1109/ICMI.2002.1166995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166995","","Personal digital assistants;Speech recognition;Speech synthesis;Natural languages;Vocabulary;Search engines;Oral communication;Bidirectional control;Application software;Personal communication networks","speech recognition;speech synthesis;notebook computers;travel industry;language translation;vocabulary;speech-based user interfaces","automatic speech translation system;travel conversation;personal digital assistants;oral communication;Japanese;English;large vocabulary;continuous speech recognition engine;lexicalized grammar;computational resources;speech synthesis engine;bi-directional speech translation system","","3","","15","","22 Jan 2003","","","IEEE","IEEE Conferences"
"A finite-state approach to machine translation","S. Bangalore; G. Riccardi","AT&T Labs.-Res., USA; AT&T Labs.-Res., USA","IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","381","388","The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection; (b) lexical reordering. We propose stochastic finite-state models for these two subproblems. Stochastic finite-state models are efficiently able to learn from data, effective for decoding and are associated with a calculus for composing models which allows for tight integration of constraints from various levels of language processing. We present a method for learning stochastic finite-state models for lexical choice and lexical reordering that are trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese translation and present the performance of these models for translation of speech and text. We also evaluate the efficacy of such a translation model in the context of a call routing task of unconstrained speech utterances.","","0-7803-7343-X","10.1109/ASRU.2001.1034665","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034665","","Stochastic processes;Decoding;Surface-mount technology;Calculus;Transducers;Speech processing;Natural languages;Context modeling;Speech recognition;Routing","language translation;speech recognition;finite state machines;stochastic automata;decoding;natural language interfaces;learning (artificial intelligence);text analysis","finite-state approach;machine translation;lexical selection;lexical reordering;stochastic models;decoding;language processing;English-Japanese translation;call routing task;speech utterances;speech recognition","","4","3","26","","7 Nov 2002","","","IEEE","IEEE Conferences"
"Image Classification in Arabic: Exploring Direct English to Arabic Translations","A. Alsudais","Department of Information Systems, College of Computer Engineering and Sciences, Prince Sattam bin Abdulaziz University, Al-Kharj, Saudi Arabia","IEEE Access","10 Sep 2019","2019","7","","122730","122739","Image classification is an ongoing research challenge. Most of the current research focuses on image classification in English with very little research in Arabic. Expanding image classification to Arabic has several applications and benefits. This paper investigates the accuracy of direct translations of English labels that are available in ImageNet, a database of images labeled in English that is commonly used in computer vision research, to Arabic. A dataset comprised of 2,887 labeled images was constructed by randomly selecting images from ImageNet. All of the labels were translated to Arabic using an online translation service. The accuracy of each translation was evaluated by a human judge. Results indicated that 65.6% of the generated Arabic labels were accurate with the highest results achieved when the labels consisted of only one word. This study makes three important contributions to the image classification literature: (1) it determines a baseline level of accuracy for image classification in Arabic algorithms; (2) it provides 1,910,935 images classified with accurate Arabic labels (based on accurately labeling 1,895 images that consist of 1,643 unique synsets); and (3) it measures the accuracy of translations of image labels in ImageNet to Arabic.","2169-3536","","10.1109/ACCESS.2019.2926924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8755849","Image classification;image processing;information retrieval;machine translation;natural language processing","Task analysis;Databases;Natural language processing;Image classification;Object recognition;Information retrieval","computer vision;image classification;language translation;natural language processing","Arabic translations;English labels;computer vision;ImageNet;image classification","","1","","43","CCBY","5 Jul 2019","","","IEEE","IEEE Journals"
"Kannada ImageNet: A Dataset for Image Classification in Kannada","R. G; S. W; A. M. Bharadwaj; C. H N","University Visvesvaraya College of Engineering,Dept. of Computer Science and Engineering,Bengaluru,India; University Visvesvaraya College of Engineering,Dept. of Computer Science and Engineering,Bengaluru,India; S J B Institute of Technology,Dept. of Computer Science and Engineering,Bengaluru,India; University Visvesvaraya College of Engineering,Dept. of Computer Science and Engineering,Bengaluru,India","2021 International Conference on Computer Communication and Informatics (ICCCI)","21 Apr 2021","2021","","","1","4","Most of the ongoing research for image classification happens in English and there is very little research about classification of images in regional Indian languages like Kannada. There are several uses and advantages of classifying images natively in Kannada. A diverse dataset of 425,723 images which is accurately labelled in Kannada is obtained through this study, which adds to the contribution this paper makes to image classification literature, that is verified and corrected by a human judge. It consists of 1,083 unique classes which was constructed by a combination of randomly selecting classes from ImageNet, a database commonly used for Image classification in English as well as 193 manually thought out classes. The classes were translated first using an online translation service and later corrected by a human judge to achieve a translation accuracy of 97.15% on the entire dataset.","2329-7190","978-1-7281-5875-4","10.1109/ICCCI50826.2021.9402356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402356","Image classification;image processing;information retrieval;Image dataset;Image classification in Indian language;machine translation","Uncertainty;Dictionaries;Databases;Informatics;Image classification","document image processing;image classification;language translation;natural language processing","image classification;regional Indian language;Kannada ImageNet database;online translation service","","","","17","","21 Apr 2021","","","IEEE","IEEE Conferences"
"Automatic detection of camera translation in eye video recordings using multiple methods","F. Karmali; M. Shelhamer","Dept. of Biomed. Eng., Johns Hopkins Univ., Baltimore, MD, USA; Dept. of Biomed. Eng., Johns Hopkins Univ., Baltimore, MD, USA","The 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","14 Mar 2005","2004","1","","1525","1528","In video eye tracking, shifting of the camera relative to the head can introduce artifacts. This work proposes a new combination of image processing techniques to automatically detect and measure the relative translation of cameras separately imaging the left and right eyes. It uses a priori physiological knowledge to improve the accuracy of algorithms. The first method compares an eye image with a reference frame using cross-correlation methods. The second isolates the upper eyelid and compares it with a reference frame to improve approximation of camera translation. The third creates an eyelid template from multiple frames and cross-correlates it with each image frame. The later has the highest accuracy, with a mean error of 1.3 pixels. It is more robust since it eliminates features of the image that may introduce errors. This excellent accuracy makes the method a viable solution for the problem of camera movement relative to the head.","","0-7803-8439-3","10.1109/IEMBS.2004.1403467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1403467","edge detection;eye;eyelid;image processing;video","Cameras;Video recording;Eyelids;Eyes;Skin;Magnetic heads;Image processing;Reflection;Aircraft;Cornea","video recording;eye;biomechanics;biomedical optical imaging;medical image processing;edge detection;video cameras;optical tracking","automatic detection;camera translation;eye video recordings;video eye tracking;artifacts;image processing techniques;cross-correlation methods;eyelid template;image frame;camera movement;eye movements;edge detection","","1","","6","","14 Mar 2005","","","IEEE","IEEE Conferences"
"Multi-class Model M","A. Emami; S. F. Chen","IBM T.J. Watson Research Center, 1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA; IBM T.J. Watson Research Center, 1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","11 Jul 2011","2011","","","5516","5519","Model M, a novel class-based exponential language model, has been shown to significantly outperform word n-gram models in state-of-the-art machine translation and speech recognition systems. The model was motivated by the observation that shrinking the sum of the parameter magnitudes in an exponential language model leads to better performance on unseen data. Being a class-based language model, Model M makes use of word classes that are found automatically from training data. In this paper, we extend Model M to allow for different clusterings to be used at different word positions. This is motivated by the fact that words play different roles depending on their position in an n-gram. Experiments on standard NIST and GALE Arabic-to-English development and test sets show improvements in machine translation quality as measured by automatic evaluation metrics.","2379-190X","978-1-4577-0539-7","10.1109/ICASSP.2011.5947608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947608","Language Modeling;Machine Translation;Maximum-Entropy Models","Clustering algorithms;Prediction algorithms;Data models;Adaptation models;Decoding;Training data;Speech recognition","language translation;speech recognition","multiclass model M;class-based exponential language model;machine translation;speech recognition system;n-gram model;GALE Arabic-to-English development;NIST","","2","","11","","11 Jul 2011","","","IEEE","IEEE Conferences"
"Sequence-to-sequence models for punctuated transcription combining lexical and acoustic features","O. Klejch; P. Bell; S. Renals","Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK; Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK; Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","5700","5704","In this paper we present an extension of our previously described neural machine translation based system for punctuated transcription. This extension allows the system to map from per frame acoustic features to word level representations by replacing the traditional encoder in the encoder-decoder architecture with a hierarchical encoder. Furthermore, we show that a system combining lexical and acoustic features significantly outperforms systems using only a single source of features on all measured punctuation marks. The combination of lexical and acoustic features achieves a significant improvement in F-Measure of 1.5 absolute over the purely lexical neural machine translation based system.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7953248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953248","punctuation;speech recognition;neural machine translation;rich transcription","Acoustics;Training;Stochastic processes;Hidden Markov models;Speech;Decoding;Data models","acoustic signal processing;decoding;encoding;language translation;neural nets;speech recognition","sequence-to-sequence models;punctuated transcription;lexical features;acoustic features;neural machine translation;word level representations;encoder-decoder architecture;hierarchical encoder;punctuation marks;F-measure;speech recognition","","6","","37","","19 Jun 2017","","","IEEE","IEEE Conferences"
"Automating Transliteration of Cuneiform from Parallel Lines with Sparse Data","B. Bogacz; M. Klingmann; H. Mara","Forensic Comput. Geometry Lab. (FCGL), Interdiscipl. Center for Sci. Comput. (IWR) Univ., Heidelberg, Germany; Forensic Comput. Geometry Lab. (FCGL), Interdiscipl. Center for Sci. Comput. (IWR) Univ., Heidelberg, Germany; Forensic Comput. Geometry Lab. (FCGL), Interdiscipl. Center for Sci. Comput. (IWR) Univ., Heidelberg, Germany","2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)","29 Jan 2018","2017","01","","615","620","Cuneiform tablets appertain to the oldest textual artifacts and are in extent comparable to texts written in Latin or ancient Greek. The Cuneiform Commentaries Project (CPP) from Yale University provides tracings of cuneiform tablets with annotated transliterations and translations. As a part of our work analyzing cuneiform script computationally with 3D-acquisition and word-spotting, we present a first approach for automatized learning of transliterations of cuneiform tablets based on a corpus of parallel lines. These consist of manually drawn cuneiform characters and their transliteration into an alphanumeric code. Since the Cuneiform script is only available as raster-data, we segment lines with a projection profile, extract Histogram of oriented Gradients (HoG) features, detect outliers caused by tablet damage, and align those features with the transliteration. We apply methods from part-of-speech tagging to learn a correspondence between features and transliteration tokens. We evaluate point-wise classification with K-Nearest Neighbors (KNN) and a Support Vector Machine (SVM); sequence classification with a Hidden Markov Model (HMM) and a Structured Support Vector Machine (SVM-HMM). Analyzing our findings, we reach the conclusion that the sparsity of data, inconsistent labeling and the variety of tracing styles do currently not allow for fully automatized transliterations with the presented approach. However, the pursuit of automated learning of transliterations is of great relevance as manual annotation in larger quantities is not viable, given the few experts capable of transcribing cuneiform tablets.","2379-2140","978-1-5386-3586-5","10.1109/ICDAR.2017.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8270037","Cuneiform;Transliteration;HMM;Machine Learning;Computer Linguistics;Online Databases;Handwriting Recognition;Language Modeling","Hidden Markov models;Feature extraction;Support vector machines;Task analysis;Image segmentation;Microsoft Windows;Histograms","feature extraction;hidden Markov models;image classification;learning (artificial intelligence);natural language processing;notebook computers;support vector machines;text analysis","parallel lines;cuneiform tablets;Cuneiform Commentaries Project;annotated transliterations;cuneiform script;cuneiform characters;transliteration tokens;fully automatized transliterations;CPP;3D-acquisition;3D-word-spotting;Yale University;structured support vector machine;hidden Markov model;SVM-HMM;k-nearest neighbors;KNN;histogram-of-oriented gradient feature extraction;HoG feature extraction","","","","25","","29 Jan 2018","","","IEEE","IEEE Conferences"
"A novel model characteristics for noise-robust Automatic Speech Recognition based on HMM","M. Saadeq Rafieee; Ali Akbar Khazaei","IEEE Signal Processing Society, Iran; Islamic Azad University-Mashhad Branch, Iran","2010 IEEE International Conference on Wireless Communications, Networking and Information Security","5 Aug 2010","2010","","","215","218","This paper proposes a new model for a noise-robust Automatic Speech Recognition (ASR) based on parallel branch Hidden Markov Model (HMM) structure with a novel approach for robust speech recognition. Automatic Speech Recognition applications such as voice command and control, audio indexing, speech-to-speech translation, do not usually work well in noisy environments. In this paper, we present the characteristics of a novel model by exploring vibrocervigraphic and electromyographic ASR methods and some other effective approaches to achieve the best results. By employing the proposed model, we obtain the word error rate, available bandwidths with cutoff frequencies, word recognition rate, etc. This paper includes advanced front-end processing with less computational requirements and a statistical modeling for large-vocabulary myoelectric speech. Therefore parameters estimation of ASR system like Mel-Frequency Cepstral Coefficients (MFCC) are investigated to create the statistically optimized model.","","978-1-4244-5849-3","10.1109/WCINS.2010.5541923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541923","component;Automatic Speech Recognition (ASR);Mel-Frequency Cepstral Coefficients (MFCC);Noise-Robust;Hidden Markov Model (HMM);Statistical Modeling","Hidden Markov models;Noise robustness;Automatic speech recognition;Speech recognition;Command and control systems;Indexing;Working environment noise;Error analysis;Bandwidth;Cutoff frequency","hidden Markov models;speech recognition","noise-robust automatic speech recognition;parallel branch hidden Markov model;voice command;voice control;audio indexing;speech-to-speech translation;vibrocervigraphic methods;electromyographic methods;word error rate;word recognition rate;Mel-frequency cepstral coefficients;statistically optimized model;parameters estimation;large-vocabulary myoelectric speech;HMM","","1","","30","","5 Aug 2010","","","IEEE","IEEE Conferences"
"MAST: Myo Armband Sign-Language Translator for Human Hand Activity Classification","Z. M. Shakeel; S. So; P. Lingga; J. P. Jeong","Sungkyunkwan University,Department of Computer Science & Engineering,Suwon,Republic of Korea; Sungkyunkwan University,Department of Computer Science & Engineering,Suwon,Republic of Korea; Sungkyunkwan University,Department of Electrical & Computer Engineering,Suwon,Republic of Korea; Sungkyunkwan University,Department of Computer Science & Engineering,Suwon,Republic of Korea","2020 International Conference on Information and Communication Technology Convergence (ICTC)","21 Dec 2020","2020","","","494","499","As Computer Science has grown into an encompassed field in various scientific areas, the need for developing a computer aided and artificially intelligent device has become more important especially in the medical field. Artificial Intelligence (AI) plays a vital role not only in accelerating and optimizing common tasks but also in performing tasks that humans are incapable of. This paper presents a Myo Armband Sign-Language Translator (MAST), which is a novel algorithm to translate a hand's gestures into medical sign language using a Myo armband sensor which collects muscles' electromyography signals and then to classify them using an enhanced version of a dynamic random forest. Our experimental results indicate that a systematic fine tuning of MAST parameters leads to an accuracy improvement of 13% over the state-of-the-art scheme such as SCIKIT's random forest. Other comparison results show an improvement of over 20% compared to a popular classification scheme such as Support Vector Machines (SVM) and a deep learning technique such as Convolutional Neural Network (CNN).","2162-1233","978-1-7281-6758-9","10.1109/ICTC49870.2020.9289153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9289153","Electromyography;Myo Armband;Signlanguage Translator;Random Forest;Support Vector Machine;Convolutional Neural Network","Deep learning;Assistive technology;Face recognition;Heuristic algorithms;Gesture recognition;Task analysis;Random forests","body sensor networks;electromyography;image classification;language translation;medical signal processing;random forests;sign language recognition","artificial intelligence;medical sign language;Myo armband sensor;MAST parameters;human hand activity classification;Myo armband sign-language translator;dynamic random forest;electromyography signals;muscles","","","","12","","21 Dec 2020","","","IEEE","IEEE Conferences"
"Integrating Speech Recognition and Machine Translation: Where do We Stand?","E. Matusov; S. Kanthak; H. Ney","Lehrstuhl für Informatik VI - Computer Science Department, RWTH Aachen University, Aachen, Germany. matusov@nformatik.rwth-aachen.de; Dept. of Comput. Sci., RWTH Aachen Univ.; Dept. of Comput. Sci., RWTH Aachen Univ.","2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings","24 Jul 2006","2006","5","","V","V","This paper describes state-of-the-art interfaces between speech recognition and machine translation. We modify two different machine translation systems to effectively process dense speech recognition lattices. In addition, we describe how to fully integrate speech translation with machine translation based on weighted finite-state transducers. With a thorough set of experiments, we show that both the acoustic model scores and the source language model positively and significantly affect the translation quality. We have found consistent improvements on three different corpora compared with translations of single best recognition results","2379-190X","1-4244-0469-X","10.1109/ICASSP.2006.1661501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661501","","Speech recognition;Automatic speech recognition;Lattices;Natural languages;Computer science;Acoustic transducers","language translation;speech recognition","speech recognition;machine translation;weighted finite-state transducers;acoustic model;source language model","","14","2","14","","24 Jul 2006","","","IEEE","IEEE Conferences"
"Response generation based on statistical machine translation for speech-oriented guidance system","K. Nishimura; H. Kawanami; H. Saruwatari; K. Shikano","Graduate School of Information Science, Nara Institute of science and Technology, Japan; Graduate School of Information Science, Nara Institute of science and Technology, Japan; Graduate School of Information Science, Nara Institute of science and Technology, Japan; Graduate School of Information Science, Nara Institute of science and Technology, Japan","Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference","17 Jan 2013","2012","","","1","4","An example-based response generation is a robust and practical approach for a real-environment information guidance system. However, this framework cannot reflect differences in nuance, because the set of answer sentences are fixed beforehand. To overcome this issue, we have proposed response generation using a statistical machine translation technique. In this paper, we make use of N-best speech recognition candidates instead of manual transcription used in our previous study. As a result, the generation rate of appropriate response sentences was improved by using multiple recognition hypothesis.","","978-0-6157-0050-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6411808","","Speech recognition;Training data;Training;Weather forecasting;Manuals;Analytical models","speech recognition;statistical analysis","response generation;speech-oriented guidance system;example-based response generation;real-environment information guidance system;statistical machine translation technique;N-best speech recognition;manual transcription;multiple recognition hypothesis","","","","5","","17 Jan 2013","","","IEEE","IEEE Conferences"
"Robust SAR Automatic Target Recognition Via Adversarial Learning","Y. Guo; L. Du; D. Wei; C. Li","National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","5 Jan 2021","2021","14","","716","729","The traditional denoising methods in noise robust synthetic aperture radar (SAR) automatic target recognition research are independent of the recognition model, which limits the robust recognition performance. In this article, we present a robust SAR automatic target recognition method via adversarial learning, which could integrate data denoising, feature extraction, and classification into a unified framework for joint learning. Different from the common recognition methods of directly inputting the SAR data into the classifiers, we add a dual-generative-adversarial-network (GAN) model between the SAR data and the classifier for data translation from a noise-polluted style to a relatively clean style to reduce the noise from SAR data. In order to ensure the target information in the SAR data can be retained during the data style translation, reconstruction constraint and label constraint are also used in the dual-GAN model. Then, the more reliable transferred SAR data are fed into the classifier. The parameters of the dual-GAN and classifier are learned through joint optimization in our method. Thus, the data separability is guaranteed in the process of denoising and feature extraction, which greatly improves the recognition performance of the method. In addition, our method can be easily extended to a semisupervised method by using different objective functions for labeled and unlabeled training data, which is more suitable for practical application. Experimental results on MSTAR dataset and Gotcha dataset show that our method can get the encouraging performance in the case of low signal-to-noise ratio and small labeled data size.","2151-1535","","10.1109/JSTARS.2020.3039235","National Natural Science Foundation of China; Higher Education Discipline Innovation Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264634","Adversarial learning;automatic target recognition (ATR);generative adversarial networks (GAN);noise robust;semisupervised learning;synthetic aperture radar (SAR)","Synthetic aperture radar;Feature extraction;Generative adversarial networks;Gallium nitride;Training;Deep learning;Target recognition","feature extraction;image classification;image denoising;learning (artificial intelligence);neural nets;radar computing;radar imaging;radar target recognition;synthetic aperture radar","reliable transferred SAR data;dual-GAN model;data style translation;noise-polluted style;data translation;dual-generative-adversarial-network model;common recognition methods;data denoising;robust SAR automatic target recognition method;robust recognition performance;recognition model;noise robust synthetic aperture radar automatic target recognition research;traditional denoising methods;adversarial learning;labeled data size;unlabeled training data;labeled training data;data separability","","","","51","CCBYNCND","19 Nov 2020","","","IEEE","IEEE Journals"
"Neural model of visual selective attention for automatic translation invariant object recognition in cluttered images","E. W. Chong; Cheng-Chew Lim; P. Lozo","Dept. of Electr. & Electron. Eng., Adelaide Univ., SA, Australia; NA; NA","1999 Third International Conference on Knowledge-Based Intelligent Information Engineering Systems. Proceedings (Cat. No.99TH8410)","6 Aug 2002","1999","","","373","376","This paper presents a biologically inspired neural model for detecting, locating and recognising all known objects in the visual scene automatically. In particular, this model employs bottom-up segmentation to achieve shifts in spatial attention, for selecting potential regions of interest across the visual scene.","","0-7803-5578-4","10.1109/KES.1999.820201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=820201","","Object recognition;Layout;Biological system modeling;Neural networks;Image segmentation;Brain modeling;Australia;Object detection;Humans;Visual perception","neural net architecture;computer vision;invariance;object recognition;noise;object detection;image segmentation","biologically inspired neural model;visual selective attention;automatic translation-invariant object recognition;cluttered images;object detection;object location;object recognition;visual scene;bottom-up image segmentation;spatial attention shifts;potential region-of-interest selection","","2","","10","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Enchanting Your Noodles: GAN-based Real-time Food-to-Food Translation and Its Impact on Vision-induced Gustatory Manipulation","K. Nakano; D. Horita; N. Sakata; K. Kiyokawa; K. Yanai; T. Narumi","Nara Institute of Science and Technology, Japan; The University of Electro-Communications, Japan; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; The University of Electro-Communications, Japan; The University of Tokyo, Japan","2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)","15 Aug 2019","2019","","","1096","1097","We propose a novel gustatory manipulation interface which utilizes the cross-modal effect of vision on taste elicited with augmented reality (AR)-based real-time food appearance modulation using a generative adversarial network (GAN). Unlike existing systems which only change color or texture pattern of a particular type of food in an inflexible manner, our system changes the appearance of food into multiple types of food in real-time flexibly, dynamically and interactively in accordance with the deformation of the food that the user is actually eating by using GAN-based image-to-image translation. The experimental results reveal that our system successfully manipulates gustatory sensations to some extent and that the effectiveness depends on the original and target types of food as well as each user's food experience.","2642-5254","978-1-7281-1377-7","10.1109/VR.2019.8798336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798336","Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception;Machine learning;Machine learning approaches;Neural networks","Real-time systems;Visualization;Resists;Generative adversarial networks;Gallium nitride;Olfactory;Servers","augmented reality;chemioception;computer vision;food technology;visual perception","vision-induced gustatory manipulation interface;GAN-based real-time food-to-food translation;cross-modal effect;food experience;GAN-based image-to-image translation;augmented reality-based real-time food appearance modulation","","2","","3","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Automatic visual inspection of printed circuit board for defect detection and classification","V. Chaudhary; I. R. Dave; K. P. Upla","S.V. National Institute of Technology, Surat, India; S.V. National Institute of Technology, Surat, India; S.V. National Institute of Technology, Surat, India","2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)","22 Feb 2018","2017","","","732","737","Inspection of printed circuit board (PCB) has been a crucial process in the electronic manufacturing industry to guarantee product quality & reliability, cut manufacturing cost and to increase production. The PCB inspection involves detection of defects in the PCB and classification of those defects in order to identify the roots of defects. In this paper, all 14 types of defects are detected and are classified in all possible classes using referential inspection approach. The proposed algorithm is mainly divided into five stages: Image registration, Pre-processing, Image segmentation, Defect detection and Defect classification. The algorithm is able to perform inspection even when captured test image is rotated, scaled and translated with respect to template image which makes the algorithm rotation, scale and translation in-variant. The novelty of the algorithm lies in its robustness to analyze a defect in its different possible appearance and severity. In addition to this, algorithm takes only 2.528 s to inspect a PCB image. The efficacy of the proposed algorithm is verified by conducting experiments on the different PCB images and it shows that the proposed afgorithm is suitable for automatic visual inspection of PCBs.","","978-1-5090-4442-9","10.1109/WiSPNET.2017.8299858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8299858","Printed Circuit Boards;Automatic Visual Inspection;Machine Vision","Wiring;Soldering;Image segmentation;Inspection;Image registration;Feature extraction;Conferences","automatic optical inspection;electronics industry;image classification;image registration;image segmentation;inspection;printed circuit manufacture;production engineering computing","automatic visual inspection;printed circuit board;defect detection;electronic manufacturing industry;cut manufacturing cost;PCB inspection;referential inspection approach;captured test image;template image;image registration;image segmentation;defect classification;PCB images","","7","","17","","22 Feb 2018","","","IEEE","IEEE Conferences"
"Automatic Sleep Monitoring Using Ear-EEG","T. Nakamura; V. Goverdovsky; M. J. Morrell; D. P. Mandic","Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Sleep and Ventilation Unit, National Heart and Lung Institute, Imperial College London, London, U.K.; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Journal of Translational Engineering in Health and Medicine","13 Jul 2017","2017","5","","1","8","The monitoring of sleep patterns without patient's inconvenience or involvement of a medical specialist is a clinical question of significant importance. To this end, we propose an automatic sleep stage monitoring system based on an affordable, unobtrusive, discreet, and long-term wearable in-ear sensor for recording the electroencephalogram (ear-EEG). The selected features for sleep pattern classification from a single ear-EEG channel include the spectral edge frequency and multi-scale fuzzy entropy, a structural complexity feature. In this preliminary study, the manually scored hypnograms from simultaneous scalpEEG and ear-EEG recordings of four subjects are used as labels for two analysis scenarios: 1) classification of ear-EEG hypnogram labels from ear-EEG recordings; and 2) prediction of scalp-EEG hypnogram labels from ear-EEG recordings. We consider both 2-class and 4-class sleep scoring, with the achieved accuracies ranging from 78.5% to 95.2% for ear-EEG labels predicted from ear-EEG, and 76.8% to 91.8% for scalp-EEG labels predicted from ear-EEG. The corresponding Kappa coefficients range from 0.64 to 0.83 for Scenario 1, and indicate substantial to almost perfect agreement, while for Scenario 2 the range of 0.65-0.80 indicates substantial agreement, thus further supporting the feasibility of in-ear sensing for sleep monitoring in the community.","2168-2372","","10.1109/JTEHM.2017.2702558","EPSRC grant Engineering; Rosetrees Trust; EPSRC Pathways to Impact; MURI/EPSRC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959059","Wearable EEG;in-ear sensing;ear-EEG;automatic sleep classification;structural complexity analysis","Sleep;Biomedical monitoring;Electroencephalography;Monitoring;Ear;Entropy;Electrodes","body sensor networks;ear;electroencephalography;feature selection;fuzzy set theory;medical signal processing;patient monitoring;signal classification;sleep","in-ear sensing;Kappa coefficients;4-class sleep scoring;2-class sleep scoring;scalp-EEG hypnogram labels;ear-EEG hypnogram labels;ear-EEG recordings;scalp-EEG recordings;manually scored hypnograms;structural complexity feature;multiscale fuzzy entropy;spectral edge frequency;single ear-EEG channel;sleep pattern classification;electroencephalogram;long-term wearable in-ear sensor;automatic sleep stage monitoring system","","9","","24","CCBY","26 Jun 2017","","","IEEE","IEEE Journals"
"Recognition of hand configuration: A critical factor in automatic sign language translation","N. Escudeiro; P. Escudeiro; F. Soares; O. Litos; M. Norberto; J. Lopes","Departamento de Engenharia Informática - Instituto, Superior de Engenharia do Porto, Porto, Portugal; Departamento de Engenharia Informática - Instituto, Superior de Engenharia do Porto, Porto, Portugal; Departamento de Engenharia Informática - Instituto, Superior de Engenharia do Porto, Porto, Portugal; National Technical University of Athens, Greece; Departamento de Engenharia Informática - Instituto, Superior de Engenharia do Porto, Porto, Portugal; Departamento de Engenharia Informática - Instituto, Superior de Engenharia do Porto, Porto, Portugal","2017 12th Iberian Conference on Information Systems and Technologies (CISTI)","13 Jul 2017","2017","","","1","5","Identifying hand configuration is a critical feature of sign language translation. In this paper, we describe our approach to recognize hand configurations in real time with the purpose of providing accurate predictions to be used in automatic sign language translation. To capture the hand configuration we rely on data gloves with 14 sensors that measure finger joints bending. These inputs are sampled at a frequency of 100Hz and fed to a classifier that predicts the current hand configuration. The classification model is created from an annotated sample of hand configurations previously acquired. We expect this approach to be accurate and robust in the sense that the performance of the classification model should not vary significantly when the classifier is being used by one or another user. The results from our experimental evaluation show that there is a very high accuracy, meaning that data gloves are a good approach to capture the descriptive features of hand configurations. However, the robustness of such an approach is not as good as desirable since the accuracy of the classifier depends on the user, i.e., the accuracy is high when the classifier is used by a user who trained it but decreases in other cases.","","978-9-8998-4347-9","10.23919/CISTI.2017.7975724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975724","sign language;deaf;sensor sloves;classification","Assistive technology;Gesture recognition;Support vector machines;Sensors;Training;Data gloves;Robustness","data gloves;image classification;image sensors;language translation;palmprint recognition;sign language recognition","hand configuration recognition;automatic sign language translation;hand configuration Identification;data gloves;sensors;finger joints bending;classifier;classification model;hand descriptive features","","1","","17","","13 Jul 2017","","","IEEE","IEEE Conferences"
"Entering Tone Recognition in a Support Vector Machine Approach","X. Wang; Y. Liu; L. Cai","Dept. of Chinese Language & Literature, Tsinghua Univ., Beijing; Dept. of Chinese Language & Literature, Tsinghua Univ., Beijing; Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing","2008 Fourth International Conference on Natural Computation","7 Nov 2008","2008","2","","61","65","This paper introduces support vector machine classifiers into entering tone recognition. Not every syllable needs recognition in a statistical way. The recognition accuracy of syllables which need recognition in the support vector machine approach is about 90%, which makes it possible to analyze poems' rhymes and translate Mandarin into many Chinese dialects. The experiments also check the influence of context window attributes on the entering tone recognition.","2157-9563","978-0-7695-3304-9","10.1109/ICNC.2008.757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4666957","Support Vector Machine;entering tone;pinyin","Support vector machines;Support vector machine classification;Kernel;Natural languages;Computer science;Statistical learning;Handwriting recognition;Tagging;Asia;Polynomials","learning (artificial intelligence);natural languages;signal classification;speech recognition;statistical analysis;support vector machines","entering tone recognition;support vector machine classifier approach;statistical learning;syllable recognition;poem rhyme analysis;Mandarin translation;Chinese dialect;context window attribute","","2","","12","","7 Nov 2008","","","IEEE","IEEE Conferences"
"Automatic carotid centerline extraction from three-dimensional ultrasound Doppler images","S. Parrini; L. Zhang; S. Condino; V. Ferrari; D. Caramella; M. Ferrari","EndoCAS Center, Department of Translational Research on New Technologies in Medicine and Surgery, University of Pisa, 56124, Pisa, Italy; Biomedical engineering school, Capital Medical University, 10069, Beijing, China; EndoCAS Center, Department of Translational Research on New Technologies in Medicine and Surgery, University of Pisa, 56124, Pisa, Italy; EndoCAS Center, Department of Translational Research on New Technologies in Medicine and Surgery, University of Pisa, 56124, Pisa, Italy; Unit of Diagnostic Radiology of the Department of Translational Research on New Technologies in Medicine and Surgery, University of Pisa, 56124, Pisa, Italy; Unit of Vascular Surgery of the Department of Translational Research on New Technologies in Medicine and Surgery, University of Pisa, 56124, Pisa, Italy","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","6 Nov 2014","2014","","","5089","5092","Vessel lumen centerline extraction is an important issue for the intra-operative guidance of endovascular instruments; furthermore, vessel centerline is often used as a reference position in many hemodynamic studies, especially in carotid arteries. In this work we propose an innovative method for the extraction of carotid vessels centerline from three-dimensional Color Doppler ultrasound images. The method was tested on carotid Color Doppler images of eighteen healthy subjects and validated by calculating the Euclidean distances between the centerlines detected by the algorithm and those manually annotated by two experts in the corresponding original US volumes. The results show that the proposed approach can accurately estimate the actual centerline with an average error of 1.08 ± 0.54 mm. Furthermore, the method is completely automatic and therefore suitable for the aforementioned purposes.","1558-4615","978-1-4244-7929-0","10.1109/EMBC.2014.6944769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944769","","Three-dimensional displays;Biomedical imaging;Doppler effect;Surgery;Carotid arteries;Ultrasonic imaging;Educational institutions","biomedical ultrasonics;cardiology;feature extraction;haemodynamics;medical image processing","automatic carotid centerline extraction;three-dimensional ultrasound Doppler images;vessel lumen centerline extraction;intraoperative guidance;endovascular instrument;hemodynamic study;carotid vessels centerline extraction;three-dimensional color Doppler ultrasound images","Adult;Algorithms;Carotid Arteries;Carotid Artery Diseases;Female;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Ultrasonography, Doppler","3","","11","","6 Nov 2014","","","IEEE","IEEE Conferences"
"Translational Motion Tracking of Leg Joints for Enhanced Prediction of Walking Tasks","R. Stolyarov; G. Burnett; H. Herr","Department of Health Sciences and Technology, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer ScienceMassachusetts Institute of Technology; Center for Extreme BionicsMassachusetts Institute of Technology","IEEE Transactions on Biomedical Engineering","19 Mar 2018","2018","65","4","763","769","Objective: Walking task prediction in powered leg prostheses is an important problem in the development of biomimetic prosthesis controllers. This paper proposes a novel method to predict upcoming walking tasks by estimating the translational motion of leg joints using an integrated inertial measurement unit. Methods: We asked six subjects with unilateral transtibial amputations to traverse flat ground, ramps, and stairs using a powered prosthesis while inertial signals were collected. We then performed an offline analysis in which we simulated a real-time motion tracking algorithm on the inertial signals to estimate knee and ankle joint translations, and then used pattern recognition separately on the inertial and translational signal sets to predict the target walking tasks of individual strides. Results: Our analysis showed that using inertial signals to derive translational signals enabled a prediction error reduction of 6.8% compared to that attained using the original inertial signals. This result was similar to that seen by addition of surface electromyography sensors to integrated sensors in previous work, but was effected without adding any extra sensors. Finally, we reduced the size of the translational set to that of the inertial set and showed that the former still enabled a composite error reduction of 5.8%. Conclusion and Significance: These results indicate that translational motion tracking can be used to substantially enhance walking task prediction in leg prostheses without adding external sensing modalities. Our proposed algorithm can thus be used as a part of a task-adaptive and fully integrated prosthesis controller.","1558-2531","","10.1109/TBME.2017.2718528","United States Army Medical Research Acquisition Activity; National Defense Science and Engineering Graduate Fellowship; Harvard-MIT Health Sciences and Technology program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7955077","Intent recognition;inertial measurement;machine learning;wearable robotics","Legged locomotion;Prosthetics;Sensors;Feature extraction;Knee;Physiology;Acceleration","biomechanics;biomimetics;electromyography;gait analysis;medical signal processing;pattern recognition;prosthetics","translational motion tracking;leg joints;task prediction;powered leg prostheses;biomimetic prosthesis controllers;integrated inertial measurement unit;unilateral transtibial amputations;real-time motion tracking algorithm;ankle joint translations;inertial signal sets;translational signal sets;translational signals;prediction error reduction;original inertial signals;integrated sensors;translational set;inertial set;fully integrated prosthesis controller;walking tasks;knee joint translations","Adult;Aged;Amputees;Artificial Limbs;Exoskeleton Device;Female;Humans;Machine Learning;Male;Middle Aged;Movement;Pattern Recognition, Automated;Signal Processing, Computer-Assisted;Walking","16","","28","","22 Jun 2017","","","IEEE","IEEE Journals"
"A Multi-Classifier System for Automatic Mitosis Detection in Breast Histopathology Images Using Deep Belief Networks","K. S. Beevi; M. S. Nair; G. R. Bindu","Electrical and Electronics Department, Thangal Kunju Musaliar College of Engineering, Kollam, India; Department of Computer Science, University of Kerala, Thiruvananthapuram, India; Electrical Engineering Department, College of Engineering Trivandrum (CET), Thiruvananthapuram, India","IEEE Journal of Translational Engineering in Health and Medicine","5 May 2017","2017","5","","1","11","Mitotic count is an important diagnostic factor in breast cancer grading and prognosis. Detection of mitosis in breast histopathology images is very challenging mainly due to diffused intensities along object boundary and shape variation in different stages of mitosis. This paper demonstrates an accurate technique for detecting the mitotic cells in Hematoxyline and Eosin stained images by step by step refinement of segmentation and classification stages. Krill Herd Algorithm-based localized active contour model precisely segments cell nuclei from background stroma. A deep belief network based multi-classifier system classifies the labeled cells into mitotic and nonmitotic groups. The proposed method has been evaluated on MITOS data set provided for MITOS-ATYPIA contest 2014 and also on clinical images obtained from Regional Cancer Centre (RCC), Thiruvananthapuram, which is a pioneer institute specifically for cancer diagnosis and research in India. The algorithm provides improved performance compared with other state-of-the-art techniques with average F-score of 84.29% for the MITOS data set and 75% for the clinical data set from RCC.","2168-2372","","10.1109/JTEHM.2017.2694004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911274","Breast histopathology;mitosis;support vector machine;random forest;multi-classifier system;deep belief networks","Feature extraction;Training;Image segmentation;Shape;Breast cancer;Computer architecture","belief networks;cancer;cellular biophysics;diagnostic radiography;image classification;image segmentation;mammography;medical image processing","cancer diagnosis;thiruvananthapuram;MITOS data set;nonmitotic cell;cell nuclei;Krill Herd algorithm-based localized active contour model;classification stage;segmentation stage;hematoxyline-and-eosin-stained-images;mitotic cells;breast cancer prognosis;breast cancer grading;mitotic count;deep belief networks;breast histopathology images;automatic mitosis detection;multiclassifier system","","18","","48","","25 Apr 2017","","","IEEE","IEEE Journals"
"A continuous speech recognition system using finite state network and Viterbi beam search for the automatic interpretation","Nam-Yong Han; Hoi-Rin Kim; Kyu-Woong Hwang; Young-Mok Ahn; Joon-Hyung Ryoo","Electron. & Telecommun. Res. Inst., Seoul, South Korea; Electron. & Telecommun. Res. Inst., Seoul, South Korea; Electron. & Telecommun. Res. Inst., Seoul, South Korea; Electron. & Telecommun. Res. Inst., Seoul, South Korea; Electron. & Telecommun. Res. Inst., Seoul, South Korea","1995 International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1995","1","","117","120 vol.1","This paper describes a Korean continuous speech recognition system using phone based semi-continuous hidden Markov model (SCHMM) method for automatic interpretation. The task domain is hotel reservation. The system (composed of speech recognition, machine translation and speech synthesis) has the following three features. First, an embedded bootstrapping training method is used that enables us to train each phone model without the need for a phoneme segmentation database. Second, a hybrid estimation method which is composed of the forward-backward algorithm and the Viterbi algorithm is proposed for the HMM parameter estimation. Third, a between-word modeling technique is used at the function word boundaries. The recognition results in speaker independent experiments are as follows. In the case of Version 1, the continuous speech recognition result is 89.1% and in Version 2, the result is 97.6%.","1520-6149","0-7803-2431-5","10.1109/ICASSP.1995.479287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=479287","","Speech recognition;Viterbi algorithm;Gold;Hidden Markov models;Vocabulary;Linear predictive coding;Dictionaries;Cepstral analysis;Electronic mail;Spatial databases","speech recognition;parameter estimation;maximum likelihood estimation;hidden Markov models;finite state machines;hotel industry;reservation computer systems;language translation;natural languages;search problems;speech synthesis","phone based semi-continuous hidden Markov model;finite state network;Viterbi beam search;automatic interpretation;Korean continuous speech recognition system;SCHMM;hotel reservation;embedded bootstrapping training method;phone model;hybrid estimation method;forward-backward algorithm;Viterbi algorithm;HMM parameter estimation;between-word modeling technique;function word boundaries;recognition results;speaker independent experiments;language model;machine translation;speech synthesis","","","1","7","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic acoustic segmentation in N-best list rescoring for lecture speech recognition","P. Shen; X. Lu; H. Kawai","National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan; National Institute of Information and Communications Technology, Japan","2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)","4 May 2017","2016","","","1","5","Speech segmentation is important in automatic speech recognition (ASR) and machine translation (MT). Particularly in N-best list rescoring processing, generalizing N-best lists consisting of as many as candidates from a decoding lattice requires proper utterance segmentation. In lecture speech recognition, only long audio recordings are provided without any utterance segmentation information. In addition, rather than only speech event, other acoustic events, e.g., laugh, applause, etc., are included in the recordings. Traditional speech segmentation algorithms for ASR focus on acoustic cues in segmentation, while in MT, speech text segmentation algorithms pay much attention to linguistic cues. In this study, we propose a three-stage speech segmentation framework by integrating both the acoustic and linguistic cues. We tested the segmentation framework for lecture speech recognition. Our results showed the effectiveness of the proposed segmentation algorithm.","","978-1-5090-4294-4","10.1109/ISCSLP.2016.7918409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918409","Acoustic segmentation;acoustic event detection;language model;N-best list rescore","Speech;Acoustics;Decoding;Speech recognition;Algorithm design and analysis;Hidden Markov models;Lattices","acoustic signal processing;computational linguistics;language translation;speech recognition;text analysis","automatic acoustic segmentation;lecture speech recognition;automatic speech recognition;ASR;machine translation;MT;N-best list rescoring processing;N-best list generalization;lattice decoding;utterance segmentation;audio recordings;acoustic cues;speech text segmentation algorithms;linguistic cues;three-stage speech segmentation framework","","","","18","","4 May 2017","","","IEEE","IEEE Conferences"
"Cross domain automatic transcription on the TC-STAR EPPS corpus","C. Gollan; M. Bisani; S. Kanthak; R. Schluter; H. Ney","Comput. Sci. Dept., Rheinisch-Westfalische Tech. Hochschule, Aachen, Germany; Comput. Sci. Dept., Rheinisch-Westfalische Tech. Hochschule, Aachen, Germany; Comput. Sci. Dept., Rheinisch-Westfalische Tech. Hochschule, Aachen, Germany; Comput. Sci. Dept., Rheinisch-Westfalische Tech. Hochschule, Aachen, Germany; Comput. Sci. Dept., Rheinisch-Westfalische Tech. Hochschule, Aachen, Germany","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","1","","I/825","I/828 Vol. 1","This paper describes the ongoing development of the British English European Parliament Plenary Session corpus. This corpus will be part of the speech-to-speech translation evaluation infrastructure of the European TC-STAR project. Furthermore, we present first recognition results on the English speech recordings. The transcription system has been derived from an older speech recognition system built for the North-American broadcast news task. We report on the measures taken for rapid cross-domain porting and present encouraging results.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1415241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415241","","Automatic speech recognition;Speech synthesis;Speech recognition;Performance loss;Humans;Natural languages;Satellite broadcasting;Computer science;Databases;Training data","speech recognition;vocabulary;language translation;speech synthesis;recording","cross domain automatic transcription;rapid cross-domain porting;TC-STAR EPPS corpus;British English European Parliament Plenary Session corpus;speech-to-speech translation evaluation infrastructure;European TC-STAR project;speech recordings;speech recognition","","16","6","13","","9 May 2005","","","IEEE","IEEE Conferences"
"The Conceptual Design of a Novel Workstation for Seizure Prediction Using Machine Learning With Potential eHealth Applications","A. E. Teijeiro; M. Shokrekhodaei; H. Nazeran","Electrical and Computer Engineering Department, The University of Texas at El Paso, El Paso, TX, USA; Electrical and Computer Engineering Department, The University of Texas at El Paso, El Paso, TX, USA; Electrical and Computer Engineering Department, The University of Texas at El Paso, El Paso, TX, USA","IEEE Journal of Translational Engineering in Health and Medicine","4 Jun 2019","2019","7","","1","10","Recent attempts to predict refractory epileptic seizures using machine learning algorithms to process electroencephalograms (EEGs) have shown great promise. However, research in this area requires a specialized workstation. Commercial solutions are unsustainably expensive, can be unavailable in most countries, and are not designed specifically for seizure prediction research. On the other hand, building the optimal workstation is a complex task, and system instability can arise from the least obvious sources imaginable. Therefore, the absence of a template for a dedicated seizure prediction workstation in today's literature is a formidable obstacle to seizure prediction research. To increase the number of researchers working on this problem, a template for a dedicated seizure prediction workstation needs to become available. This paper proposes a novel dedicated system capable of machine learning-based seizure prediction and training for under U.S. $1000, which is significantly less expensive (U.S. $700 or more) than comparable commercial solutions. This powerful workstation will be capable of training sophisticated machine learning algorithms that can be deployed to lightweight wearable devices, which enables the creation of wearable EEG-based seizure early warning systems.","2168-2372","","10.1109/JTEHM.2019.2910063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715377","EEG analysis;EEG machine learning computer;seizure prediction;seizure prediction workstation","Brain modeling;Electroencephalography;Computational modeling;Deep learning;Graphics processing units;Workstations","electroencephalography;learning (artificial intelligence);medical signal processing","optimal workstation;specialized workstation;refractory epileptic seizures;potential eHealth applications;novel workstation;conceptual design;wearable EEG-based seizure early warning systems;sophisticated machine learning algorithms;powerful workstation;machine learning-based seizure prediction;dedicated seizure prediction workstation;seizure prediction research","","2","","10","","15 May 2019","","","IEEE","IEEE Journals"
"A 12-Lead ECG-Based System With Physiological Parameters and Machine Learning to Identify Right Ventricular Hypertrophy in Young Adults","G. -M. Lin; H. H. -S. Lu","Department of Preventive Medicine, Feinberg School of Medicine, Northwestern University, Chicago, IL, USA; Institute of Statistics, National Chiao Tung University, Hsinchu, Taiwan","IEEE Journal of Translational Engineering in Health and Medicine","3 Jun 2020","2020","8","","1","10","Objective: The presence of right ventricular hypertrophy (RVH) accounts for approximately 5-10% in young adults. The sensitivity estimated by commonly used 12-lead electrocardiographic (ECG) criteria for identifying the presence of RVH is under 20% in the general population. The aim of this study is to develop a 12-lead ECG system with the related information of age, body height and body weight via machine learning to increase the sensitivity and the precision for detecting RVH. Method: In a sample of 1,701 males, aged 17-45 years, support vector machine is used for the training of 31 parameters including age, body height and body weight in addition to 28 ECG data such as axes, intervals and wave voltages as the inputs to link the output RVH. The RVH is defined on the echocardiographic finding for young males as right ventricular anterior wall thickness > 5.5 mm. Results: On the system goal for increasing sensitivity, the specificity is controlled around 70-75% and all data tested in the proposed method show competent sensitivity up to 70.3%. The values of area under curve of receiver operating characteristic curve and precision-recall curve using the proposed method are 0.780 and 0.285, respectively, which are better than 0.518 and 0.112 using the Sokolow-Lyon voltage criterion, respectively, for detecting unspecific RVH. Conclusion: We present a method using simple physiological parameters with ECG data to effectively identify more than 70% of the RVH among young adults. Clinical Impact: This system provides a fast, precise and feasible diagnosis tool to screen RVH.","2168-2372","","10.1109/JTEHM.2020.2996370","Hualien Armed Forces General Hospital, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097893","Electrocardiographic system;right ventricular hypertrophy;support vector machine;physiological parameters;young adults","Electrocardiography;Machine learning;Training;Support vector machines;Physiology;Sensitivity;Hospitals","echocardiography;electrocardiography;medical signal detection;medical signal processing;support vector machines","Sokolow-Lyon voltage criterion;area-under-curve-of-receiver operating characteristic curve;right ventricular anterior wall thickness;echocardiographic finding;wave voltages;RVH detection;12-lead ECG-based system;right ventricular hypertrophy;young adults;physiological parameters;precision-recall curve;system goal;young males;output RVH;ECG data;support vector machine;machine learning;body weight;body height;12-lead electrocardiographic criteria;age 17.0 year to 45.0 year","","3","","57","CCBY","21 May 2020","","","IEEE","IEEE Journals"
"Machine translation system to convert Sinhala and English Braille documents into voice","K. S. Anuradha; S. Thelijjagoda","Sri Lanka Institute of Information Technology,Faculty of Graduate Studies & Research,Sri Lanka; Sri Lanka Institute of Information Technology,SLIIT Business School,Sri Lanka","2020 International Research Conference on Smart Computing and Systems Engineering (SCSE)","12 Jan 2021","2020","","","7","16","Reading Braille documents are a time-consuming and labor-intensive task. A blind person should touch every Braille letter by his or her fingers. Therefore, high sensitivity in fingers and memorizing every Braille letters are key factors in Braille reading. Due to enhancement in technology, several Optical Character Recognition (OCR) systems have been introduced for different languages in different parts of the world. However, in Sri Lanka, there are no systems that extract Sinhala or English Braille characters using OCR and convert those Braille codes to sound output. The main purpose of the research is to create a system that extracts both Sinhala and English Braille segments from a given Braille document and makes the Braille content into voice. At the beginning Embossed Sinhala or English Braille image which took from webcam or a high-resolution phone will use for image processing techniques such as gray scaling, thresholding, erosion, and dilation. Erosion and gray scaling help to eliminate the noise and the noise-free image used to detect contour. After pre-processing, the image using an OpenCV library do the segmentation and character extraction. Braille character recognition has done by taking binary to decimal equivalent numbers. Before generating voice output, recognized Braille letters should convert to corresponding language letters (either Sinhala letter or English) and mapping English letters can directly generate English words but in Sinhala need an additional step called Unicode Mapping to generate a Sinhala word. When a Braille document is in high quality, the system will reach for 95 percent accuracy level and generally, results have around 85.30 percent success rate. This software provides many usability characteristics to increase simplicity when it's using OpenCV and related technologies. Even teachers can use this to improve their teaching terminologies and explain things more clearly in their lessons.","2613-8662","978-1-7281-7249-1","10.1109/SCSE49731.2020.9313020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9313020","Braille letters;Image processing;OCR;Segmentation;Unicode mapping","Image processing;Libraries;Kernel;Image segmentation;Blindness;Thresholding (Imaging);Optical character recognition software","document image processing;feature extraction;handicapped aids;image segmentation;language translation;natural language processing;optical character recognition;speech processing","optical character recognition systems;OCR;English Braille characters;Braille codes;English Braille segments;English Braille image;high-resolution phone;image processing techniques;gray scaling;noise-free image;character extraction;Braille character recognition;voice output;Sinhala letter;English words;Sinhala word;machine translation system;English Braille documents;labor-intensive task;Braille letter;OpenCV;Webcam","","","","10","","12 Jan 2021","","","IEEE","IEEE Conferences"
"Automatic Detection and Segmentation of Text in Low Quality Thai Sign Images","W. Jirattitichareon; T. H. Chalidabhongse","Faculty of Information Technology, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand. s7066450@it.kmitl.ac.th; Faculty of Information Technology, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand. thanarat@it.kmitl.ac.th","APCCAS 2006 - 2006 IEEE Asia Pacific Conference on Circuits and Systems","10 Apr 2007","2006","","","1000","1003","A system for automatic detection and segmentation of text in low quality Thai sign images is presented in this paper. The method is designed as a part of a real-time Thai sign translator system which can be used in many applications. First, an input image is pre-processed to enhance its quality. Secondly, we apply LoG (Laplacian of Gaussian) to the image for edge detection. After edge detection, in the third step, we perform connected component labeling and morphological operations for contour filling. To detect an element in a Thai sentence, we have to set some appropriate ratios and compare them with each closed region to find consonants, vowels, tones and special symbols. Next, we employ 4-line Thai character criteria for layout analysis. Finally, we use GMM (Gaussian mixture model) to represent foreground and background, and perform color segmentation in selected color model. Finally, a method for perspective distortion correction is also performed to prepare the segmented texts be ready for further recognition process. We tested the system on 192 Thai sign images which contain total of 4681 characters. The images were captured from several environments in various lighting conditions. The detection accuracy is 90.22%","","1-4244-0386-3","10.1109/APCCAS.2006.342256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4145564","text detection;text segmentation;thai sign translation","Image segmentation;Image edge detection;Design methodology;Real time systems;Laplace equations;Labeling;Morphological operations;Filling;Text recognition;System testing","edge detection;Gaussian processes;image segmentation;Laplace transforms;real-time systems","text detection;text segmentation;real-time Thai sign translator system;Laplacian of Gaussian;edge detection;connected component labeling;contour filling;layout analysis;Gaussian mixture model;color segmentation;perspective distortion correction;192 Thai sign images","","4","","6","","10 Apr 2007","","","IEEE","IEEE Conferences"
"Grounded Sequence to Sequence Transduction","L. Specia; L. Barrault; O. Caglayan; A. Duarte; D. Elliott; S. Gella; N. Holzenberger; C. Lala; S. J. Lee; J. Libovicky; P. Madhyastha; F. Metze; K. Mulligan; A. Ostapenko; S. Palaskar; R. Sanabria; J. Wang; R. Arora","Department of Computing, Imperial College London, London, U.K.; Department of Computer Science, University of Sheffield, Sheffield, U.K.; Department of Computing, Imperial College London, London, U.K.; Department of Signal Theory and Communications, Universitat Politcnica de Catalunya, Barcelona, Spain; Department of Computer Science, University of Copenhagen, Kobenhavn, Denmark; Institute for Language, Cognition and Computation, University of Edinburgh, Edinburgh, U.K.; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA; Department of Computer Science, University of Sheffield, Sheffield, U.K.; University of Pennsylvania, Philadelphia, PA, USA; Center for Information and Language Processing, LMU Munich, Munich, Germany; Department of Computing, Imperial College London, London, U.K.; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Johns Hopkins University, Baltimore, MD, USA; Worcester Polytechnic Institute, Worcester, MA, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Computing, Imperial College London, London, U.K.; Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA","IEEE Journal of Selected Topics in Signal Processing","25 Jun 2020","2020","14","3","577","591","Speech recognition and machine translation have made major progress over the past decades, providing practical systems to map one language sequence to another. Although multiple modalities such as sound and video are becoming increasingly available, the state-of-the-art systems are inherently unimodal, in the sense that they take a single modality - either speech or text - as input. Evidence from human learning suggests that additional modalities can provide disambiguating signals crucial for many language tasks. In this article, we describe the How2 dataset , a large, open-domain collection of videos with transcriptions and their translations. We then show how this single dataset can be used to develop systems for a variety of language tasks and present a number of models meant as starting points. Across tasks, we find that building multimodal architectures that perform better than their unimodal counterpart remains a challenge. This leaves plenty of room for the exploration of more advanced solutions that fully exploit the multimodal nature of the How2 dataset , and the general direction of multimodal learning with other datasets as well.","1941-0484","","10.1109/JSTSP.2020.2998415","NSF BIGDATA; NSF CRCNS; NSF CAREER; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103248","Grounding;multimodal machine learning;speech recognition;machine translation;representation learning;summarization","Visualization;Feature extraction;Speech recognition;Task analysis;Training;Adaptation models","language translation;learning (artificial intelligence);speech recognition","machine translation;multimodal architectures;multimodal learning;grounded sequence to sequence transduction;speech recognition","","","","81","IEEE","28 May 2020","","","IEEE","IEEE Journals"
"A mobile application of American sign language translation via image processing algorithms","C. M. Jin; Z. Omar; M. H. Jaward","Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Skudai, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Skudai, Malaysia; School of Engineering, Monash University Malaysia, 47500 Bandar Sunway, Malaysia","2016 IEEE Region 10 Symposium (TENSYMP)","25 Jul 2016","2016","","","104","109","Due to the relative lack of pervasive sign language usage within our society, deaf and other verbally-challenged people tend to face difficulty in communicating on a daily basis. Our study thus aims to provide research into a sign language translator applied on the smartphone platform, due to its portability and ease of use. In this paper, a novel framework comprising established image processing techniques is proposed to recognise images of several sign language gestures. More specifically, we initially implement Canny edge detection and seeded region growing to segment the hand gesture from its background. Feature points are then extracted with Speeded Up Robust Features (SURF) algorithm, whose features are derived through Bag of Features (BoF). Support Vector Machine (SVM) is subsequently applied to classify our gesture image dataset; where the trained dataset is used to recognize future sign language gesture inputs. The proposed framework has been successfully implemented on smartphone platforms, and experimental results show that it is able to recognize and translate 16 different American Sign Language gestures with an overall accuracy of 97.13%.","","978-1-5090-0931-2","10.1109/TENCONSpring.2016.7519386","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7519386","Computer Vision;Gesture Recognition;Image Processing;Machine Learning;Sign Language","Feature extraction;Assistive technology;Gesture recognition;Image edge detection;Support vector machines;Image segmentation;Image color analysis","computer vision;edge detection;feature extraction;image classification;image segmentation;language translation;mobile computing;sign language recognition;smart phones;support vector machines","mobile application;American sign language translation;image processing algorithm;smart phone platform;feature point extraction;SURF algorithm;bag of features;BoF;support vector machine;SVM;gesture image classification;speeded up robust features algorithm;Canny edge detection;seeded region growing;hand gesture segmentation","","25","","17","","25 Jul 2016","","","IEEE","IEEE Conferences"
"Neural network joint modeling via context-dependent projection","Y. Tam; Y. Lei","Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave, Menlo Park, CA 94025, USA; Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave, Menlo Park, CA 94025, USA","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5356","5360","Neural network joint modeling (NNJM) has produced huge improvement in machine translation performance. As in standard neural network language modeling, a context-independent linear projection is applied to project a sparse input vector into a continuous representation at each word position. Because neighboring words are dependent on each other, context-independent projection may not be optimal. We propose a context-dependent linear projection approach which considers neighboring words. Experimental results showed that the proposed approach further improves NNJM by 0.5 BLEU for English-Iraqi Arabic translation in N-best rescoring. Compared to a baseline using hierarchical phrases and sparse features, NNJM with our proposed approach has achieved a 2 BLEU improvement.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7178994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178994","Neural network joint modeling;context-dependent linear projection;position-dependent linear projection;statistical machine translation","Artificial neural networks;Pragmatics;Syntactics;History","language translation;neural nets;speech recognition","neural network joint modeling;machine translation;context-independent linear projection;sparse input vector;English-Iraqi Arabic translation;N-best rescoring","","","","19","","6 Aug 2015","","","IEEE","IEEE Conferences"
"Topic-Independent Speaking-Style Transformation of Language Model for Spontaneous Speech Recognition","Y. Akita; T. Kawahara","Academic Center for Computing and Media Studies, Kyoto University, Kyoto 606-8501, Japan; Academic Center for Computing and Media Studies, Kyoto University, Kyoto 606-8501, Japan","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-33","IV-36","For language modeling of spontaneous speech, we propose a novel approach, based on the statistical machine translation framework, which transforms a document-style model to the spoken style. For better coverage and more reliable estimation, incorporation of POS (part-of-speech) information is explored in addition to lexical information. In this paper, we investigate several methods that combine POS-based model or integrate POS information in the ME (maximum entropy) scheme. They achieve significant reduction in perplexity and WER in a meeting transcription task. Moreover, the model is applied to different domains or committee meetings of different topics. As a result, even larger perplexity reduction is achieved compared with the case tested in the same domain. The result demonstrates the generality and portability of the model.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218030","language model;statistical transformation;spontaneous speech;automatic speech recognition","Natural languages;Speech recognition;Probability;Entropy;Testing;Automatic speech recognition;Differential equations;Costs;Databases;Speech synthesis","language translation;maximum entropy methods;speech recognition;statistical analysis","topic-independent speaking-style transformation;language model;spontaneous speech recognition;statistical machine translation framework;document-style model;part-of-speech information;lexical information;maximum entropy scheme","","5","","8","","4 Jun 2007","","","IEEE","IEEE Conferences"
"Efficient rule scoring for improved grapheme-based lexicons","W. Hartmann; L. Lamel; J. Gauvain","Spoken Language Processing Group, LIMSI-CNRS, 91403 Orsay, France; Spoken Language Processing Group, LIMSI-CNRS, 91403 Orsay, France; Spoken Language Processing Group, LIMSI-CNRS, 91403 Orsay, France","2014 22nd European Signal Processing Conference (EUSIPCO)","13 Nov 2014","2014","","","1477","1481","For many languages, an expert-defined phonetic lexicon may not exist. One popular alternative is the use of a grapheme-based lexicon. However, there may be a significant difference between the orthography and the pronunciation of the language. In our previous work, we proposed a statistical machine translation based approach to improving grapheme-based pronunciations. Without knowledge of true target pronunciations, a phrase table was created where each individual rule improved the likelihood of the training data when applied. The approach improved recognition accuracy, but required significant computational cost. In this work, we propose an improvement that increases the speed of the process by more than 80 times without decreasing recognition accuracy.","2076-1465","978-0-9928-6261-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6952535","automatic speech recognition;grapheme-based speech recognition;pronunciation learning","Hidden Markov models;Abstracts;Acoustics","language translation;speech recognition;statistical analysis","rule scoring;improved grapheme-based lexicons;expert-defined phonetic lexicon;orthography;statistical machine translation based approach;language pronunciation;grapheme based pronunciations;automatic speech recognition","","","","17","","13 Nov 2014","","","IEEE","IEEE Conferences"
"Automatic visual to tactile translation. II. Evaluation of the TACTile image creation system","T. P. Way; K. E. Barner","Dept. of Electr. Eng., Delaware Univ., Newark, DE, USA; NA","IEEE Transactions on Rehabilitation Engineering","6 Aug 2002","1997","5","1","95","105","This is the second part of a two-part paper that develops a method for the automatic conversion of images from visual to tactile form. In Part I (see ibid. vol.5, no.1, p.81-94 (1997)), a variety of topics mere reviewed including issues in human factors, access technology for tactile graphics production, and image processing. In this part, the material presented in the first part is used to motivate, develop, and support the methods used in the development of a prototype visual-to-tactile translator called the TACTile Image Creation System (TACTICS). The specific choices made in the design of the system are discussed and justified, including selection of software platform, tactile output format, tactile image creation procedure, aggregate image processing sequences used, and principles from the discipline of psychophysics. The results of four experiments on tactile image discrimination, identification, and comprehension are reported and discussed, and future directions in this area are proposed.","1558-0024","","10.1109/86.559354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559354","","Image processing;Image converters;Graphics;Production;Prototypes;Aggregates;Bandwidth;Hardware;Software algorithms;Human factors","mechanoception;handicapped aids;image sequences;computer graphic equipment;image representation;human factors;image classification;computer vision","Automatic visual to tactile translation;TACTile image creation system;visual-to-tactile translator;TACTICS;design;software platform;tactile output format;tactile image creation procedure;aggregate image processing sequences;psychophysics;tactile image discrimination;identification;comprehension","Adult;Algorithms;Blindness;Female;Humans;Image Processing, Computer-Assisted;Male;Middle Aged;Psychophysics;Sensory Aids;Software Validation;Touch;User-Computer Interface;Visual Perception","21","","","","6 Aug 2002","","","IEEE","IEEE Journals"
"Improved punctuation recovery through combination of multiple speech streams","J. Miranda; J. P. Neto; A. W. Black","INESC-ID / Instituto Superior Técnico, Lisboa, Portugal; INESC-ID / Instituto Superior Técnico, Lisboa, Portugal; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA","2013 IEEE Workshop on Automatic Speech Recognition and Understanding","9 Jan 2014","2013","","","132","137","In this paper, we present a technique to use the information in multiple parallel speech streams, which are approximate translations of each other, in order to improve performance in a punctuation recovery task. We first build a phraselevel alignment of these multiple streams, using phrase tables to link the phrase pairs together. The information so collected is then used to make it more likely that sentence units are equivalent across streams. We applied this technique to a number of simultaneously interpreted speeches of the European Parliament Committees, for the recovery of the full stop, in four different languages (English, Italian, Portuguese and Spanish). We observed an average improvement in SER of 37% when compared to an existing baseline, in Portuguese and English.","","978-1-4799-2756-2","10.1109/ASRU.2013.6707718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707718","speech recognition;machine translation;punctuation;multistream;combination","Speech;Lattices;Speech recognition;Feature extraction;Europe;Measurement;Entropy","language translation;natural language processing;speech processing;speech recognition","automatic speech recognition;ASR;machine translation;SER improvement;Spanish language;Portuguese language;Italian language;English language;full stop recovery;European Parliament Committees;simultaneously interpreted speeches;sentence units;phrase pairs;phrase tables;phrase-level alignment;approximate translations;multiple parallel speech streams;multiple speech stream combination;punctuation recovery","","1","","18","","9 Jan 2014","","","IEEE","IEEE Conferences"
"Noise adaptive training using a vector taylor series approach for noise robust automatic speech recognition","O. Kalinli; M. L. Seltzer; A. Acero","Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA; Microsoft Corporation, One Microsoft Way, Redmond, WA 98052, USA","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","3825","3828","In traditional methods for noise robust automatic speech recognition, the acoustic models are typically trained using clean speech or using multi-condition data that is processed by the same feature enhancement algorithm expected to be used in decoding. In this paper, we propose a noise adaptive training (NAT) algorithm that can be applied to all training data that normalizes the environmental distortion as part of the model training. In contrast to the feature enhancement methods, NAT estimates the underlying ldquopseudo-cleanrdquo model parameters directly without relying on point estimates of the clean speech features as an intermediate step. The pseudo-clean model parameters learned with NAT are later used with vector Taylor series (VTS) model adaptation for decoding noisy utterances at test time. Experiments performed on the Aurora 2 and Aurora 3 tasks, demonstrate that the proposed NAT method obtain relative improvements of 18.83% and 32.02%, respectively, over VTS model adaptation.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960461","Noise adaptive training;model adaptation;robust automatic speech recognition;vector Taylor series","Taylor series;Noise robustness;Automatic speech recognition;Acoustic noise;Network address translation;Speech enhancement;Decoding;Adaptation model;Speech processing;Working environment noise","nonlinear equations;speech recognition","noise adaptive training;vector Taylor series approach;noise robust automatic speech recognition;acoustic models;multi-condition data;feature enhancement algorithm;pseudo-clean model parameters","","33","","9","","26 May 2009","","","IEEE","IEEE Conferences"
"Speech Enhancement Parameter Adjustment to Maximize Accuracy of Automatic Speech Recognition","T. Kawase; M. Okamoto; T. Fukutomi; Y. Takahashi","Media Intelligence Laboratories, NTT Corporation, Kanagawa, Japan; Media Intelligence Laboratories, NTT Corporation, Kanagawa, Japan; Media Intelligence Laboratories, NTT Corporation, Kanagawa, Japan; Media Intelligence Laboratories, NTT Corporation, Kanagawa, Japan","IEEE Transactions on Consumer Electronics","23 Apr 2020","2020","66","2","125","133","Consumer electronics equipped with a microphone array, such as car navigation devices and headsets commonly implement speech enhancement techniques based on the gradient method to cope with additive noise. However, while these techniques had been originally developed for voice communication and can maximize the signal-to-distortion ratio (SDR), they cannot always maximize automatic speech recognition (ASR) accuracy. For this reason, the front-end speech enhancement parameters have been adjusted by human experts to each environment and acoustic model. In this study, we developed a novel system for maximizing the accuracy of a given ASR engine by automatically adjusting the front-end speech enhancement. The proposed method allows consumers to use ASR through the consumer electronics with less stress when ambient noise varies. A genetic algorithm (GA) is used to generate parameter values of the front-end speech enhancement for particular environments. The generated values can be dynamically assigned to input speech signals by preliminarily clustering the environments based on noise features. In evaluations, parameter values determined by our method outperformed one adjusted by a human expert.","1558-4127","","10.1109/TCE.2020.2986003","“Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology—(I. Research and Development of Multilingual Speech Translation Technology)” from the Ministry of Internal Affairs and Communications of Japan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9061056","Array signal processing;speech enhancement;automatic speech recognition;genetic algorithms;voice interface","Speech enhancement;Microphone arrays;Genetic algorithms;Adaptation models;Acoustics;Performance evaluation","acoustic signal processing;consumer electronics;genetic algorithms;gradient methods;speech enhancement;speech recognition","genetic algorithm;acosutic model;environment model;SDR maximization;voice communication;input speech signals;ASR engine;front-end speech enhancement parameters;automatic speech recognition accuracy;signal-to-distortion ratio;additive noise;consumer electronics;speech enhancement parameter adjustment;parameter values","","2","","47","IEEE","8 Apr 2020","","","IEEE","IEEE Journals"
"Acoustic Analysis of Inhaler Sounds From Community-Dwelling Asthmatic Patients for Automatic Assessment of Adherence","M. S. Holmes; S. D'arcy; R. W. Costello; R. B. Reilly","Trinity Centre for Bioengineering, Trinity College Dublin, Dublin, Ireland; Trinity Centre for Bioengineering, Trinity College Dublin, Dublin, Ireland; Royal College of Surgeons in Ireland, Pulmonary Function Unit, Beaumont Hospital, Dublin, Ireland; Trinity Centre for Bioengineering, Trinity College Dublin, Dublin, Ireland","IEEE Journal of Translational Engineering in Health and Medicine","20 May 2017","2014","2","","1","10","Inhalers are devices which deliver medication to the airways in the treatment of chronic respiratory diseases. When used correctly inhalers relieve and improve patients' symptoms. However, adherence to inhaler medication has been demonstrated to be poor, leading to reduced clinical outcomes, wasted medication, and higher healthcare costs. There is a clinical need for a system that can accurately monitor inhaler adherence as currently no method exists to evaluate how patients use their inhalers between clinic visits. This paper presents a method of automatically evaluating inhaler adherence through acoustic analysis of inhaler sounds. An acoustic monitoring device was employed to record the sounds patients produce while using a Diskus dry powder inhaler, in addition to the time and date patients use the inhaler. An algorithm was designed and developed to automatically detect inhaler events from the audio signals and provide feedback regarding patient adherence. The algorithm was evaluated on 407 audio files obtained from 12 community dwelling asthmatic patients. Results of the automatic classification were compared against two expert human raters. For patient data for whom the human raters Cohen's kappa agreement score was , results indicated that the algorithm's accuracy was 83% in determining the correct inhaler technique score compared with the raters. This paper has several clinical implications as it demonstrates the feasibility of using acoustics to objectively monitor patient inhaler adherence and provide real-time personalized medical care for a chronic respiratory illness.","2168-2372","","10.1109/JTEHM.2014.2310480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6762909","Acoustics;adherence;algorithm;chronic respiratory diseases;inhaler","Acoustics;Algorithm design and analysis;Diseases;Training;Respiratory diseases;Biomedical monitoring;Classification algorithms","bioacoustics;biomedical equipment;biomedical measurement;diseases;drug delivery systems;feedback;medical signal detection;medical signal processing;patient monitoring;pneumodynamics;psychology;signal classification;telemedicine;telemetry","acoustic analysis;inhaler sound analysis;community-dwelling asthmatic patients;automatic inhaler adherence assessment;inhaler devices;medication delivery;chronic respiratory disease treatment;inhaler medication adherence;clinical outcome reduction;high healthcare costs;accurate inhaler adherence monitoring;acoustic monitoring device;Diskus dry powder inhaler;inhaler use time;inhaler use date;algorithm design;algorithm development;automatic inhaler event detection;audio signal detection;patient adherence feedback;audio files;automatic classification;human rater Cohen kappa agreement score;algorithm accuracy;correct inhaler technique score;objective patient inhaler adherence monitoring;real-time personalized medical care","","36","","","","20 May 2017","","","IEEE","IEEE Journals"
"Phonetically-oriented word error alignment for speech recognition error analysis in speech translation","N. Ruiz; M. Federico","Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy","2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)","11 Feb 2016","2015","","","296","302","We propose a variation to the commonly used Word Error Rate (WER) metric for speech recognition evaluation which incorporates the alignment of phonemes, in the absence of time boundary information. After computing the Levenshtein alignment on words in the reference and hypothesis transcripts, spans of adjacent errors are converted into phonemes with word and syllable boundaries and a phonetic Levenshtein alignment is performed. The phoneme alignment information is used to correct the word alignment labels in each error region. We demonstrate that our Phonetically-Oriented Word Error Rate (POWER) yields similar scores to WER with the added advantages of better word alignments and the ability to capture one-to-many alignments corresponding to homophonic errors in speech recognition hypotheses. These improved alignments allow us to better trace the impact of Levenshtein error types in speech recognition on downstream tasks such as speech translation.","","978-1-4799-7291-3","10.1109/ASRU.2015.7404808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404808","automatic speech recognition;speech translation;mixed-effects models;error analysis","Speech recognition;Error analysis;Speech;Measurement;Matrices;Pragmatics;Analytical models","error analysis;speech recognition;word processing","phonetically-oriented word error alignment;speech recognition error analysis;speech translation;word error rate metric;WER metric;speech recognition evaluation;time boundary information;syllable boundaries;phonetic Levenshtein alignment;phoneme alignment information;word alignment labels;phonetically-oriented word error rate;POWER;speech recognition hypothesis","","2","","17","","11 Feb 2016","","","IEEE","IEEE Conferences"
"Thai automatic speech recognition","S. Suebvisai; P. Charoenpornsawat; A. Black; M. Woszczyna; T. Schultz","Interactive Syst. Labs., Carnegie Mellon Univ., Pittsburgh, PA, USA; Interactive Syst. Labs., Carnegie Mellon Univ., Pittsburgh, PA, USA; Interactive Syst. Labs., Carnegie Mellon Univ., Pittsburgh, PA, USA; NA; NA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","1","","I/857","I/860 Vol. 1","We describe the development of a robust and flexible Thai speech recognizer as integrated into our English-Thai speech-to-speech translation system. We focus on the discussion of the rapid deployment of ASR for Thai under limited time and data resources, including rapid data collection issues, acoustic model bootstrap, and automatic generation of pronunciations. Issues relating to the translation and overall system will be reported elsewhere.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1415249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415249","","Automatic speech recognition;Natural languages;Loudspeakers;Speech recognition;Laboratories;Cepstral analysis;Robustness;Biomedical acoustics;Dictionaries;Character generation","speech recognition;language translation;speech synthesis","automatic speech recognition;Thai speech recognizer;English-Thai translation;speech-to-speech translation system;rapid deployment;rapid data collection;acoustic model bootstrap;automatic pronunciation generation","","7","","8","","9 May 2005","","","IEEE","IEEE Conferences"
"Learning online alignments with continuous rewards policy gradient","Y. Luo; C. Chiu; N. Jaitly; I. Sutskever","Tsinghua University, China; Google Brain, United States of America; Google Brain, United States of America; Open AI, China","2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","19 Jun 2017","2017","","","2801","2805","Sequence-to-sequence models with soft attention had significant success in machine translation, speech recognition, and question answering. Though capable and easy to use, they require that the entirety of the input sequence is available at the beginning of inference, an assumption that is not valid for instantaneous translation and speech recognition. To address this problem, we present a new method for solving sequence-to-sequence problems using hard online alignments instead of soft offline alignments. The online alignments model is able to start producing outputs without the need to first process the entire input sequence. A highly accurate online sequence-to-sequence model is useful because it can be used to build an accurate voice-based instantaneous translator. Our model uses hard binary stochastic decisions to select the timesteps at which outputs will be produced. The model is trained to produce these stochastic decisions using a standard policy gradient method. In our experiments, we show that this model achieves encouraging performance on TIMIT and Wall Street Journal (WSJ) speech recognition datasets.","2379-190X","978-1-5090-4117-6","10.1109/ICASSP.2017.7952667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7952667","Automatic Speech Recognition;End-to-End Speech Recognition;Very Deep Convolutional Neural Networks","Mathematical model;Speech recognition;Predictive models;Entropy;Stochastic processes;Training;Computational modeling","decision theory;gradient methods;language translation;speech recognition;stochastic processes","continuous rewards policy gradient method;machine translation;instantaneous translation;hard online alignments;online sequence-to-sequence model;voice-based instantaneous translator;hard binary stochastic decisions;TIMIT speech recognition datasets;Wall Street Journal speech recognition datasets","","3","1","25","","19 Jun 2017","","","IEEE","IEEE Conferences"
"Variational Bayesian approach for automatic generation of HMM topologies","T. Jitsuhiro; S. Nakamura","ATR Spoken Language Translation Res. Labs., Kyoto, Japan; ATR Spoken Language Translation Res. Labs., Kyoto, Japan","2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721)","2 Aug 2004","2003","","","77","82","We propose a new method of automatically creating non-uniform, context-dependent HMM topologies by using the variational Bayesian (VB) approach. The maximum likelihood (ML) criterion is generally used to create HMM topologies. However, it has an overfitting problem. Information criteria have been used to overcome this problem, but, theoretically, they cannot be applied to complicated models like HMMs. Recently, to avoid these problems, a VB approach has been developed in the machine-learning field. The successive state splitting (SSS) algorithm is a method of creating contextual and temporal variations for HMMs. We introduce the VB approach to the SSS algorithm, and define the prior and posterior probability densities and free energy as split and stop criteria. Experimental results show that the proposed method can automatically create the proper model and obtain better performance, especially for vowels, than the original method.","","0-7803-7980-2","10.1109/ASRU.2003.1318407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1318407","","Bayesian methods;Hidden Markov models;Topology;Maximum likelihood estimation;Clustering algorithms;Decision trees;Speech recognition;Training data;Natural languages;Laboratories","hidden Markov models;speech recognition;Bayes methods;topology;variational techniques;learning (artificial intelligence)","variational Bayesian approach;HMM topologies;maximum likelihood criterion;overfitting problem;machine-learning;successive state splitting algorithm;contextual variations;temporal variations;probability densities;free energy;split criteria;stop criteria;vowels;speech recognition","","2","","11","","2 Aug 2004","","","IEEE","IEEE Conferences"
"Automatic generation of non-uniform HMM structures based on variational Bayesian approach","T. Jitsuhiro; S. Nakamura","ATR Spoken Language Translation Res. Labs., Kyoto, Japan; ATR Spoken Language Translation Res. Labs., Kyoto, Japan","2004 IEEE International Conference on Acoustics, Speech, and Signal Processing","30 Aug 2004","2004","1","","I","805","We propose using the variational Bayesian (VB) approach for automatically creating nonuniform, context-dependent HMM topologies in speech recognition. The maximum likelihood (ML) criterion is generally used to create HMM topologies. However, it has an over-fitting problem. Information criteria have been used to overcome this problem, but theoretically they cannot be applied to complicated models like HMM. Recently, to avoid these problems, the VB approach has been developed in the machine-learning field. We introduce the VB approach to the successive state splitting (SSS) algorithm, which can create both contextual and temporal variations for HMM. We define the prior and posterior probability densities and free energy with latent variables as split and stop criteria. Experimental results show that the proposed method can automatically create a more efficient model and obtain better performance, especially for vowels, than the original method.","1520-6149","0-7803-8484-9","10.1109/ICASSP.2004.1326108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1326108","","Hidden Markov models;Bayesian methods;Clustering algorithms;Maximum likelihood estimation;Topology;Speech recognition;Decision trees;Training data;Natural languages;Laboratories","hidden Markov models;Bayes methods;speech recognition;topology;variational techniques","automatic generation;nonuniform HMM structures;variational Bayesian approach;context-dependent HMM topologies;successive state splitting algorithm;SSS algorithm;vowel performance;speech recognition","","4","","9","","30 Aug 2004","","","IEEE","IEEE Conferences"
"Comparison of Data Augmentation and Adaptation Strategies for Code-switched Automatic Speech Recognition","M. Ma; B. Ramabhadran; J. Emond; A. Rosenberg; F. Biadsy","Google Inc., USA; Google Inc., USA; Google Inc., USA; Google Inc., USA; Google Inc., USA","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","6081","6085","Code-switching occurs when the speaker alternates between two or more languages or dialects. It is a pervasive phenomenon in most Indic spoken languages. Code-switching poses a challenge in language modeling as it complicates the orthographic realization of text, and generally, there is a shortage of code-switched data. In this paper, we investigate data augmentation and adaptation strategies for language modeling. Using Bengali and English as an example, we study augmenting the code-switched transcripts with separate transliterated Bengali and English corpora. We present results on two speech recognition tasks, namely, voice search and dictation. We show improvements on both tasks with Maximum Entropy (MaxEnt) and Long Short-Term Memory (LSTM) language models (LMs). We also explore different adaptation strategies for MaxEnt LM and LSTM LM, demonstrating that the transliteration-based data-augmented LSTM LM matches the adapted MaxEnt LM which is trained on more Bengali-English data.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682824","data augmentation;language model adaptation;code-switched automatic speech recognition","Adaptation models;Data models;Poles and towers;Writing;Predictive models;Training;Training data","entropy;speech coding;speech recognition","code-switched data;data augmentation;code-switched transcripts;transliteration-based data-augmented LSTM LM;adapted MaxEnt LM;Bengali-English data;code-switched automatic speech recognition;pervasive phenomenon;long short-term memory language models;Indic spoken language modeling;maximum entropy","","","","30","","17 Apr 2019","","","IEEE","IEEE Conferences"
"Japanese named entity recognition for question answering system","Y. Liu; F. Ren","Graduate School of Advanced Technology and Science, The University of Tokushima, Tokushima, Japan, Faculty of Engineering, The University of Tokushima, Japan; Graduate School of Advanced Technology and Science, The University of Tokushima, Tokushima, Japan, Faculty of Engineering, The University of Tokushima, Japan","2011 IEEE International Conference on Cloud Computing and Intelligence Systems","13 Oct 2011","2011","","","402","406","Current question answering (QA) systems usually contain named entity recognizer (NER) as a core component. NER is an important and difficult task in computational linguistics. It plays an important role in natural language processing application such as Question Answering, Machine Translation, and Information Retrieval etc. NER includes the identification and classification of certain proper nouns (like location, organization, person, data, money and others) in a text. The purpose of our study is to recognize and extract the exact Japanese sight seeing domain named entities. It is a basic step for the following processing: question analysis and keyword extraction information retrieval. As well as, through doing the named entity recognition, we consider that it can mine exact information from text document to respond to user. This paper describes how to do the Japanese sightseeing named entity recognition due to we are constructing a Japanese sightseeing question answering system. We adopt the hybrid method which combined with machine learning and rule-base method. In the experiment of Japanese sightseeing domain named entity recognition we have got excellent precision and recalling rates. It shows that our method is effective and can be used in a practical question answering system.","2376-595X","978-1-61284-204-2","10.1109/CCIS.2011.6045098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045098","question answering system;named entity recognition;machine learning;rule-base","Organizations;Educational institutions;Training;Text recognition;Machine learning;Support vector machines;Data mining","information retrieval;language translation;natural language processing;speech recognition","Japanese named entity recognition;question answering system;QA;NER;computational linguistics;natural language processing;machine translation;keyword extraction information retrieval;text document","","3","","15","","13 Oct 2011","","","IEEE","IEEE Conferences"
"Scalable Multilingual Frontend for TTS","A. Conkie; A. Finch",Apple; Apple,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","6684","6688","This paper describes progress towards making a Neural Text-to-Speech (TTS) Frontend that works for many languages and can be easily extended to new languages. We take a Machine Translation (MT) inspired approach to constructing the frontend, and model both text normalization and pronunciation on a sentence level by building and using sequence-to-sequence (S2S) models. We experimented with training normalization and pronunciation as separate S2S models and with training a single S2S model combining both functions. For our language-independent approach to pronunciation we do not use a lexicon. Instead all pronunciations, including context-based pronunciations, are captured in the S2S model. We also present a language-independent chunking and splicing technique that allows us to process arbitrary-length sentences. Models for 18 languages were trained and evaluated. Many of the accuracy measurements are above 99%. We also evaluated the models in the context of end-to-end synthesis against our current production system.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054560","speech synthesis;machine learning","Training;Production systems;Splicing;Data models;Iron;Speech processing;Context modeling","language translation;natural language processing;neural nets;speech recognition;speech synthesis;text analysis","TTS;machine translation;model both text normalization;pronunciation;training normalization;single S2S model;context-based pronunciations;language-independent chunking;splicing technique;arbitrary-length sentences;text-to-speech frontend","","1","","20","","9 Apr 2020","","","IEEE","IEEE Conferences"
"American Sign Language Recognition using Deep Learning and Computer Vision","K. Bantupalli; Y. Xie","Department of Computer Science, Kennesaw State University, Kennesaw, USA; Department of Computer Science, Kennesaw State University, Kennesaw, USA","2018 IEEE International Conference on Big Data (Big Data)","24 Jan 2019","2018","","","4896","4899","Speech impairment is a disability which affects an individuals ability to communicate using speech and hearing. People who are affected by this use other media of communication such as sign language. Although sign language is ubiquitous in recent times, there remains a challenge for non-sign language speakers to communicate with sign language speakers or signers. With recent advances in deep learning and computer vision there has been promising progress in the fields of motion and gesture recognition using deep learning and computer vision based techniques. The focus of this work is to create a visionbased application which offers sign language translation to text thus aiding communication between signers and non-signers. The proposed model takes video sequences and extracts temporal and spatial features from them. We then use Inception, a CNN (Convolutional Neural Network) for recognizing spatial features. We then use a RNN (Recurrent Neural Network) to train on temporal features. The dataset used is the American Sign Language Dataset.","","978-1-5386-5035-6","10.1109/BigData.2018.8622141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622141","computer science;machine learning;computer vision;sign language","Gesture recognition;Assistive technology;Feature extraction;Training;Hidden Markov models;Conferences;Recurrent neural networks","computer vision;convolutional neural nets;feature extraction;gesture recognition;handicapped aids;image sequences;language translation;learning (artificial intelligence);recurrent neural nets;sign language recognition","deep learning;computer vision;gesture recognition;sign language translation;American sign language recognition;speech impairment;nonsign language;sign language speakers;motion recognition;American sign language dataset;video sequences;temporal feature extraction;spatial feature extraction;Inception;CNN;convolutional neural network;spatial feature recognition;RNN;recurrent neural network","","14","","10","","24 Jan 2019","","","IEEE","IEEE Conferences"
"Sign language localization: Learning to eliminate language dialects","M. Tariq; A. Iqbal; A. Zahid; Z. Iqbal; J. Akhtar","Department of Computer Science, Kinnaird College for Women, Lahore, Pakistan; Department of Computer Science, Kinnaird College for Women, Lahore, Pakistan; Department of Computer Science, Kinnaird College for Women, Lahore, Pakistan; Department of Computer Science, Kinnaird College for Women, Lahore, Pakistan; Department of Computer Science, School of Science and Engineering, LUMS, Lahore, Pakistan","2012 15th International Multitopic Conference (INMIC)","2 May 2013","2012","","","17","22","Machine translation of sign language into spoken languages is an important yet non-trivial task. The sheer variety of dialects that exist in any sign language makes it only harder to come up with a generalized sign language classification system. Though a lot of work has been done in this area previously but most of the approaches rely on intrusive hardware in the form of wired or colored gloves or are specific language/dialect dependent for accurate sign language interpretation. We propose a cost-effective, non-intrusive webcam based solution in which a person from any part of the world can train our system to make it learn the sign language in their own specific dialect, so that our software can then correctly translate the hand signs into a commonly spoken language, such as English. Image based hand gesture recognition carries sheer importance in this task. The heart of hand gesture recognition systems is the detection and extraction of the sign (hand gesture) from the input image stream. Our work uses functions like skin color based thresholding, contour detection and convexity defect for detection of hands and identification of important points on the hand respectively. The distance of these important contour points from the centroid of the hand becomes our feature vector against which we train our neural network. The system works in two phases. In the training phase the correspondence between users hand gestures against each sign language symbol is learnt using a feed forward neural network with back propagation learning algorithm. Once the training is complete, user is free to use our system for translation or communication with other people. Experimental results based on training and testing the system with numerous users show that the proposed method can work well for dialect-free sign language translation (numerals and alphabets) and gives us average recognition accuracies of around 65% and 55% with the maximum recognition accuracies rising upto 77% and 62% respectively.","","978-1-4673-2252-2","10.1109/INMIC.2012.6511463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511463","Sign Language Recognition;Dialect Independence;Machine Translation;Digital Image Processing;Artificial Neural Networks","","backpropagation;feature extraction;feedforward neural nets;image colour analysis;image segmentation;language translation;sign language recognition","sign language localization;language dialects;machine translation;sign language classification system;nonintrusive Webcam based solution;sign detection;sign extraction;hand gesture recognition systems;skin color based thresholding;image stream;contour points;feature vector;sign language symbol;feed forward neural network;back propagation learning algorithm;contour detection;convexity defect","","4","","11","","2 May 2013","","","IEEE","IEEE Conferences"
"Cross-View Action Recognition Based on a Statistical Translation Framework","J. Wang; H. Zheng; J. Gao; J. Cen","School of Information Science and Technology, Sun Yat-sen University, Guangzhou, China; School of Information Science and Technology, Sun Yat-sen University, Guangzhou, China; School of Information Science and Technology, Sun Yat-sen University, Guangzhou, China; School of Information Science and Technology, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Circuits and Systems for Video Technology","2 Aug 2016","2016","26","8","1461","1475","Actions captured under view changes pose serious challenges to modern action recognition methods. In this paper, we propose an effective approach for cross-view action recognition based on a statistical translation framework, which boils down to estimation of visual word transfer probabilities across views. Specifically, local features are extracted from action video frames and form bags of words based on k-means clustering. Though the appearance of an action may vary due to view changes, the underlying transfer tendency between visual words across views can be exploited. We propose two methods to measure the visual-word-based transfer relationship that are eventually based on frequency counts of word pairs. In the first method, word transfer probabilities are estimated by maximizing the likelihood of a shared action set with the EM algorithm. In the second method, word transfer probabilities are estimated by using likelihood-ratio tests. The two methods achieve comparable results and perform better when they are combined. For cross-view action classification, we compute action transfer probabilities based on the estimated word transfer probabilities and then implement a K-NN-like classification based on action video transfer probabilities. We verified our method on the public multiview IXMAS dataset and the WVU dataset. Promising results are obtained compared with state-of-the-art methods.","1558-2205","","10.1109/TCSVT.2014.2382984","National Natural Science Foundation of China; Key Projects in the National Science and Technology Pillar Program during the 12th Five-Year Plan Period; Science and Technology Program of Guangzhou, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6990544","Cross-view action recognition;expectation-maximization algorithm;log-likelihood-ratio tests;statistical machine translation;transfer probabilities","Visualization;Probability;Estimation;Feature extraction;Training;Hidden Markov models;Target recognition","feature extraction;image classification;image motion analysis;maximum likelihood estimation;pattern clustering;probability;statistical analysis","cross-view action recognition;statistical translation framework;visual word transfer probabilities;feature extraction;action video frames;bags-of-words;k-means clustering;visual-word-based transfer relationship;frequency counts;word transfer probabilities;EM algorithm;cross-view action classification;K-NN-classification;action video transfer probabilities;public multiview IXMAS dataset;WVU dataset","","10","","56","","18 Dec 2014","","","IEEE","IEEE Journals"
"A system for the automatic measurement of the nuchal translucency thickness from ultrasound video stream of the foetus","A. Anzalone; G. Fusco; F. Isgrò; E. Orlandi; R. Prevete; G. Sciortino; D. Tegolo; C. Valenti","IASF, National Institute of Astrophysics, Palermo, Italy; Dipartimento di Informatica, Bioingegneria, Robotica e Ingegneria dei Sistemi, Università di Genova, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università di Napoli Federico II, Italy; Azienda Ospedaliera Universitaria Policlinico Paolo Giaccone di Palermo, Italy; Dipartimento di Ingegneria Elettrica e delle Tecnologie dell'Informazione, Università di Napoli Federico II, Italy; Dipartimento di Matematica e Informatica, Università di Palermo, Italy; Dipartimento di Matematica e Informatica, Università di Palermo, Italy; Dipartimento di Matematica e Informatica, Università di Palermo, Italy","Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems","10 Oct 2013","2013","","","239","244","Nowadays the measurement of the nuchal translucency thickness is being used as part of routine ultrasound scanning during the end of the first trimester of pregnancy, for the screening of chromosomal defects, as trisomy 21. Currently, the measurement is being performed manually by physicians. The measurement can take a long time for being accomplished, needs to be performed by highly skilled operators, and is prone to errors. Semi-automated methods requires that the user manually selects a region of the image containing the nuchal translucency, procedure that is somewhat time consuming. In this paper we present a complete system prototype that is able to perform the measurement of the nuchal translucency thickness without any manual intervention from the operator, operating on the video stream coming out from the ultrasound machine.","1063-7125","978-1-4799-1053-3","10.1109/CBMS.2013.6627795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6627795","","Thickness measurement;Ultrasonic variables measurement;Ultrasonic imaging;Medical services;Cost function;Extraterrestrial measurements;Streaming media","biomedical ultrasonics;medical image processing;obstetrics;thickness measurement;video signal processing","automatic measurement;nuchal translucency thickness;ultrasound video stream;foetus;ultrasound scanning;pregnancy;chromosomal defect screening;ultrasound machine","","6","","14","","10 Oct 2013","","","IEEE","IEEE Conferences"
"Neonatal seizure detection using convolutional neural networks","A. O'Shea; G. Lightbody; G. Boylan; A. Temko","Irish Centre for Fetal and Neonatal Translational Research, University College Cork; Irish Centre for Fetal and Neonatal Translational Research, University College Cork; Irish Centre for Fetal and Neonatal Translational Research, University College Cork; Irish Centre for Fetal and Neonatal Translational Research, University College Cork","2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP)","7 Dec 2017","2017","","","1","6","This study presents a novel end-to-end architecture that learns hierarchical representations from raw EEG data using fully convolutional deep neural networks for the task of neonatal seizure detection. The deep neural network acts as both feature extractor and classifier, allowing for end-to-end optimization of the seizure detector. The designed system is evaluated on a large dataset of continuous unedited multichannel neonatal EEG totaling 835 hours and comprising of 1389 seizures. The proposed deep architecture, with sample-level filters, achieves an accuracy that is comparable to the state-of-the-art SVM-based neonatal seizure detector, which operates on a set of carefully designed hand-crafted features. The fully convolutional architecture allows for the localization of EEG waveforms and patterns that result in high seizure probabilities for further clinical examination.","","978-1-5090-6341-3","10.1109/MLSP.2017.8168193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8168193","neonatal seizure detection;convolutional neural networks;support vector machine;EEG waveforms","Electroencephalography;Feature extraction;Pediatrics;Support vector machines;Convolution;Time-frequency analysis;Training","convolution;electroencephalography;feature extraction;filtering theory;medical disorders;medical signal processing;neural nets;optimisation;paediatrics;signal classification;signal representation;signal sampling","raw EEG data;fully convolutional deep neural networks;neonatal seizure detection;deep neural network;feature extractor;classifier;end-to-end optimization;continuous unedited multichannel neonatal EEG;deep architecture;fully convolutional architecture;convolutional neural networks;end-to-end architecture;hierarchical representations;EEG waveforms;sample-level filters","","3","","27","","7 Dec 2017","","","IEEE","IEEE Conferences"
"An Effective Coverage Approach for Attention-based Neural Machine Translation","H. -Q. Nguyen; T. -M. Nguyen; H. -H. Vu; V. -V. Nguyen; P. -T. Nguyen; T. -N. -M. Dao; K. -H. Tran; K. -Q. Dinh","University of Engineering and Technology, VNU,Department of Computer Science,Hanoi; University of Engineering and Technology, VNU,Department of Computer Science,Hanoi; University of Engineering and Technology, VNU,Department of Computer Science,Hanoi; University of Engineering and Technology, VNU,Department of Computer Science,Hanoi; University of Engineering and Technology, VNU,Department of Computer Science,Hanoi; University of Languages and International Studies, VNU,Faculty of Japanese Language and Culture,Hanoi; University of Languages and International Studies, VNU,Faculty of Japanese Language and Culture,Hanoi; University of Engineering and Technology, VNU,Department of Computer Science,Hanoi","2019 6th NAFOSTED Conference on Information and Computer Science (NICS)","5 Mar 2020","2019","","","240","245","The following topics are dealt with: learning (artificial intelligence); convolutional neural nets; feature extraction; neural nets; image classification; Internet of Things; computer vision; free-space optical communication; image segmentation; support vector machines.","","978-1-7281-5163-2","10.1109/NICS48868.2019.9023793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023793","","Natural language processing;Neural networks;Machine translation","feature extraction;free-space optical communication;image classification;Internet of Things;learning (artificial intelligence);neural nets","support vector machines;image segmentation;free-space optical communication;computer vision;Internet of Things;image classification;neural nets;feature extraction;convolutional neural nets;learning (artificial intelligence)","","1","","12","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Large-Scale Distributed Language Modeling","A. Emami; K. Papineni; J. Sorensen","IBM T J Watson Research Center, 1101 Kitchawan Rd., Yorktown Heights, NY 10598. emami@us.ibm.com; Yahoo! Research, 45 W. 18th St., NewYork, NY 10011. kpapi@yahoo-inc.com; IBM T J Watson Research Center, 1101 Kitchawan Rd., Yorktown Heights, NY 10598. sorenj@us.ibm.com","2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07","4 Jun 2007","2007","4","","IV-37","IV-40","A novel distributed language model that has no constraints on the n-gram order and no practical constraints on vocabulary size is presented. This model is scalable and allows for an arbitrarily large corpus to be queried for statistical estimates. Our distributed model is capable of producing n-gram counts on demand. By using a novel heuristic estimate for the interpolation weights of a linearly interpolated model, it is possible to dynamically compute the language model probabilities. The distributed architecture follows the client-server paradigm and allows for each client to request an arbitrary weighted mixture of the corpus. This allows easy adaptation of the language model to particular test conditions. Experiments using the distributed LM for re-ranking N-best lists of a speech recognition system resulted in considerable improvements in word error rate (WER), while integration with a machine translation decoder resulted in significant improvements in translation quality as measured by the BLEU score.","2379-190X","1-4244-0727-3","10.1109/ICASSP.2007.367157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4218031","Statistical language modeling;Speech recognition;Statistical machine translation;Client-server systems;Distributed memory systems","Large-scale systems;Decoding;Natural languages;Training data;Automatic speech recognition;Vocabulary;Probability;Speech recognition;Error analysis;Surface-mount technology","client-server systems;decoding;interpolation;language translation;natural language processing;speech coding;speech recognition;statistical analysis","re-ranking N-best lists;speech recognition system;word error rate;n-gram order;interpolation weights;language model probabilities;client-server paradigm;machine translation decoder;large-scale distributed language modeling","","6","6","12","","4 Jun 2007","","","IEEE","IEEE Conferences"
"Answering English Queries in Automatically Transcribed Arabic Speech","A. F. A. Nwesri; S. M. M. Tahaghoghi; F. Scholer","RMIT University, Australia; RMIT University, Australia; RMIT University, Australia","6th IEEE/ACIS International Conference on Computer and Information Science (ICIS 2007)","23 Jul 2007","2007","","","11","16","There are several well-known approaches to parsing Arabic text in preparation for indexing and retrieval. Techniques such as stemming and stopping have been shown to improve search results on written newswire dispatches, but few comparisons are available on other data sources. In this paper, we apply several alternative stemming and stopping approaches to Arabic text automatically extracted from the audio soundtrack of news video footage, and compare these with approaches that rely on machine translation of the underlying text. Using the TRECVID video collection and queries, we show that normalisation, stopword- removal, and light stemming increase retrieval precision, but that heavy stemming and trigrams have a negative effect. We also show that the choice of machine translation engine plays a major role in retrieval effectiveness.","","0-7695-2841-4","10.1109/ICIS.2007.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4276350","Arabic information retrieval;Cross-language;information retrieval;Machine translation.","Information retrieval;Data mining;Automatic speech recognition;Indexing;Acoustic noise;Shape;Computer science;Information technology;Australia;Engines","grammars;indexing;language translation;natural languages;speech recognition;text analysis;video retrieval","English query answering;automatically transcribed Arabic speech;parsing;indexing;audio soundtrack;news video footage;machine translation engine;Arabic information retrieval;Arabic text","","","","18","","23 Jul 2007","","","IEEE","IEEE Conferences"
"Towards a Modular Brain-Machine Interface for Intelligent Vehicle Systems Control – A CARLA Demonstration","C. Dunlap; L. Bird; I. Burkhart; K. Eipel; S. Colachis; N. Annetta; P. Ganzer; G. Sharma; D. Friedenberg; R. Franklin; M. Gerhardt; A. Ravindran; A. Hassani; D. Filev; F. Solzbacher; M. Bockbrader","The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA; The Ohio State University,Columbus,OH,USA","2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)","28 Nov 2019","2019","","","277","284","Objective: Individuals with paralysis often have mobility and dexterity impairments that limit their ability to operate motor vehicle controls. Integrating brain-machine interface (BMI) neurotechnology with vehicle control systems (VCS) provides a novel solution to this problem. In this proof-of-concept study, we show that an intracortical BMI developed to restore voluntary grasp can be repurposed to decode motor intention for vehicle velocity and steering control. Methods: The BMI-VCS consists of four components: 1) implanted motor cortex microelectrode array and NeuroPort data acquisition system, 2) machine learning workstation, 3) Python interface to generate control signals, and 4) vehicle control system. Results: Direct cortical steering and velocity control were achieved through accurate decoding of movement intention (supination, pronation, hand open, hand close) from the participant's motor cortex, translating intention into vehicle commands (turn right, turn left, accelerate, decelerate, respectively), and dynamically switching between commands to turn corners, start and stop, shift from forward to reverse, and parallel park. Conclusion: By translating BMI decoder outputs into high-level vehicle commands, a participant with tetraparesis from C5 ASIA A spinal cord injury successfully navigated CARLA driving simulator courses in real time. These decoder outputs could also be used offline for shared control of a scale model car. Significance: High-level, shared vehicle control with BMI-VCS offers an innovative way to return independent driving abilities to those with disability. BMI systems that can control multiple end-effectors may be particularly useful to those with paralysis.","2577-1655","978-1-7281-4569-3","10.1109/SMC.2019.8914317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8914317","Assistive technology;brain-machine interface;steering control;velocity control;intelligent vehicle systems","Decoding;Task analysis;Training;Automobiles;Support vector machines;Cruise control","bioelectric potentials;biomechanics;brain-computer interfaces;control engineering computing;decoding;handicapped aids;injuries;learning (artificial intelligence);medical signal processing;microelectrodes;neurocontrollers;neurophysiology;patient rehabilitation;prosthetics;velocity control","modular brain-machine interface;intelligent vehicle systems control;CARLA demonstration;paralysis;mobility;dexterity impairments;motor vehicle controls;integrating brain-machine interface neurotechnology;vehicle control systems;intracortical BMI;voluntary grasp;decode motor intention;vehicle velocity;steering control;BMI-VCS;control signals;velocity control;accurate decoding;movement intention;turn corners;translating BMI decoder;high-level vehicle commands;CARLA driving simulator courses;decoder outputs;shared control;shared vehicle control;BMI systems","","","","20","","28 Nov 2019","","","IEEE","IEEE Conferences"
"DeepTaste: Augmented Reality Gustatory Manipulation with GAN-Based Real-Time Food-to-Food Translation","K. Nakano; D. Horita; N. Sakata; K. Kiyokawa; K. Yanai; T. Narumi",Nara Institute of Science and Technology; The University of Electro-Communications; Nara Institute of Science and Technology; Nara Institute of Science and Technology; The University of Electro-Communications; The University of Tokyo,"2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","30 Dec 2019","2019","","","212","223","We have been studying augmented reality (AR)-based gustatory manipulation interfaces and previously proposed a gustatory manipulation interface using generative adversarial network (GAN)-based real time image-to-image translation. Unlike three-dimensional (3D) food model-based systems that only change the color or texture pattern of a particular type of food in an inflexible manner, our GAN-based system changes the appearance of food into multiple types of food in real time flexibly, dynamically, and interactively. In the present paper, we first describe in detail a user study on a vision-induced gustatory manipulation system using a 3D food model and report its successful experimental results. We then summarize identified problems of the 3D model-based system and describe implementation details of the GAN-based system. We finally report in detail the main user study in which we investigated the impact of the GAN-based system on gustatory sensations and food recognition when somen noodles were turned into ramen noodles or fried noodles, and steamed rice into curry and rice or fried rice. The experimental results revealed that our system successfully manipulates gustatory sensations to some extent and that the effectiveness seems to depend on the original and target types of food as well as the experience of each individual with the food.","1554-7868","978-1-7281-0987-9","10.1109/ISMAR.2019.000-1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943779","Human centered computing Human computer interaction (HCI) Interaction paradigms Mixed / augmented reality Computing methodologies Computer graphics Graphics systems and interfaces Perception Computing methodologies Machine learning Machine learning approaches Neural networks","Three-dimensional displays;Solid modeling;Real-time systems;Visualization;Resists;Image color analysis;Modulation","augmented reality;chemioception;computer vision;food products;image texture;mobile computing","3D model-based system;vision-induced gustatory manipulation system;three-dimensional food model-based systems;image-to-image translation;generative adversarial network-based;gustatory manipulation interface;augmented reality-based gustatory manipulation interfaces;GAN-based real-time food-to-food;augmented reality gustatory manipulation;food recognition;gustatory sensations;GAN-based system","","1","","47","","30 Dec 2019","","","IEEE","IEEE Conferences"
"Fully Automatic Three-Dimensional Ultrasound Imaging Based on Conventional B-Scan","Q. Huang; B. Wu; J. Lan; X. Li","School of Mechanical Engineering and the Center for Optical Imagery Analysis and Learning, Northwestern Polytechnical University, Xi'an, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, China","IEEE Transactions on Biomedical Circuits and Systems","21 Mar 2018","2018","12","2","426","436","Robotic ultrasound systems have turned into clinical use over the past few decades, increasing precision and quality of medical operations. In this paper, we propose a fully automatic scanning system for three-dimensional (3-D) ultrasound imaging. A depth camera was first used to obtain the depth data and color data of the tissue surface. Based on the depth image, the 3-D contour of the tissue was rendered and the scan path of ultrasound probe was automatically planned. Following the scan path, a 3-D translating device drove the probe to move on the tissue surface. Simultaneously, the B-scans and their positional information were recorded for subsequent volume reconstruction. In order to stop the scanning process when the pressure on the skin exceeded a preset threshold, two force sensors were attached to the front side of the probe for force measurement. In vitro and in vivo experiments were conducted for assessing the performance of the proposed system. Quantitative results show that the error of volume measurement was less than 1%, indicating that the system is capable of automatic ultrasound scanning and 3-D imaging. It is expected that the proposed system can be well used in clinical practices.","1940-9990","","10.1109/TBCAS.2017.2782815","National Natural Science Foundation of China; Guangzhou Key Lab of Body Data Science; Natural Science Foundation of Guangdong Province, China; Project of Science and Technology Department of Guangdong Province; Science and Technology Program of Guangzhou; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8279642","Automatic ultrasound scanning;depth imaging;robotic 3D ultrasound;scan path planning","Ultrasonic imaging;Three-dimensional displays;Probes;Imaging;Robots;Breast;Image reconstruction","biological tissues;biomedical ultrasonics;cameras;force measurement;force sensors;image colour analysis;image reconstruction;medical image processing;medical robotics;volume measurement","force measurement;automatic ultrasound scanning;3-D imaging;robotic ultrasound systems;medical operations;fully automatic scanning system;depth camera;depth data;color data;tissue surface;depth image;3-D contour;scan path;ultrasound probe;3-D translating device;B-scans;subsequent volume reconstruction;scanning process;autofully automatic three-dimensional ultrasound imaging","Adult;Breast;Female;Humans;Imaging, Three-Dimensional;Lumbosacral Region;Male;Phantoms, Imaging;Ultrasonography;Ultrasonography, Prenatal;Young Adult","11","","51","","2 Feb 2018","","","IEEE","IEEE Journals"
"Automatic Three-Dimensional Ultrasound Scanning System Based on RGB-D Camera","J. Lan; Q. Huang","South China University of Technology, Department of Electronic and Information Engineering, Guangzhou, 510641, China; South China University of Technology, Department of Electronic and Information Engineering, Guangzhou, 510641, China","2018 2nd International Conference on Robotics and Automation Sciences (ICRAS)","23 Aug 2018","2018","","","1","5","Ultrasound (US) imaging, as a non-invasive, low cost, real-time imaging modality, is widely used in the routine diagnosis and medical evaluation. To increase the performance of current US system, more and more attention has been attracted in the field of integrating the auxiliary devices into the US system. In this paper, we proposed a scanning system based on RGB-D camera for three-dimensional imaging. A depth camera was adopted to capture the point cloud of the scanned tissue. The operator could determine the scan path in the way of drawing a straight line. According to the point cloud, a 3D translating device would drive the probe to scan the tissue. Meanwhile, the B-scans and their corresponding positional information would be recorded for volume reconstruction. In vitro and in vivo experiments were performed to validate the performance of the proposed system. Quantitative results show that the error of volume measurement was less than 1%. It is expected that the proposed system will be clinically useful.","","978-1-5386-7370-6","10.1109/ICRAS.2018.8442356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8442356","automatic ultrasound scanning;RGB-D;3D ultrasound;Scan path","Three-dimensional displays;Probes;Calibration;Color;Breast;Imaging;Steel","biological tissues;biomedical ultrasonics;cameras;cloud computing;image reconstruction;medical image processing;volume measurement","tissue scanning;point cloud capture;volume reconstruction;B-scans;3D translating device;depth camera;auxiliary devices;real-time imaging modality;ultrasound imaging;RGB-D camera;automatic three-dimensional ultrasound scanning system","","1","","23","","23 Aug 2018","","","IEEE","IEEE Conferences"
"Deep Neural Network for Automatic Speech Recognition from Indonesian Audio using Several Lexicon Types","T. F. Abidin; A. Misbullah; R. Ferdhiana; M. Z. Aksana; L. Farsiah","Universitas Syiah Kuala,Department of Informatics,Banda Aceh,Indonesia; Universitas Syiah Kuala,Department of Informatics,Banda Aceh,Indonesia; Universitas Syiah Kuala,Department of Statistics,Banda Aceh,Indonesia; Universitas Syiah Kuala,Department of Informatics,Banda Aceh,Indonesia; Universitas Syiah Kuala,Department of Informatics,Banda Aceh,Indonesia","2020 International Conference on Electrical Engineering and Informatics (ICELTICs)","12 Jan 2021","2020","","","1","5","Recently, automatic speech recognition has benefited from advances in deep neural networks (DNNs) to train and deploy speech recognition models. Speech recognition models enable computers to recognize and translate spoken language into text. In this paper, we present an approach to creating an Indonesian voice-to-text dataset using audio collected from YouTube channels and to evaluating the speech recognition models using several lexicon types. The lexicons are created from unique words of the speech corpus. We compared the performance of Time Delay Neural Network Factorization (TDNNF) and Gaussian Mixture Model - Hidden Markov Model (GMM-HMM) models for mono phone, DELTA+DELTA-DELTA, and Speaker Adaptive Training (SAT) based on the %WER when trained using unvalidated and validated datasets. The results showed that there was no significant difference in the %WER among the lexicon types. Moreover, the results revealed that the models trained using a validated dataset perform better than an unvalidated dataset. Additionally, lexicon enmap_kv_vocab_full returned the best result with 29.41% WER when trained using the TDNNF model on an unvalidated dataset. However, lexicon enmap_vocab_l_char provided the best result, with 11.35% WER, when trained using the TDNNF model on a validated dataset.","","978-1-7281-8199-8","10.1109/ICELTICs50595.2020.9315538","Universitas Syiah Kuala; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9315538","Automatic speech recognition;deep neural networks;lexicons;acoustic and language models","Speech recognition;Hidden Markov models;Acoustics;Training;Neural networks;Informatics;Dictionaries","Gaussian processes;hidden Markov models;neural nets;speech recognition","deep Neural Network;automatic speech recognition;lexicon types;deep neural networks;speech recognition models;Indonesian voice-to-text dataset;speech corpus;Time Delay Neural Network Factorization;Gaussian Mixture Model;Hidden Markov Model models;validated dataset;unvalidated dataset;TDNNF model","","","","10","","12 Jan 2021","","","IEEE","IEEE Conferences"
"Automatic registration of serial mammary gland sections","I. Arganda-Carreras; R. Fernandez-Gonzalez; C. Ortiz-de-Solorzano","Biocomputing Unit, Univ. Autonoma de Madrid, Spain; NA; NA","The 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","14 Mar 2005","2004","1","","1691","1694","We present two methods for automatic registration of microscope images of consecutive tissue sections. They represent two possibilities for the first step in the 3-D reconstruction of histological structures from serially sectioned tissue blocks. The goal is to accurately align the sections in order to place every relevant shape contained in each image in front of its corresponding shape in the following section before detecting the structures of interest and rendering them in 3D. This is accomplished by finding the best rigid body transformation (translation and rotation) of the image being registered by maximizing a matching function based on the image content correlation. The first method makes use of the entire image information, whereas the second one uses only the information located at specific sites, as determined by the segmentation of the most relevant tissue structures. To reduce computing time, we use a multiresolution pyramidal approach that reaches the best registration transformation in increasing resolution steps. In each step, a subsampled version of the images is used. Both methods rely on a binary image which is a thresholded version of the Sobel gradients of the image (first method) or a set of boundaries manually or automatically obtained that define important histological structures of the sections. Then distance-transform of the binary image is computed. A proximity function is then calculated between the distance image of the image being registered and that of the reference image. The transformation providing a maximum of the proximity function is then used as the starting point of the following step. This is iterated until the registration error lies below a minimum value.","","0-7803-8439-3","10.1109/IEMBS.2004.1403509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1403509","Automatic registration;Image processing;Biotechnology","Mammary glands;Image reconstruction;Microscopy;Three dimensional displays;Shape;Biotechnology;Rendering (computer graphics);Image segmentation;Image processing;Visualization","biological tissues;optical microscopes;biomedical optical imaging;medical image processing;image registration;image reconstruction;image matching;image segmentation;image resolution","automatic image registration;serial mammary gland sections;microscope images;consecutive tissue sections;3-D histological structure reconstruction;serially sectioned tissue blocks;rigid body transformation;image matching;image content correlation;image information;image segmentation;multiresolution pyramidal approach;Sobel gradients","","5","3","4","","14 Mar 2005","","","IEEE","IEEE Conferences"
"Hierarchical automatic speech recognition powered by data infrastructure","A. Jagatheesan; J. Ahnn; T. Phan; A. Singh; J. Lee","Samsung Research America - Silicon Valley 75 W Plumeria Drive, San Jose, CA 95134, USA; Samsung Research America - Silicon Valley 75 W Plumeria Drive, San Jose, CA 95134, USA; Samsung Research America - Silicon Valley 75 W Plumeria Drive, San Jose, CA 95134, USA; Samsung Research America - Silicon Valley 75 W Plumeria Drive, San Jose, CA 95134, USA; Samsung Research America - Silicon Valley 75 W Plumeria Drive, San Jose, CA 95134, USA","2014 IEEE 11th Consumer Communications and Networking Conference (CCNC)","3 Nov 2014","2014","","","1140","1141","Automatic Speech Recognition (ASR) has evolved remarkably over the years and is expected to become a primary form of input to mobile devices including smartphones and wearables. Most large-scale mobile platforms perform speech recognition in the cloud today. There are both advantages and disadvantages to this Cloud-based ASR (Cloud-ASR) approach. Cloud-ASR approach allows for a context oriented human-computer-interaction using speech rather than a mere speech-to-text translation. A Cloud-ASR also has disadvantages such as interruption of the speech service when there is no access to the Cloud-ASR, and also the energy consumption for radio communications, which can drain a mobile battery sooner. We propose the usage of Hierarchical Speech Recognizer (HSR) as an alternative approach to overcome the shortcomings of the Cloud-ASR approach. In the HSR approach, mobile devices perform “selective speech recognition” by themselves as much as possible without contacting an external cloud-based ASR service. In this demonstration, we show our proof-of-concept HSR along with its feasibility and advantages.","2331-9860","978-1-4799-2355-7","10.1109/CCNC.2014.6940492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6940492","Consumer Electronics;Smart Phone;Automatic Speech Recognition;S-Voice;Data infrastructure","Speech recognition;Acoustics;Smart phones;Speech;Computational modeling;Batteries","mobile computing;mobile handsets;speech recognition","hierarchical automatic speech recognition;data infrastructure;context oriented human computer interaction;speech service;hierarchical speech recognizer;mobile devices;selective speech recognition","","","","2","","3 Nov 2014","","","IEEE","IEEE Conferences"
"A Concise and Rapid Error Correction Algorithm in Automatic Optical Inspection of Solder Paste","J. Wu; Z. Chen","Sch. of Software Eng., Huazhong Univ. of Sci. & Technol., Wuhan, China; Sch. of Software Eng., Huazhong Univ. of Sci. & Technol., Wuhan, China","2011 International Conference on Internet Computing and Information Services","31 Oct 2011","2011","","","325","329","Image-array-based automatic optical inspection system captures multiple images of the inspected printed circuit board through synchronous exposure. Therefore a correct mosaic of images is very import to subsequent solder paste inspection. However, Design error of the mechanism always leads to geometric errors in the image such as translation, rotation and scaling distortion. These errors make severe displacements in the image. To solve such problems, a concise and rapid error correction algorithm is proposed base on multiple point interpolation. Experiments show that the algorithm can assure rapid and correct mosaic of images of the whole printed circuit board.","","978-1-4577-1561-7","10.1109/ICICIS.2011.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063262","Automatic Optical Inspection;Image Array;Geometric Error;Multiple Point Interpolation","Cameras;Interpolation;Error correction;Algorithm design and analysis;Inspection;Standards Board","automatic optical inspection;error correction;image segmentation;interpolation;printed circuit testing;printed circuits;solders","image array based automatic optical inspection system;inspected printed circuit board;synchronous exposure;solder paste inspection;design error;geometric errors;image mosaic;error correction algorithm;multiple point interpolation","","1","3","5","","31 Oct 2011","","","IEEE","IEEE Conferences"
"Automatic visual to tactile translation. I. Human factors, access methods and image manipulation","T. P. Way; K. E. Barner","Dept. of Electr. Eng., Delaware Univ., Newark, DE, USA; NA","IEEE Transactions on Rehabilitation Engineering","6 Aug 2002","1997","5","1","81","94","This is the first part of a two-part paper that motivates and evaluates a method for the automatic conversion of images from visual to tactile form. In this part, a broad-ranging background is provided in the areas of human factors, including the human sensory system, tactual perception and blindness, access technology for tactile graphics production, and image processing techniques and their appropriateness to tactile image creation. In Part II, this background is applied in the development of the TACTile Image Creation System (TACTICS), a prototype for an automatic visual-to-tactile translator. The results of an experimental evaluation are then presented and discussed, and possible future work in this area is outlined.","1558-0024","","10.1109/86.559353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=559353","","Human factors;Pervasive computing;Image converters;Computer displays;Blindness;Graphics;Image processing;Internet;Speech synthesis;Graphical user interfaces","mechanoception;human factors;handicapped aids;vision defects;image representation;computer graphics;computer vision;visual perception","automatic visual to tactile translation;human factors;access methods;image manipulation;tactile form;broad-ranging background;human sensory system;tactual perception;blindness;access technology;tactile graphics production;image processing techniques;tactile image creation;TACTile Image Creation System;TACTICS","Algorithms;Blindness;Humans;Image Processing, Computer-Assisted;Man-Machine Systems;Sensory Aids;Software;Touch;User-Computer Interface;Visual Perception","26","6","","","6 Aug 2002","","","IEEE","IEEE Journals"
"Coping with out-of-vocabulary words: Open versus huge vocabulary asr","M. Gerosa; M. Federico","FBK-irst - Fondazione Bruno Kessler, Via Sommarive 18, 38100 Povo (TN), Italy; FBK-irst - Fondazione Bruno Kessler, Via Sommarive 18, 38100 Povo (TN), Italy","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","4313","4316","This paper investigates methods for coping with out-of-vocabulary words in a large vocabulary speech recognition task, namely the automatic transcription of Italian broadcast news. Two alternative ways for augmenting a 64 K(thousand)-word recognition vocabulary and language model are compared: introducing extra words with their phonetic transcription up to 1.2 M (million) words, or extending the language model with so-called graphones, i.e. subword units made of phone-character sequences. Graphones and phonetic transcriptions of words are automatically generated by adapting an off-the-shelf statistical machine translation toolkit. We found that the word-based and graphone based extentions allow both for better recognition performance, with the former performing significantly better than the latter. In addition, the word-based extension approach shows interesting potential even under conditions of little supervision. In fact, by training the grapheme to phoneme translation system with only 2 K manually verified transcriptions, the final word error rate increases by just 3% relative, with respect to starting from a lexicon of 64 K words.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960583","Automatic Speech Recognition;Open-vocabulary speech recognition;OOV words","Vocabulary;Automatic speech recognition;Speech recognition;Broadcasting;Error analysis;Art;Robustness;Training data;Natural languages;Documentation","language translation;speech processing;speech recognition;statistical analysis","out-of-vocabulary word;vocabulary speech recognition;automatic transcription;Italian broadcast news;language model;phoneme translation system;word error rate;word recognition;phonetic transcription;statistical machine translation toolkit","","4","","13","","26 May 2009","","","IEEE","IEEE Conferences"
"Partial least squares modelling for imaging-genetics in Alzheimer's disease: Plausibility and generalization","M. Lorenzi; B. Gutman; D. P. Hibar; A. Altmann; N. Jahanshad; P. M. Thompson; S. Ourselin","Translational Imaging Group, CMIC, University College London, London, UK; Imaging Genetics Center, University of Southern California, Marina del Rey, CA, USA; Imaging Genetics Center, University of Southern California, Marina del Rey, CA, USA; Translational Imaging Group, CMIC, University College London, London, UK; Imaging Genetics Center, University of Southern California, Marina del Rey, CA, USA; Imaging Genetics Center, University of Southern California, Marina del Rey, CA, USA; Translational Imaging Group, CMIC, University College London, London, UK","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","16 Jun 2016","2016","","","838","841","In this work we evaluate the ability of PLS in generalizing to unseen clinical cohorts when applied to the analysis of the joint variation between genotype and phenotype in Alzheimer's disease (AD). The model is trained on single-nucleotide polymorphisms (SNPs) and brain volumes obtained from the ADNI database for a large cohort of healthy individuals and AD patients, and validated on the ADNI MCI and ENIGMA cohorts. The experimental results confirm the ability of PLS in providing a meaningful description of the joint dynamics between brain atrophy and genotype data in AD, while providing important generalization results when tested on clinically heterogeneous cohorts.","1945-8452","978-1-4799-2349-6","10.1109/ISBI.2016.7493396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493396","GWA;imaging-genetics;genotype;phenotype;Alzheimer's disease;machine learning","Genetics;Biological cells;Brain modeling;Imaging;Correlation;Alzheimer's disease","brain;diseases;genetics;genomics;least squares approximations;medical image processing;neurophysiology","partial least squares modelling;imaging genetics;Alzheimer's disease;clinical cohorts;joint variation;genotype;phenotype;single-nucleotide polymorphisms;brain volumes;ADNI database;ADNI MCI cohorts;ENIGMA cohorts;genotype data;brain atrophy;clinically heterogeneous cohorts","","6","","12","","16 Jun 2016","","","IEEE","IEEE Conferences"
"Acoustic unit discovery and pronunciation generation from a grapheme-based lexicon","W. Hartmann; A. Roy; L. Lamel; J. Gauvain","Spoken Language Processing Group, LIMSI-CNRS 91403 Orsay, France; Spoken Language Processing Group, LIMSI-CNRS 91403 Orsay, France; Spoken Language Processing Group, LIMSI-CNRS 91403 Orsay, France; Spoken Language Processing Group, LIMSI-CNRS 91403 Orsay, France","2013 IEEE Workshop on Automatic Speech Recognition and Understanding","9 Jan 2014","2013","","","380","385","We present a framework for discovering acoustic units and generating an associated pronunciation lexicon from an initial grapheme-based recognition system. Our approach consists of two distinct contributions. First, context-dependent grapheme models are clustered using a spectral clustering approach to create a set of phone-like acoustic units. Next, we transform the pronunciation lexicon using a statistical machine translation-based approach. Pronunciation hypotheses generated from a decoding of the training set are used to create a phrase-based translation table. We propose a novel method for scoring the phrase-based rules that significantly improves the output of the transformation process. Results on an English language dataset demonstrate the combined methods provide a 13% relative reduction in word error rate compared to a baseline grapheme-based system. Our approach could potentially be applied to low-resource languages without existing lexicons, such as in the Babel project.","","978-1-4799-2756-2","10.1109/ASRU.2013.6707760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707760","acoustic unit discovery;automatic speech recognition;grapheme-based speech recognition;pronunciation learning","Hidden Markov models;Acoustics;Context modeling;Dictionaries;Training;Computational modeling;Training data","acoustic signal processing;language translation;natural language processing;pattern clustering;spectral analysis;speech recognition;statistical analysis","phone-like acoustic unit discovery;pronunciation lexicon generation;grapheme-based lexicon;grapheme-based speech recognition system;context-dependent grapheme models;spectral clustering approach;statistical machine translation-based approach;pronunciation hypothesis generation;training set decoding;phrase-based translation table;phrase-based rule scoring;transformation process output improvement;English language dataset;relative reduction;word error rate;low-resource languages;Babel project","","5","","18","","9 Jan 2014","","","IEEE","IEEE Conferences"
"WERD: Using social text spelling variants for evaluating dialectal speech recognition","A. Ali; P. Nakov; P. Bell; S. Renals","Qatar Computing Research Institute, HBKU, Doha, Qatar; Qatar Computing Research Institute, HBKU, Doha, Qatar; Centre for Speech Technology Research, University of Edinburgh, UK; Centre for Speech Technology Research, University of Edinburgh, UK","2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","25 Jan 2018","2017","","","141","148","We study the problem of evaluating automatic speech recognition (ASR) systems that target dialectal speech input. A major challenge in this case is that the orthography of dialects is typically not standardized. From an ASR evaluation perspective, this means that there is no clear gold standard for the expected output, and several possible outputs could be considered correct according to different human annotators, which makes standard word error rate (WER) inadequate as an evaluation metric. Such a situation is typical for machine translation (MT), and thus we borrow ideas from an MT evaluation metric, namely TERp, an extension of translation error rate which is closely-related to WER. In particular, in the process of comparing a hypothesis to a reference, we make use of spelling variants for words and phrases, which we mine from Twitter in an unsupervised fashion. Our experiments with evaluating ASR output for Egyptian Arabic, and further manual analysis, show that the resulting WERd (i.e., WER for dialects) metric, a variant of TERp, is more adequate than WER for evaluating dialectal ASR.","","978-1-5090-4788-8","10.1109/ASRU.2017.8268928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268928","Automatic speech recognition;dialectal ASR;ASR evaluation;word error rate;multi-reference WER","Speech;Measurement;Standards;Training;Error analysis;Twitter","language translation;natural language processing;social networking (online);speech recognition","translation error rate;WER;dialectal speech recognition;automatic speech recognition systems;dialectal speech input;machine translation","","1","","35","","25 Jan 2018","","","IEEE","IEEE Conferences"
"A Sequence-to-Sequence Model Approach for ImageCLEF 2018 Medical Domain Visual Question Answering","R. Ambati; C. Reddy Dudyala","International Institute of Information Technology, Bangalore,Multimodal Perception Lab,Bangalore,India; International Institute of Information Technology, Bangalore,Multimodal Perception Lab,Bangalore,India","2018 15th IEEE India Council International Conference (INDICON)","16 Mar 2020","2018","","","1","6","Numerous attempts have been made in the recent past for the task of free-form and open-ended Visual Question Answering (VQA). Solving VQA problem typically requires techniques from both computer vision for a deeper understanding of the images and Natural language processing for understanding the semantics of the question and generating appropriate answers. It has caught the attention of a lot of researchers because of its enormous applications in the real-world scenarios. But none of the existing approaches are designed for the medical image-question pairs which require a sequence of words as an answer. We propose a novel approach by combining the tasks of Image captioning and Machine translation and provided a comprehensive model that takes a medical image-question pair as an input and generates a sequence of words as an answer. We evaluate our model on the dataset provided by ImageCLEF as a part of the ImageCLEF 2018 VQA-med challenge. We outperformed all the contestants of the challenge by achieving the best BLEU and WBSS scores. Furthermore, we provide additional insights that can be adopted to develop our baseline model and the challenges that lie ahead of us while building Machine learning models for medical datasets.","2325-9418","978-1-5386-8235-7","10.1109/INDICON45594.2018.8987108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8987108","Visual question answering;ImageCLEF VQA-med;Image captioning;Machine translation;Computer vision;Natural Language Processing","Task analysis;Biomedical imaging;Computational modeling;Knowledge discovery;Visualization;Compounds","computer vision;data visualisation;image annotation;language translation;learning (artificial intelligence);medical image processing;medical information systems;natural language processing;neural nets;question answering (information retrieval);text analysis","sequence-to-sequence model approach;VQA problem;natural language processing;ImageCLEF 2018 VQA-med challenge;machine learning models;medical datasets;ImageCLEF 2018 medical domain visual question answering;computer vision;medical image-question pairs;image captioning;machine translation","","","","23","","16 Mar 2020","","","IEEE","IEEE Conferences"
"MT-based artificial hypothesis generation for unsupervised discriminative language modeling","E. Dikici; M. Saraçlar","Bogazici University, Department of Electrical and Electronics Engineering, 34342, Bebek, Istanbul, Turkey; Bogazici University, Department of Electrical and Electronics Engineering, 34342, Bebek, Istanbul, Turkey","2015 23rd European Signal Processing Conference (EUSIPCO)","28 Dec 2015","2015","","","1401","1405","Discriminative language modeling (DLM) is used as a postprocessing step to correct automatic speech recognition (ASR) errors. Traditional DLM training requires a large number of ASR N-best lists together with their reference transcriptions. It is possible to incorporate additional text data into training via artificial hypothesis generation through confusion modeling. A weighted finite-state transducer (WFST) or a machine translation (MT) system can be used to generate the artificial hypotheses. When the reference transcriptions are not available, training can be done in an unsupervised way via a target output selection scheme. In this paper we adapt the MT-based artificial hypothesis generation approach to un-supervised discriminative language modeling, and compare it with the WFST-based setting. We achieve improvements in word error rate of up to 0.7% over the generative baseline, which is significant at p <; 0.001.","2076-1465","978-0-9928-6263-3","10.1109/EUSIPCO.2015.7362614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362614","Discriminative language model;confusion model;machine translation;unsupervised training","Training;Adaptation models;Data models;Europe;Signal processing;Speech;Manuals","finite state machines;language translation;speech recognition","unsupervised discriminative language modeling;automatic speech recognition errors;ASR errors;DLM training;confusion modeling;weighted finite-state transducer;WFST;machine translation system;MT system;target output selection scheme;MT-based artificial hypothesis generation approach","","","","22","","28 Dec 2015","","","IEEE","IEEE Conferences"
"A machine learning based approach for the detection and recognition of Bangla sign language","M. Hasan; T. H. Sajib; M. Dey","Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, 4349, Bangladesh; Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, 4349, Bangladesh; Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, 4349, Bangladesh","2016 International Conference on Medical Engineering, Health Informatics and Technology (MediTec)","30 Jan 2017","2016","","","1","5","Speech impaired people are detached from the mainstream society due to the lacking of proper communication aid. Sign language is the primary means of communication for them which normal people do not understand. In order to facilitate the conversation conversion of sign language to audio is very necessary. This paper aims at conversion of sign language to speech so that disabled people have their own voice to communicate with the general people. In this paper, Hand Gesture recognition is performed using HOG (Histogram of Oriented Gradients) for extraction of features from the gesture image and SVM (Support Vector Machine) as classifier. Finally, predict the gesture image with output text. This output text is converted into audible sound using TTS (Text to Speech) converter.","","978-1-5090-5421-3","10.1109/MEDITEC.2016.7835387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835387","SVM;Classification;BdSL;TTS Engine;Feature;HOG;Contouring;Prediction;Recognition rate","Gesture recognition;Support vector machines;Assistive technology;Training;Testing;Feature extraction;Databases","feature extraction;language translation;medical image processing;pattern classification;sign language recognition;speech synthesis;support vector machines","machine learning;Bangla sign language detection;Bangla sign language recognition;speech impaired people;sign language conversation conversion;hand gesture recognition;Histogram of Oriented Gradients;feature extraction;gesture image;support vector machine;SVM classifier;audible sound;text-to-speech converter","","7","","12","","30 Jan 2017","","","IEEE","IEEE Conferences"
"Lungprints: An alternative view in multiple-stream time-series analysis of bioimpedance signals","A. Zifan; P. Liatsis","Information Engineering and Medical Imaging Group, School of Engineering and Mathematical Sciences, City University, London, United Kingdom; Information Engineering and Medical Imaging Group, School of Engineering and Mathematical Sciences, City University, London, United Kingdom","2012 19th International Conference on Systems, Signals and Image Processing (IWSSIP)","31 May 2012","2012","","","236","239","In this paper we introduce Lungprints, which are 2D barcode-pattern like images representing ventilation characteristics of a patient. Recent advances in impedance imaging, namely electrical impedance tomography (EIT), allow for the development of an inexpensive, non-ionizing and non-invasive way of monitoring lung ventilation. Various methods have been proposed for the characterization of temporal lung tissue behaviour during ventilation using EIT. Impedance images are built in real time (13 frames/sec) by processing the measured voltage-streams on the thoracic surface. However, impedance recovery is a highly non-linear ill-posed inverse problem, thus the generated images are highly fuzzy and blurry. The latter characteristics impede robust and precise feature extraction and quantification, which could be used as suitable features for lung signal analysis and diagnosis. In this light, we propose an automatic pipeline for lung bioimpedance signal representation and analysis using rapid lungprint classification. We train a single-class vector machine (SVM) classifier, which estimates the parameters of the class from a dictionary of features, composed of moments extracted from a multi-scale representation of healthy patient lungprints onto a translation invariant set of atoms, namely stationary wavelets. Results show the appropriateness of the proposed method in automatic lung signal representation and classification and its potential advantages not only as a bed-side lung monitoring tool, but in other areas such as multiple stream time-series analysis where data streams exhibit cyclic behavior.","2157-8702","978-3-200-02328-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6208116","Lungprints;impedance tomography;translation invariant wavelets;support vector machines","Tomography;Voltage measurement;Feature extraction;Impedance;Lungs;Support vector machines;Streaming media","biological tissues;cellular biophysics;electric impedance imaging;feature extraction;image classification;image representation;inverse problems;lung;medical image processing;patient monitoring;support vector machines;time series","multiple-stream time-series analysis;bioimpedance signals;2D barcode-pattern like image representing ventilation characteristics;impedance imaging;electrical impedance tomography;thoracic surface;highly nonlinear ill-posed inverse problem;feature extraction;lung signal analysis;lung signal diagnosis;automatic pipeline;lung bioimpedance signal representation;rapid lungprint classification;single-class vector machine classifier;SVM;multiscale representation;healthy patient lungprints;translation invariant set;stationary wavelets;automatic lung signal representation;automatic lung signal classification;bed-side lung monitoring tool;noninvasive monitoring lung ventilation","","","","6","","31 May 2012","","","IEEE","IEEE Conferences"
"Toward Non-Invasive and Automatic Intravenous Infiltration Detection: Evaluation of Bioimpedance and Skin Strain in a Pig Model","A. O. Bicen; L. L. West; L. Cesar; O. T. Inan","Inan Research Laboratory, School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Pediatric Technology Center, Georgia Institute of Technology, Atlanta, GA, USA; Independent Contributor, Atlanta, GA, USA; Inan Research Laboratory, School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Journal of Translational Engineering in Health and Medicine","18 Apr 2018","2018","6","","1","7","Intravenous (IV) therapy is prevalent in hospital settings, where fluids are typically delivered with an IV into a peripheral vein of the patient. IV infiltration is the inadvertent delivery of fluids into the extravascular space rather than into the vein (and requires urgent treatment to avoid scarring and severe tissue damage), for which medical staff currently needs to check patients periodically. In this paper, the performance of two non-invasive sensing modalities, electrical bioimpedance (EBI), and skin strain sensing, for the automatic detection of IV infiltration was investigated in an animal model. Infiltrations were physically simulated on the hind limb of anesthetized pigs, where the sensors for EBI and skin strain sensing were co-located. The obtained data were used to examine the ability to distinguish between infusion into the vein and an infiltration event using bioresistance and bioreactance (derived from EBI), as well as skin strain. Skin strain and bioresistance sensing could achieve detection rates greater than 0.9 for infiltration fluid volumes of 2 and 10 mL, respectively, for a given false positive, i.e., false alarm rate of 0.05. Furthermore, the fusion of multiple sensing modalities could achieve a detection rate of 0.97 with a false alarm rate of 0.096 for 5mL fluid volume of infiltration. EBI and skin strain sensing can enable non-invasive and real-time IV infiltration detection systems. Fusion of multiple sensing modalities can help to detect expanded range of leaking fluid volumes. The provided performance results and comparisons in this paper are an important step towards clinical translation of sensing technologies for detecting IV infiltration.","2168-2372","","10.1109/JTEHM.2018.2815539","Georgia Tech/Emory University Coulter Translational Research Partnership Program and the Pediatric Technology Center at Georgia Tech; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330755","Bioimpedance;detection performance;extravasation;IV infiltration;non-invasive sensing;sensor fusion;skin strain","Skin;Strain;Fluids;Veins;Animals;Capacitive sensors","bioelectric phenomena;biological tissues;biomedical measurement;blood vessels;diseases;hospitals;medical image processing;patient care;patient monitoring;patient treatment;skin","tissue damage;automatic intravenous infiltration detection;noninvasive intravenous infiltration detection;bioimpedance evaluation;skin strain evaluation;intravenous IV therapy;hospital settings;real-time IV infiltration detection systems;multiple sensing modalities;false alarm rate;infiltration fluid volumes;bioresistance sensing;infiltration event;automatic detection;skin strain sensing;EBI;noninvasive sensing modalities;peripheral vein;pig model","","2","","22","","3 Apr 2018","","","IEEE","IEEE Journals"
"Automatic Segmentation of Skin Lesion Images using Evolutionary Strategy","N. Situ; X. Yuan; G. Zouridakis; N. Mullani","Computer Science Department, University of Houston; Engineering Technology Department, University of Houston; Computer Science Department, University of Houston; Translite LLC","2007 IEEE International Conference on Image Processing","12 Nov 2007","2007","6","","VI - 277","VI - 280","Malignant melanoma has a good prognosis if treated early. Accurate skin lesion segmentation from the background skin is important not only because the shape feature can be directly derived from the process, but also because it can provide a scope for texture analysis. In this paper, we propose an evolutionary strategy based segmentation algorithm to identify the lesion area by an ellipse. It can detect the lesion automatically without setting parameters manually. The method is validated by experiments and comparisons with manually segmentation by an expert and algorithms developed by M. Doshi, et al. (2004).","2381-8549","978-1-4244-1436-9","10.1109/ICIP.2007.4379575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4379575","Evolutionary Strategy;image segmentation;biomedical application;fitness function;skin lesion","Image segmentation;Lesions;Cancer detection;Malignant tumors;Skin cancer;Microscopy;Pigmentation;Computer science;Shape;Fuzzy systems","bioluminescence;biomedical optical imaging;cancer;evolutionary computation;image segmentation;image texture;medical image processing;skin;tumours","automatic segmentation;skin lesion images;evolutionary strategy;malignant melanoma;shape feature;texture analysis;cross-polarization epiluminescence microscopy;transillumination epiluminescence microscopy","","8","","14","","12 Nov 2007","","","IEEE","IEEE Conferences"
"Measurement of noise in ECG signals to improve automatic delineation","L. Galeotti; L. Johannesen; J. Vicente; D. G. Strauss","Division of Physics, Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, US Food and Drug Administration, Silver Spring, MD, USA; Division of Pharmacometrics, Office of Clinical Pharmacology, Office of Translational Sciences, Center for Drug Evaluation and Research, US Food and Drug Administration, Silver Spring, MD, USA; Division of Physics, Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, US Food and Drug Administration, Silver Spring, MD, USA; Division of Physics, Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, US Food and Drug Administration, Silver Spring, MD, USA","Computing in Cardiology 2013","16 Jan 2014","2013","","","511","514","Evaluation of quality in digital ECG recordings is of importance when analyzing data archives or for automatic patient monitoring. In these settings, using noisy signals may lead to incorrectly measured ECGs or triggering false alarms in patient monitoring. Empirically defined thresholds are usually employed to exclude noisy recordings.","2325-8853","978-1-4799-0886-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6713426","","Electrocardiography;Noise;Noise measurement;Biomedical measurement;Abstracts;Databases;Estimation","electrocardiography;medical signal processing;patient monitoring;signal denoising","ECG signals;automatic delineation;digital ECG recordings;data archives;patient monitoring;noisy signals;residual noise;muscular noise;ECG measurements;generalized approach;ECG noise levels;T-offset error;noise patterns","","","","12","","16 Jan 2014","","","IEEE","IEEE Conferences"
"Recovery of acronyms, out-of-lattice words and pronunciations from parallel multilingual speech","J. Miranda; J. P. Neto; A. W. Black","INESC-ID/Instituto Superior Técnico, Lisboa, Portugal; INESC-ID/Instituto Superior Técnico, Lisboa, Portugal; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA","2012 IEEE Spoken Language Technology Workshop (SLT)","31 Jan 2013","2012","","","348","353","In this work we present a set of techniques which explore information from multiple, different language versions of the same speech, to improve Automatic Speech Recognition (ASR) performance. Using this redundant information we are able to recover acronyms, words that cannot be found in the multiple hypotheses produced by the ASR systems, and pronunciations absent from their pronunciation dictionaries. When used together, the three techniques yield a relative improvement of 5.0% over the WER of our baseline system, and 24.8% relative when compared with standard speech recognition, in an Europarl Committee dataset with three different languages (Portuguese, Spanish and English). One full iteration of the system has a parallel Real Time Factor (RTF) of 3.08 and a sequential RTF of 6.44.","","978-1-4673-5126-3","10.1109/SLT.2012.6424248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424248","speech recognition;machine translation;pronunciation;out-of-lattice;acronyms","Speech;Lattices;Speech recognition;Acoustics;Dictionaries;Europe","language translation;natural language processing;performance evaluation;speech recognition","acronym recovery;out-of-lattice word recovery;parallel multilingual speech;out-of-lattice pronunciation recovery;automatic speech recognition performance improvement;ASR performance improvement;ASR systems;pronunciation dictionaries;Europarl Committee dataset;Portuguese language;Spanish language;English language;parallel real time factor;parallel RTF;machine translation","","2","","11","","31 Jan 2013","","","IEEE","IEEE Conferences"
"Joint optimization of LCMV beamforming and acoustic echo cancellation for automatic speech recognition","W. Herbordtt; S. Nakamura; W. Kellermann","ATR Spoken Language Translation Res. Labs., Kyoto, Japan; ATR Spoken Language Translation Res. Labs., Kyoto, Japan; NA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","3","","iii/77","iii/80 Vol. 3","For full-duplex hands-free acoustic human/machine interfaces, a combination of acoustic echo cancellation and speech enhancement is often required in order to suppress acoustic echoes, local interference and noise. In order to exploit positive synergies between acoustic echo cancellation and speech enhancement optimally, we previously presented a combined least-squares (LS) optimization criterion for the integration of acoustic echo cancellation and adaptive linearly-constrained minimum variance (LCMV) beamforming (Herbordt, W. et al., Proc. EURASIP European Sig. Process. Conf., 2004). By means of speech recognition experiments, we now illustrate the efficiency of the proposed solution in situations with high levels of background noise and with time-varying echo paths and frequent double-talk.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1415650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415650","","Array signal processing;Echo cancellers;Automatic speech recognition;Speech enhancement;Humans;Echo interference;Interference suppression;Acoustic noise;Noise cancellation;Speech recognition","optimisation;echo suppression;acoustic signal processing;speech recognition;audio user interfaces;speech-based user interfaces;natural language interfaces;speech enhancement;interference suppression;acoustic noise;random noise;array signal processing","adaptive linearly-constrained minimum variance beamforming;acoustic echo cancellation;automatic speech recognition;hands-free acoustic human/machine interfaces;speech enhancement;interference suppression;noise suppression;least-squares optimization criterion;time-varying echo paths;double-talk;adaptive beamforming microphone arrays","","16","15","14","","9 May 2005","","","IEEE","IEEE Conferences"
"Cross-lingual lexical language discovery from audio data using multiple translations","F. Stahlberg; T. Schlippe; S. Vogel; T. Schultz","Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany; Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany; Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar; Cognitive Systems Lab, Karlsruhe Institute of Technology, Germany","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","6 Aug 2015","2015","","","5823","5827","Zero-resource Automatic Speech Recognition (ZR ASR) addresses target languages without given pronunciation dictionary, transcribed speech, and language model. Lexical discovery for ZR ASR aims to extract word-like chunks from speech. Lexical discovery benefits from the availability of written translations in another source language. In this paper, we improve lexical discovery even more by combining multiple source languages. We present a novel method for combining noisy word segmentations resulting in up to 11.2% relative F-score gain. When we extract word pronunciations from the combined segmentations to bootstrap an ASR system, we improve accuracy by 9.1% relative compared to the best system with only one translation, and by 50.1% compared to monolingual lexical discovery.","2379-190X","978-1-4673-6997-8","10.1109/ICASSP.2015.7179088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7179088","Lexical language discovery;zero-resource automatic speech recognition;word-to-phoneme alignment;non-written languages","Speech;Zirconium;Dictionaries;Acoustics;Automatic speech recognition;Computational modeling","natural language processing;speech recognition","cross lingual lexical language discovery;audio data;multiple language translation;zero resource automatic speech recognition;word-like chunks;lexical discovery;multiple source language;noisy word segmentation","","2","","28","","6 Aug 2015","","","IEEE","IEEE Conferences"
"Extreme Learning Machine With Affine Transformation Inputs in an Activation Function","J. Cao; K. Zhang; H. Yong; X. Lai; B. Chen; Z. Lin","Key Lab for IOT and Information Fusion Technology of Zhejiang, Hangzhou Dianzi University, Hangzhou, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Key Lab for IOT and Information Fusion Technology of Zhejiang, Hangzhou Dianzi University, Hangzhou, China; School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Neural Networks and Learning Systems","18 Jun 2019","2019","30","7","2093","2107","The extreme learning machine (ELM) has attracted much attention over the past decade due to its fast learning speed and convincing generalization performance. However, there still remains a practical issue to be approached when applying the ELM: the randomly generated hidden node parameters without tuning can lead to the hidden node outputs being nonuniformly distributed, thus giving rise to poor generalization performance. To address this deficiency, a novel activation function with an affine transformation (AT) on its input is introduced into the ELM, which leads to an improved ELM algorithm that is referred to as an AT-ELM in this paper. The scaling and translation parameters of the AT activation function are computed based on the maximum entropy principle in such a way that the hidden layer outputs approximately obey a uniform distribution. Application of the AT-ELM algorithm in nonlinear function regression shows its robustness to the range scaling of the network inputs. Experiments on nonlinear function regression, real-world data set classification, and benchmark image recognition demonstrate better performance for the AT-ELM compared with the original ELM, the regularized ELM, and the kernel ELM. Recognition results on benchmark image data sets also reveal that the AT-ELM outperforms several other state-of-the-art algorithms in general.","2162-2388","","10.1109/TNNLS.2018.2877468","National Natural Science Foundation of China; Deutscher Akademischer Austauschdienst; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8533625","Affine transformation (AT) activation function;classification;extreme learning machine (ELM);maximum entropy;regression","Training;Entropy;Benchmark testing;Distributed databases;Learning systems;Tuning;Approximation algorithms","affine transforms;entropy;image classification;learning (artificial intelligence);regression analysis","nonlinear function regression;original ELM;regularized ELM;kernel ELM;extreme learning machine;affine transformation inputs;fast learning speed;node parameters;hidden node;improved ELM algorithm;translation parameters;hidden layer;uniform distribution;AT-ELM algorithm;network inputs;generalization performance;activation function","","23","","53","","13 Nov 2018","","","IEEE","IEEE Journals"
"Fusing images with different focuses using support vector machines","Shutao Li; J. T. -. Kwok; I. W. -. Tsang; Yaonan Wang","Coll. of Electr. & Inf. Eng., Hunan Univ., Changsha, China; NA; NA; NA","IEEE Transactions on Neural Networks","8 Nov 2004","2004","15","6","1555","1561","Many vision-related processing tasks, such as edge detection, image segmentation and stereo matching, can be performed more easily when all objects in the scene are in good focus. However, in practice, this may not be always feasible as optical lenses, especially those with long focal lengths, only have a limited depth of field. One common approach to recover an everywhere-in-focus image is to use wavelet-based image fusion. First, several source images with different focuses of the same scene are taken and processed with the discrete wavelet transform (DWT). Among these wavelet decompositions, the wavelet coefficient with the largest magnitude is selected at each pixel location. Finally, the fused image can be recovered by performing the inverse DWT. In this paper, we improve this fusion procedure by applying the discrete wavelet frame transform (DWFT) and the support vector machines (SVM). Unlike DWT, DWFT yields a translation-invariant signal representation. Using features extracted from the DWFT coefficients, a SVM is trained to select the source image that has the best focus at each pixel location, and the corresponding DWFT coefficients are then incorporated into the composite wavelet representation. Experimental results show that the proposed method outperforms the traditional approach both visually and quantitatively.","1941-0093","","10.1109/TNN.2004.837780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1353290","Image fusion;support vector machines;wavelet transform","Focusing;Support vector machines;Discrete wavelet transforms;Layout;Image edge detection;Image segmentation;Lenses;Image fusion;Wavelet coefficients;Signal representations","support vector machines;discrete wavelet transforms;image segmentation;stereo image processing;image matching;image representation;edge detection","wavelet-based image fusion;support vector machines;discrete wavelet frame transform;translation-invariant signal representation;feature extraction;composite wavelet representation","Algorithms;Artificial Intelligence;Computer Simulation;Computing Methodologies;Fixation, Ocular;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Neural Networks (Computer);Pattern Recognition, Automated;Signal Processing, Computer-Assisted;Subtraction Technique","93","55","32","","8 Nov 2004","","","IEEE","IEEE Journals"
"Heart Sound Signal Classification Algorithm: A Combination of Wavelet Scattering Transform and Twin Support Vector Machine","J. Li; L. Ke; Q. Du; X. Ding; X. Chen; D. Wang","School of Electrical Engineering, Shenyang University of Technology, Shenyang, China; School of Electrical Engineering, Shenyang University of Technology, Shenyang, China; School of Electrical Engineering, Shenyang University of Technology, Shenyang, China; School of Electrical Engineering, Shenyang University of Technology, Shenyang, China; School of Electrical Engineering, Shenyang University of Technology, Shenyang, China; School of Electrical Engineering, Shenyang University of Technology, Shenyang, China","IEEE Access","20 Dec 2019","2019","7","","179339","179348","By classifying the heart sound signals, it can provide very favorable clinical information to the diagnosis of cardiovascular diseases. According to the characteristics of heart sound signals which are complex and difficult to classify and recognize, a new method of feature extraction and classification about heart sound signal is proposed by a combination of wavelet scattering transform and twin support vector machine in this paper. The method is as follows: The heart sound signal data set is firstly divided into two parts, one as a training set and the other as a testing set. Then the wavelet scattering transform is applied to the heart sound signals in the training set and the testing set. The scattering transform is a new time-frequency analysis method. It overcomes the shortcomings of the traditional wavelet transform which has the time-shift changes. It has the advantages of translation invariance and elastic deformation stability. Thus obtain the scattering feature matrix of the heart sound signal. Due to the large dimension of scattering feature matrix, this paper uses multidimensional scaling (MDS) method to reduce the dimension. This method is compared with the classical dimension reduction method-principal component analysis (PCA). Finally, the dimensionality-reduced feature matrix is input into the twin support vector machine (TWSVM) for training. After training the classifier to get the optimal parameters, the dimensionality-reduced scattering feature matrix of the testing signal is input into the classifier for testing. Experimental results show that the classification accuracy of the proposed method can reach 98% or more, and the running time is greatly reduced compared with support vector machine (SVM).","2169-3536","","10.1109/ACCESS.2019.2959081","National Natural Science Foundation of China; Natural Science Foundation of Liaoning Province; Fundamental Research Funds in Heilongjiang Provincial Universities of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931765","Wavelet scattering transform;multidimensional scaling (MDS);twin support vector machine (TWSVM);signal classification","Heart;Wavelet transforms;Support vector machines;Feature extraction;Scattering;Classification algorithms","cardiology;diseases;feature extraction;matrix algebra;medical signal processing;signal classification;support vector machines;time-frequency analysis;wavelet transforms","multidimensional scaling;translation invariance;elastic deformation stability;time-frequency analysis;feature extraction;principal component analysis;cardiovascular diseases;dimensionality-reduced scattering feature matrix;twin support vector machine;wavelet scattering transform;heart sound signal classification algorithm","","9","","42","CCBY","12 Dec 2019","","","IEEE","IEEE Journals"
"Generation of Stimulus Triggering From Intracortical Spike Activity for Brain–Machine–Body Interfaces (BMBIs)","S. Shahdoost; R. J. Nudo; P. Mohseni","Electrical Engineering and Computer Science Department, Case Western Reserve University, Cleveland, OH, USA; Rehabilitation Medicine Department, University of Kansas Medical Center, Kansas City, KS; Electrical Engineering and Computer Science Department, Case Western Reserve University, Cleveland, OH, USA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","3 Aug 2017","2017","25","7","998","1008","Brain-machine-body interfaces (BMBIs) aim to create an artificial connection in the nervous system by converting neural activity recorded from one cortical region to electrical stimuli delivered to another cortical region, spinal cord, or muscles in real-time. In particular, conditioning-mode BMBIs utilize such activity-dependent stimulation strategies to induce functional re-organization in the nervous system and promote functional recovery after injury by exploiting mechanisms underlying neuroplasticity. This paper reports on reconfigurable, field-programmable gate array (FPGA)-based implementation of a translation algorithm to extract multichannel stimulus trigger signals from intracortical neural spike activity. The approach features digital spike discrimination based on user-set thresholding and time-amplitude windowing, decision making to support different triggering patterns for various stimulation scenarios, as well as trigger-pattern-dependent blanking schemes for robust operation in the presence of stimulus artifacts. Readily lending itself to low-power, low-area implementation for future integration, the algorithm has been synthesized on a Cyclone II FPGA using Altera's Quartus II design software and validated experimentally with prerecorded intracortical neural spike activity from an anesthetized laboratory rat.","1558-0210","","10.1109/TNSRE.2016.2615270","Family Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7583656","Brain–machine–body interfaces (BMBIs);closed-loop systems;spike discrimination;stimulation;trigger generation","Field programmable gate arrays;Timing;Clocks;Real-time systems;Nervous system;Blanking;Hardware","brain-computer interfaces;decision making;electroencephalography;field programmable gate arrays;injuries;medical signal processing;muscle;neurophysiology","stimulus triggering;intracortical spike activity;brain-machine-body interfaces;artificial connection;nervous system;neural activity;cortical region;electrical stimuli;spinal cord;muscles;conditioning-mode BMBI;activity-dependent stimulation strategies;functional reorganization;functional recovery;injury;neuroplasticity;reconfigurable field-programmable gate array-based implementation;translation algorithm;multichannel stimulus trigger signals;digital spike discrimination;user-set thresholding;time-amplitude windowing;decision making;triggering patterns;trigger-pattern-dependent blanking schemes;stimulus artifacts;Cyclone II FPGA;Altera's Quartus II design software;prerecorded intracortical neural spike activity;anesthetized laboratory rat","","","","33","","5 Oct 2016","","","IEEE","IEEE Journals"
"Intelligent Biofeedback Augmented Content Comprehension (TellBack)","H. Hijazi; R. Couceiro; J. Castelhano; P. De Carvalho; M. Castelo-Branco; H. Madeira","Centre of Informatics and Systems, University of Coimbra (CISUC), Coimbra, Portugal; Centre of Informatics and Systems, University of Coimbra (CISUC), Coimbra, Portugal; Coimbra Institute for Biomedical Imaging and Translational Research (CIBIT), Institute of Nuclear Sciences Applied to Health (ICNAS), University of Coimbra, Coimbra, Portugal; Centre of Informatics and Systems, University of Coimbra (CISUC), Coimbra, Portugal; Coimbra Institute for Biomedical Imaging and Translational Research (CIBIT), Institute of Nuclear Sciences Applied to Health (ICNAS), University of Coimbra, Coimbra, Portugal; Centre of Informatics and Systems, University of Coimbra (CISUC), Coimbra, Portugal","IEEE Access","19 Feb 2021","2021","9","","28393","28406","Assessing comprehension difficulties requires the ability to assess cognitive load. Changes in cognitive load induced by comprehension difficulties could be detected with an adequate time resolution using different biofeedback measures (e.g., changes in the pupil diameter). However, identifying the Spatio-temporal sources of content comprehension difficulties (i.e., when, and where exactly the difficulty occurs in content regions) with a fine granularity is a big challenge that has not been explicitly addressed in the state-of-the-art. This paper proposes and evaluates an innovative approach named Intelligent BiofeedbackAugmented Content Comprehension (TellBack) to explicitly address this challenge. The goal is to autonomously identify regions of digital content that cause user's comprehension difficulty, opening the possibility to provide real-time comprehension support to users. TellBack is based on assessing the cognitive load associated with content comprehension through non-intrusive cheap biofeedback devices that acquire measures such as pupil response or Heart Rate Variability (HRV). To identify when exactly the difficulty in comprehension occurs, physiological manifestations of the Autonomic Nervous System (ANS) such as the pupil diameter variability and the modulation of HRV are exploited, whereas the fine spatial resolution (i.e., the region of content where the user is looking at) is provided by eye-tracking. The evaluation results of this approach show an accuracy of 83.00% ± 0.75 in classifying regions of content as difficult or not difficult using Support Vector Machine (SVM), and precision, recall, and micro F1-score of 0.89, 0.79, and 0.83, respectively. Results obtained with 4 other classifiers, namely Random Forest, k-nearest neighbor, Decision Tree, and Gaussian Naive Bayes, showed a slightly lower precision. TellBack outperforms the state-of-the-art in precision & recall by 23% and 17% respectively.","2169-3536","","10.1109/ACCESS.2021.3058664","BASE project; POCI - 01-0145 - FEDER- 031581; Centro de Informtica e Sistemas da Universidade de Coimbra (CISUC); Coimbra Institute for Biomedical Imaging and Translational Research (CIBIT), Institute of Nuclear Sciences Applied to Health (ICNAS), University of Coimbra; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9352725","Biomedical measurement;cognitive load;content comprehension;eye-tracking;heart rate variability;machine learning","Heart rate variability;Task analysis;Software;Pupils;Real-time systems;Biomarkers;Informatics","biomedical measurement;cardiology;cognition;decision trees;Gaussian processes;gaze tracking;medical signal processing;naive Bayes methods;nearest neighbour methods;physiology;random forests;signal classification;signal resolution;support vector machines","random forest;k-nearest neighbor;decision tree;Gaussian naive Bayes;biomedical measurement;region classification;classifiers;SVM;support vector machine;eye-tracking;ANS;autonomic nervous system;physiological manifestations;HRV;heart rate variability;pupil response;comprehension difficulties;biofeedback measures;spatio-temporal sources;intelligent biofeedback augmented content comprehension;spatial resolution;pupil diameter variability;nonintrusive cheap biofeedback devices;real-time comprehension support;digital content;content regions;time resolution;cognitive load;TellBack","","","","29","CCBY","11 Feb 2021","","","IEEE","IEEE Journals"
"Efficient Labeling of EEG Signal Artifacts Using Active Learning","V. Lawhern; D. Slayback; D. Wu; B. J. Lance","Translational Neurosci. Branch, US Army Res. Lab., Aberdeen Proving Ground, MD, USA; Translational Neurosci. Branch, US Army Res. Lab., Aberdeen Proving Ground, MD, USA; Machine Learning Lab., GE Global Res., Niskayuna, NY, USA; Translational Neurosci. Branch, US Army Res. Lab., Aberdeen Proving Ground, MD, USA","2015 IEEE International Conference on Systems, Man, and Cybernetics","14 Jan 2016","2015","","","3217","3222","Electroencephalography (EEG) has been widely used in a variety of contexts, including medical monitoring of subjects as well as performance monitoring in healthy individuals. Recent technological advances have now enabled researchers to quickly record and collect EEG on a wide scale. Although EEG is fairly easy to record, it is highly susceptible to noise sources called artifacts which can occur at amplitudes several times greater than the EEG signal of interest. Because of this, users must manually annotate the EEG signal to identify artifact regions in the data prior to any downstream processing. This can be time-consuming and impractical for large data collections. In this paper we present a method which uses Active Learning (AL) to improve the reliability of existing EEG artifact classifiers with minimal amounts of user interaction. Our results show that classification accuracy equivalent to classifiers trained on full data annotation can be obtained while labeling less than 25% of the data. This suggests significant time savings can be obtained when manually annotating artifacts in large EEG data collections.","","978-1-4799-8697-2","10.1109/SMC.2015.558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379690","EEG;Artifacts;Active Learning;Support Vector Machine;Autoregressive Model","Electroencephalography;Brain modeling;Labeling;Support vector machines;Electrooculography;Electromyography;Muscles","electroencephalography;learning (artificial intelligence);medical signal processing;reliability;signal classification","EEG signal artifact labeling;active learning;electroencephalography;medical monitoring;performance monitoring;noise sources;large data collections;reliability improvement;EEG artifact classifiers;data annotation","","8","","26","","14 Jan 2016","","","IEEE","IEEE Conferences"
"Hierarchical internal representation of spectral features in deep convolutional networks trained for EEG decoding","K. G. Hartmann; R. T. Schirrmeister; T. Ball","Translational Neurotechnology Lab, Medical Center - University of Freiburg, Freiburg, Germany; Translational Neurotechnology Lab, Medical Center - University of Freiburg, Freiburg, Germany; Translational Neurotechnology Lab, Medical Center - University of Freiburg, Freiburg, Germany","2018 6th International Conference on Brain-Computer Interface (BCI)","12 Mar 2018","2018","","","1","6","Recently, there is increasing interest and research on the interpretability of machine learning models, for example how they transform and internally represent EEG signals in Brain-Computer Interface (BCI) applications. This can help to understand the limits of the model and how it may be improved, in addition to possibly provide insight about the data itself. Schirrmeister et al. (2017) have recently reported promising results for EEG decoding with deep convolutional neural networks (ConvNets) trained in an end-to-end manner and, with a causal visualization approach, showed that they learn to use spectral amplitude changes in the input. In this study, we investigate how ConvNets represent spectral features through the sequence of intermediate stages of the network. We show higher sensitivity to EEG phase features at earlier stages and higher sensitivity to EEG amplitude features at later stages. Intriguingly, we observed a specialization of individual stages of the network to the classical EEG frequency bands alpha, beta, and high gamma. Furthermore, we find first evidence that particularly in the last convolutional layer, the network learns to detect more complex oscillatory patterns beyond spectral phase and amplitude, reminiscent of the representation of complex visual features in later layers of ConvNets in computer vision tasks. Our findings thus provide insights into how ConvNets hierarchically represent spectral EEG features in their intermediate layers and suggest that ConvNets can exploit and might help to better understand the compositional structure of EEG time series.","2572-7672","978-1-5386-2574-3","10.1109/IWW-BCI.2018.8311493","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8311493","Electroencephalography;EEG analysis;machine learning;convolutional networks;visualization;model inter-pretability;spectral features","Electroencephalography;Perturbation methods;Correlation;Convolution;Decoding;Brain modeling;Feature extraction","brain-computer interfaces;computer vision;convolution;electroencephalography;feedforward neural nets;learning (artificial intelligence);medical signal processing","spectral EEG features;computer vision tasks;complex visual features;spectral phase;convolutional layer;classical EEG frequency bands alpha;individual stages;EEG amplitude features;earlier stages;EEG phase features;higher sensitivity;intermediate stages;spectral amplitude changes;causal visualization approach;deep convolutional neural networks;Brain-Computer Interface applications;machine learning models;EEG decoding;deep convolutional networks;spectral features;hierarchical internal representation;ConvNets;intermediate layers","","7","","20","","12 Mar 2018","","","IEEE","IEEE Conferences"
"Machine Learning Techniques for Ophthalmic Data Processing: A Review","M. H. Sarhan; M. A. Nasseri; D. Zapp; M. Maier; C. P. Lohmann; N. Navab; A. Eslami","Translational Research Lab, Carl Zeiss Meditec, Munich, Germany; Department of Ophthalmology, Klinikum rechts der Isar, Technical University of Munich, Munich, Germany; Department of Ophthalmology, Klinikum rechts der Isar, Technical University of Munich, Munich, Germany; Department of Ophthalmology, Klinikum rechts der Isar, Technical University of Munich, Munich, Germany; Department of Ophthalmology, Klinikum rechts der Isar, Technical University of Munich, Munich, Germany; Computer Aided Medical Procedures chair, Technical University of Munich, Munich, Germany; Translational Research Lab, Carl Zeiss Meditec, Munich, Germany","IEEE Journal of Biomedical and Health Informatics","4 Dec 2020","2020","24","12","3338","3350","Machine learning and especially deep learning techniques are dominating medical image and data analysis. This article reviews machine learning approaches proposed for diagnosing ophthalmic diseases during the last four years. Three diseases are addressed in this survey, namely diabetic retinopathy, age-related macular degeneration, and glaucoma. The review covers over 60 publications and 25 public datasets and challenges related to the detection, grading, and lesion segmentation of the three considered diseases. Each section provides a summary of the public datasets and challenges related to each pathology and the current methods that have been applied to the problem. Furthermore, the recent machine learning approaches used for retinal vessels segmentation, and methods of retinal layers and fluid segmentation are reviewed. Two main imaging modalities are considered in this survey, namely color fundus imaging, and optical coherence tomography. Machine learning approaches that use eye measurements and visual field data for glaucoma detection are also included in the survey. Finally, the authors provide their views, expectations and the limitations of the future of these techniques in the clinical practice.","2168-2208","","10.1109/JBHI.2020.3012134","Carl Zeiss Meditec AG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9151176","Ophthalmic diagnostics;deep learning;diabetic retinopathy;age-related macular degeneration;glaucoma","Image segmentation;Diabetes;Retinopathy;Machine learning;Lesions;Retina;Ophthalmology;Glaucoma","biomedical optical imaging;blood vessels;data analysis;diseases;eye;image segmentation;learning (artificial intelligence);medical image processing;neural nets;optical tomography;reviews;vision defects","ophthalmic data processing;deep learning techniques;medical image;data analysis;ophthalmic diseases;diabetic retinopathy;age-related macular degeneration;lesion segmentation;machine learning;retinal vessels segmentation;retinal layers;fluid segmentation;color fundus imaging;visual field data;glaucoma detection;optical coherence tomography;eye measurements","Deep Learning;Diagnostic Techniques, Ophthalmological;Glaucoma;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Retinal Diseases;Tomography, Optical Coherence","2","","107","CCBY","28 Jul 2020","","","IEEE","IEEE Journals"
"A brain machine interface control algorithm designed from a feedback control perspective","V. Gilja; P. Nuyujukian; C. A. Chestek; J. P. Cunningham; B. M. Yu; J. M. Fan; S. I. Ryu; K. V. Shenoy","Dept. of Computer Science, Stanford University, CA, USA; Dept. of Bioengineering, Stanford University, CA, USA; Stanford Inst. for Neuro-Innovation and Translational Neuroscience, Stanford University, CA, USA; Dept. of Electr. Eng., Stanford Univ., Stanford, CA, USA; Dept. of Electr. Eng., Stanford Univ., Stanford, CA, USA; Dept. of Bioengineering, Stanford University, CA, USA; Dept. of Electr. Eng., Stanford Univ., Stanford, CA, USA; Stanford Inst. for Neuro-Innovation and Translational Neuroscience, Stanford University, CA, USA","2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","10 Nov 2012","2012","","","1318","1322","We present a novel brain machine interface (BMI) control algorithm, the recalibrated feedback intention-trained Kalman filter (ReFIT-KF). The design of ReFIT-KF is motivated from a feedback control perspective applied to existing BMI control algorithms. The result is two design innovations that alter the modeling assumptions made by these algorithms and the methods by which these algorithms are trained. In online neural control experiments recording from a 96-electrode array implanted in M1 of a macaque monkey, the ReFIT-KF control algorithm demonstrates large performance improvements over the current state of the art velocity Kalman filter, reducing target acquisition time by a factor of two, while maintaining a 500 ms hold period, thereby increasing the clinical viability of BMI systems.","1558-4615","978-1-4577-1787-1","10.1109/EMBC.2012.6346180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6346180","","Kinematics;Kalman filters;Decoding;Technological innovation;Algorithm design and analysis;Uncertainty;Prosthetics","biomedical electrodes;feedback;Kalman filters;medical control systems;medical signal processing;user interfaces","brain machine interface control algorithm;feedback control;recalibrated feedback intention-trained Kalman filter;design innovation;online neural control experiment;96-electrode array;macaque monkey;performance improvement;velocity Kalman filter;BMI system;time 500 ms","Algorithms;Animals;Arm;Biomechanical Phenomena;Brain-Computer Interfaces;Electrodes, Implanted;Feedback;Macaca;Male","10","1","18","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Single closed contour trademark classification based on support vector machine","R. Haitao; L. Yeli; L. Likun","Beijng Institute of Graphic Communication, Beijing, China; Beijng Institute of Graphic Communication, Beijing, China; Beijng Institute of Graphic Communication, Beijing, China","2010 3rd International Congress on Image and Signal Processing","29 Nov 2010","2010","4","","1942","1946","Given an single closed contour trademark image, shape is one of the most important features in content-based trademark image retrieval and classification. So, we can extract the target image contour Fourier descriptor as feature vector. Fourier moments are not invariant to image scaling, rotation and translation, therefore Fourier moments are used as feature vector such that Classifier has better classification performance than traditional classification methods. The application of Support Vector Machine model solves the problems of poor generalization performance, local minimum and over fitting. In addition, kernel function applied in support vector machine maps data set linear inseparable to a higher dimensional space where the training set is separable. For this reason Support Vector Machine classifiers are widely used in pattern recognition.","","978-1-4244-6516-3","10.1109/CISP.2010.5648105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648105","trademark classfication;fourier descriptor;support vector machine;kernel function","Trademarks;Shape;Support vector machines;Feature extraction;Frequency domain analysis;Kernel;Training","content-based retrieval;feature extraction;Fourier transforms;image classification;image retrieval;support vector machines","single closed contour trademark classification;support vector machine;single closed contour trademark image;content-based trademark image retrieval;image classification;image contour Fourier descriptor;feature vector;image scaling;Fourier moments;pattern recognition","","1","","10","","29 Nov 2010","","","IEEE","IEEE Conferences"
"Sign language recognition using the Extreme Learning Machine","M. M. Sole; M. S. Tsoeu","Department of Electrical Engineering, University Of Cape Town (UCT), South Africa; Department of Electrical Engineering, University Of Cape Town (UCT), South Africa","IEEE Africon '11","10 Nov 2011","2011","","","1","6","The Extreme Learning Machine (ELM) is a simplified neural network. It non-linearly embeds input data in a higher dimensional space using randomly generated sigmoidal basis functions. The training target vector is then approximated by a linear weighted sum of these basis functions. In this paper the ELM algorithm has been applied to classify static hand gestures that represent different letters of the Auslan(Australian Sign Language) dictionary. ELM offers very fast learning with very consistent performance. It has only one tuning parameter. It can be adapted to multiclass classification and multi output regression without any increase in training time. Its low computational intensity and short training time makes it superior to traditional algorithms such as Hidden Markov Models (HMMs), Single and Recurrent Feed Forward Neural Networks for real time translations. Preliminary experimental results have shown that ELM can produce good generalization performance as a classifier. Increasing the dimensionality of the data results in better separation. Consequently the gestures become more distinguishable improving the probability of correct classification. An investigation into its classification performance for the entire alphabet is currently under way.","2153-0033","978-1-61284-993-5","10.1109/AFRCON.2011.6072114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6072114","Extreme Learning machine (ELM);Artificial Neural Networks (ANN);Instrumented glove;Hidden Markov Models (HMMs)","Handicapped aids;Hidden Markov models;Artificial neural networks;Neurons;Training;Training data;Fingers","gesture recognition;image classification;neural nets;regression analysis","sign language recognition;extreme learning machine;neural network;sigmoidal basis function;Australian sign language dictionary;multiclass classification;multioutput regression;classification probability;static hand gestures recognition","","12","","19","","10 Nov 2011","","","IEEE","IEEE Conferences"
"Machine Learning Model Based Digital Hardware System Design for Detection of Sleep Apnea Among Neonatal Infants","O. Hassan; D. Parvin; S. Kamrul","University of Missouri,Islam Department of Electrical Engineering and Computer Science,Columbia,MO,USA,65211; University of Missouri,Islam Department of Electrical Engineering and Computer Science,Columbia,MO,USA,65211; University of Missouri,Islam Department of Electrical Engineering and Computer Science,Columbia,MO,USA,65211","2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS)","2 Sep 2020","2020","","","607","610","This paper presents a fully integrated machine learning (ML) based hardware system for detection of sleep apnea among infants in neonatal intensive care unit (NICU). The system is comprised of a PVDF sensor and a pulse oximeter to acquire breathing signal and oxygen saturation level, respectively, representing the input data. Accuracy rate of this system is over 85 percent with low error loss. The trained ML model has been developed in digital hardware design platform by translating each component into corresponding logic block. Estimated power consumption budget of this system is below 9W. This model can be adopted in future low-cost ML on-chip biomedical system design for apnea detection.","1558-3899","978-1-7281-8058-8","10.1109/MWSCAS48704.2020.9184554","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184554","NICU;Respiratory Disorder;Sleep Apnea;Neural Network;Machine Learning;Hardware;Biomedical Device","","learning (artificial intelligence);medical disorders;medical signal processing;oximetry;paediatrics;piezoelectric transducers;pneumodynamics;sleep","sleep apnea;neonatal intensive care unit;breathing signal;oxygen saturation level;trained ML model;digital hardware system design;neonatal infants;fully integrated machine learning based hardware system;PVDF sensor;pulse oximeter;logic block;ML on-chip biomedical system design","","","","18","","2 Sep 2020","","","IEEE","IEEE Conferences"
"Vision-based obstacle detection using a support vector machine","T. W. Ubbens; D. C. Schuurman","Redeemer University College, Ancaster, Ontario, Canada; Redeemer University College, Ancaster, Ontario, Canada","2009 Canadian Conference on Electrical and Computer Engineering","19 Jun 2009","2009","","","459","462","This paper describes a monocular vision-based obstacle detection method for a mobile robot using a support vector machine (SVM). A single camera is mounted on the front of a mobile robot and an SVM is trained to classify obstacles as they are encountered by the robot. Since it is not possible to train on all obstacle types a-priori, a one-class SVM is used to learn the appearance of the floor in the absence of obstacles. Anything that is not recognized as a floor is classified as an obstacle. To improve robustness in recognizing floor features, images are preprocessed using a Fast Fourier Transform (FFT) to provide translation invariance. Experimental results indicate high accuracy and specificity for four different floor surfaces that were tested.","0840-7789","978-1-4244-3509-8","10.1109/CCECE.2009.5090176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090176","Learning systems;machine vision;robot vision systems;mobile robot motion-planning","Support vector machines;Support vector machine classification;Mobile robots;Robot vision systems;Kernel;Training data;Cameras;Image recognition;Testing;Robot sensing systems","fast Fourier transforms;image classification;image recognition;learning (artificial intelligence);mobile robots;robot vision;support vector machines","support vector machine;monocular vision-based obstacle detection;mobile robot;image processing;fast Fourier transform","","4","","8","","19 Jun 2009","","","IEEE","IEEE Conferences"
"On-line longitudinal rip detection of conveyor belts based on machine vision","Y. Yang; Y. Zhao; C. Miao; L. Wang","Tianjin Key Laboratory of Optoelectronic Detection Technology and Systems, Tianjin Polytechnic University, Tianjin, China; Tianjin Key Laboratory of Optoelectronic Detection Technology and Systems, Tianjin Polytechnic University, Tianjin, China; Tianjin Key Laboratory of Optoelectronic Detection Technology and Systems, Tianjin Polytechnic University, Tianjin, China; Tianjin Key Laboratory of Optoelectronic Detection Technology and Systems, Tianjin Polytechnic University, Tianjin, China","2016 IEEE International Conference on Signal and Image Processing (ICSIP)","30 Mar 2017","2016","","","315","318","Longitudinal rip of conveyor belts is a serious threat to safety production. Based on the machine vision technology, an algorithm used to find longitudinal rip of belts on-line from gray belt images directly is proposed. A gray image is first translated into a unidimensional vector. The unidimensional vector is further analyzed to obtain rip eigenfunctions. Then, faults of longitudinal rips are diagnosed by using the rip eigenfunction. The calculation of searching from the unidimensional vector is smaller than searching from the gray image. The validity of the proposed algorithm is testified by the testing results with some belt images.","","978-1-5090-2377-6","10.1109/SIPROCESS.2016.7888275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888275","machine vision;fault detection;gray image;conveyor belt;longitudinal rip","Belts;Eigenvalues and eigenfunctions;Machine vision;Inspection;Testing;Safety","belts;computer vision;conveyors;eigenvalues and eigenfunctions;vectors","longitudinal rip;conveyor belts;safety production;machine vision technology;gray belt images;unidimensional vector;rip eigenfunctions","","2","","10","","30 Mar 2017","","","IEEE","IEEE Conferences"
"GPU Accelerated Contactless Human Machine Interface for Driving Car","F. Magoulès; Q. Zou","CentraleSupelec, Univ. Paris-Saclay, Paris, France; CentraleSupelec, Univ. Paris-Saclay, Paris, France","2017 16th International Symposium on Distributed Computing and Applications to Business, Engineering and Science (DCABES)","11 Jan 2018","2017","","","7","10","In this paper we present an original contactless human machine interface for driving car. The proposed framework is based on the image sent by a simple camera device, which is then processed by various computer vision algorithms. These algorithms allow the isolation of the user's hand on the camera frame and translate its movements into orders sent to the computer in a real time process. The optimization of the implemented algorithms on graphics processing unit leads to real time interaction between the user, the computer and the machine. The user can easily modify or create the interfaces displayed by the proposed framework to fit his personnel needs. A contactless driving car interface is here produced to illustrate the principle of our framework.","2473-3636","978-1-5386-2162-2","10.1109/DCABES.2017.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253024","computer vision;gesture analysis;video analysis;image processing;machine learning;parallel computing;graphics processing unit","Graphics processing units;Image color analysis;Cameras;Automobiles;Skin;Mice;Libraries","cameras;computer vision;gesture recognition;graphics processing units;human computer interaction;intelligent transportation systems","camera frame;contactless driving car interface;GPU accelerated contactless human machine interface;original contactless human machine interface;simple camera device;computer vision algorithms;graphics processing unit","","","","18","","11 Jan 2018","","","IEEE","IEEE Conferences"
"Diversity in Machine Learning","Z. Gong; P. Zhong; W. Hu","National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IEEE Access","29 May 2019","2019","7","","64323","64350","Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a totally good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though diversity plays an important role in the machine learning process, there is no systematical analysis of the diversification in the machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work. Our analysis provides a deeper understanding of the diversity technology in machine learning tasks and hence can help design and learn more effective models for real-world applications.","2169-3536","","10.1109/ACCESS.2019.2917620","National Natural Science Foundation of China; Foundation for the Author of National Excellent Doctoral Dissertation of the People's Republic of China; Program for New Century Excellent Talents in University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8717641","Diversity;training data;model learning;inference;supervised learning;active learning;unsupervised learning;posterior regularization","Machine learning;Data models;Training data;Task analysis;Training;Diversity methods;Supervised learning","cameras;image segmentation;language translation;learning (artificial intelligence);object detection;remote sensing","diversity technology;machine learning system;model training process;data diversification;model diversification;inference diversification;remote sensing imaging tasks;machine translation;camera relocalization;image segmentation;object detection;topic modeling","","5","","164","","17 May 2019","","","IEEE","IEEE Journals"
"Automatic registration of printed analog screen-film mammograms","M. Mustra; D. Mileta; M. Grgic","University of Zagreb, Faculty of EE and Comp, Unska 3, HR-10000 Zagreb, Croatia; Ministry of the Sea, Transport and Infrastructure, Prisavlje 14, HR-10000 Zagreb, Croatia; University of Zagreb, Faculty of EE and Comp, Unska 3, HR-10000 Zagreb, Croatia","2011 18th International Conference on Systems, Signals and Image Processing","8 Aug 2011","2011","","","1","4","Computer aided detection (CAD) in mammography relies on accurate image segmentation. If we consider using CAD on non-digital mammograms, one of the steps which need to be performed is scanning. After scanning, images have to be segmented and this is more complicated than segmentation of digital mammograms. In this paper we propose a method for automatic registration of printed and scanned mammograms. This process allows efficient manual segmentation which gives as accurate as possible results. To translate manual segmentation results performed by radiologists from paper to digital format, image needs to be registered. Manual image registration generally takes too much time and therefore is not as efficient as automatic. This paper considers automatic registration of scanned images using Hough transform. Accurate detection and alignment of marker lines on scanned images gives the correct angle of rotation, as well as translation and resizing coefficient.","2157-8702","978-9958-9966-1-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5977364","Image Registration;Hough Transform;Computer Aided Detection","Image segmentation;Pixel;Image registration;Printing;Transforms;Accuracy;Manuals","diagnostic radiography;Hough transforms;image registration;image segmentation;mammography;medical image processing","automatic mammogram registration;printed analog screen film mammograms;computer aided detection;CAD;image segmentation;nondigital mammograms;Hough transform;marker line detection;marker line alignment;translation coefficient;resizing coefficient","","","","6","","8 Aug 2011","","","IEEE","IEEE Conferences"
"Using non-word lexical units in automatic speech understanding","M. Penagarikano; G. Bordel; A. Varona; K. Lopez de Ipina","Dept. Electr. y Electron., Pais Vasco Univ., Lejona, Spain; NA; NA; NA","1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)","6 Aug 2002","1999","2","","621","624 vol.2","If the objective of a continuous automatic speech understanding system is not a speech-to-text translation, words are not strictly needed, and then the use of alternative lexical units (LUs) will bring us a new degree of freedom to improve the system performance. Consequently, we experimentally explore some methods to automatically extract a set of LUs from a Spanish training corpus and verify that the system can be improved in two ways: reducing the computational costs and increasing the recognition rates. Moreover, preliminary results point out that, even if the system target is a speech-to-text translation, using non-word units and post-processing the output to produce the corresponding word chain outperforms the word based system.","1520-6149","0-7803-5041-3","10.1109/ICASSP.1999.759743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=759743","","Speech;Computational efficiency;Natural languages;System performance;Costs;Joining processes;Adaptation model;Lead;Automatic testing;Databases","speech recognition;computational complexity;natural languages","nonword lexical units;continuous automatic speech understanding system;alternative lexical units;system performance;Spanish training corpus;computational costs;recognition rates;speech-to-text translation;post-processing;word chain","","3","","8","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Creating language and acoustic models using Kaldi to build an automatic speech recognition system for Kannada language","Y. G. Thimmaraja; H. S. Jayanna","Department of Electronics and Communication Engineering Siddaganga Institute of Technology, Tumakuru, Karnataka, India; Department of Information Science and Engineering Siddaganga Institute of Technology, Tumakuru, Karnataka, India","2017 2nd IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","15 Jan 2018","2017","","","161","165","In this paper, creation of the Language Models (LMs) and Acoustic Models (AMs) using Kaldi speech recognition toolkit to build a robust Automatic Speech Recognition (ASR) system for Kannada language is demonstrated. The speech data is collected from the farmers of Karnataka under uncontrolled environment is used for the development of ASR models. The collected speech data needs to be translated to machine level language and hence the Indic Language Transliteration Tool (IT3 to UTF-8) is used for transcription. The dictionary for the collected speech data is created by using Indian Language Speech sound Label (ILSL12) set. The AMs are created by using Gaussian Mixture Model (GMM) and Subspace GMM (SGMM). The 80% and 20% of validated speech data is used for training and testing respectively. The accuracy and Word Error Rate (WER) of ASR models are highlighted and discussed in this work. The developed ASR models can be used in spoken query system which enables the farmers to access the on time agricultural commodity prices and weather information in Kannada language.","","978-1-5090-3704-9","10.1109/RTEICT.2017.8256578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8256578","Kaldi;Accuracy;Word Error Rate (WER);Speech recognition;Language Models (LMs);Acoustic Models (AMs)","Speech;Hidden Markov models;Speech recognition;Training;Testing;Tools;Dictionaries","Gaussian processes;natural language processing;speech recognition","GMM;spoken query system;Gaussian Mixture Model;Indian Language Speech sound Label set;Indic Language Transliteration Tool;ASR models;robust Automatic Speech Recognition system;Kaldi speech recognition toolkit;AMs;Kannada language","","4","","14","","15 Jan 2018","","","IEEE","IEEE Conferences"
"Automatic Spatially Varying Illumination Recovery of Indoor Scenes Based on a Single RGB-D Image","G. Xing; Y. Liu; H. Ling; X. Granier; Y. Zhang","National Key Laboratory of Fundamental Science on Synthetic Vision, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; Department of Computer and Information Sciences, Center for Data Analytics and Biomedical Informatics, Temple University, Philadelphia, PA, USA; LP2N laboratory, Institut d'Optique Graduate School, Bordeaux, France; College of Computer Science, Sichuan University, Chengdu, China","IEEE Transactions on Visualization and Computer Graphics","27 Feb 2020","2020","26","4","1672","1685","We propose an automatic framework to recover the illumination of indoor scenes based on a single RGB-D image. Unlike previous works, our method can recover spatially varying illumination without using any lighting capturing devices or HDR information. The recovered illumination can produce realistic rendering results. To model the geometry of the visible and invisible parts of scenes corresponding to the input RGB-D image, we assume that all objects shown in the image are located in a box with six faces and build a planar-based geometry model based on the input depth map. We then present a confidence-scoring based strategy to separate the light sources from the highlight areas. The positions of light sources both in and out of the camera's view are calculated based on the classification result and the recovered geometry model. Finally, an iterative procedure is proposed to calculate the colors of light sources and the materials in the scene. In addition, a data-driven method is used to set constraints on the light source intensities. Using the estimated light sources and geometry model, environment maps at different points in the scene are generated that can model the spatial variance of illumination. The experimental results demonstrate the validity and flexibility of our approach.","1941-0506","","10.1109/TVCG.2018.2876541","National Natural Science Foundation of China; Open Project Program of the Science and Technology on Optical Radiation Laboratory; National Key Research and Development Program of China Stem Cell and Translational Research; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8511066","Illumination recovery;automatic;indoor scenes;single RGB-D image","Lighting;Light sources;Geometry;Cameras;Three-dimensional displays;Dynamic range;Probes","cameras;geometry;image classification;image colour analysis;iterative methods;lighting","iterative procedure;illumination recovery;spatial variance;light source intensities;classification result;confidence-scoring based strategy;input depth map;planar-based geometry model;RGB-D image;indoor scenes","","","","33","IEEE","26 Oct 2018","","","IEEE","IEEE Journals"
"HyCLASSS: A Hybrid Classifier for Automatic Sleep Stage Scoring","X. Li; L. Cui; S. Tao; J. Chen; X. Zhang; G. -Q. Zhang","Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, OH, USA; Department of Computer Science, University of Kentucky, Lexington, KY, USA; Institute for Biomedical Informatics, University of Kentucky, Lexington, KY, USA; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; College of Information Sciences and Technology, Pennsylvania State University, University Park, PA, USA; Institute for Biomedical Informatics, University of Kentucky, Lexington, KY, USA","IEEE Journal of Biomedical and Health Informatics","6 Mar 2018","2018","22","2","375","385","Automatic identification of sleep stage is an important step in a sleep study. In this paper, we propose a hybrid automatic sleep stage scoring approach, named HyCLASSS, based on single channel electroencephalogram (EEG). HyCLASSS, for the first time, leverages both signal and stage transition features of human sleep for automatic identification of sleep stages. HyCLASSS consists of two parts: A random forest classifier and correction rules. Random forest classifier is trained using 30 EEG signal features, including temporal, frequency, and nonlinear features. The correction rules are constructed based on stage transition feature, importing the continuity property of sleep, and characteristic of sleep stage transition. Compared with the gold standard of manual scoring using Rechtschaffen and Kales criterion, the overall accuracy and kappa coefficient applied on 198 subjects has reached 85.95% and 0.8046 in our experiment, respectively. The performance of HyCLASS compared favorably to previous work, and it could be integrated with sleep evaluation or sleep diagnosis system in the future.","2168-2208","","10.1109/JBHI.2017.2668993","University of Kentucky; Center for Clinical and Translational Science, University of Illinois at Chicago; National Sleep Research Resource; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858702","Automatic sleep stage scoring;EEG;hybrid classifier;PSG;sleep stage transition","Sleep;Electroencephalography;Feature extraction;Decision trees;Standards;Training data;Manuals","bioelectric potentials;electroencephalography;feature extraction;medical signal detection;medical signal processing;neurophysiology;signal classification;sleep","EEG signal feature extraction;nonlinear feature extraction;hybrid automatic sleep stage scoring approach;random forest classifier;automatic identification;single channel electroencephalogram","Adolescent;Adult;Algorithms;Electroencephalography;Female;Humans;Male;Polysomnography;Signal Processing, Computer-Assisted;Sleep Stages;Young Adult","12","","62","","17 Feb 2017","","","IEEE","IEEE Journals"
"Hand and Lip Desynchronization Analysis in French Cued Speech: Automatic Temporal Segmentation of Hand Flow","N. Aboutabit; D. Beautemps; L. Besacier","Institut de la Communication Parlee, CNRS UMR5009/INPG/Universite Stendhal, 46 A venue Felix Viallet, 38031 Grenoble, Cedex 1, France. Noureddine.aboutabit@icp.inpg.fr; Institut de la Commun. Parlee, Stendhal Univ., Grenoble; Institut de la Commun. Parlee, Stendhal Univ., Grenoble","2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings","24 Jul 2006","2006","1","","I","I","In the context of cued speech gesture phonetic translation, the automatic recognition of lip and hand movements is a key factor. The hand and the lip parameters are not synchronized, thus the fusion of the two channels (hand and lips) needs the knowledge of the desynchronized delay. This contribution focuses on the presentation of an automatic algorithm for temporal segmentation of the hand cue information based on Gaussian modeling of the hand position and minimum of velocity. The segmentation delivers the beginning of the hand transition and the instant of attained position. The hand segmentation is used to calculate the delay between hand and lip targets, in relation with the corresponding acoustic realization in the case of French CV syllables extracted from a corpus of phrases uttered and coded by a cued speech speaker. This study confirms in a more complex context the importance of the instant of attained hand position as pointed out by Attina and colleagues, in terms of control and for the fusion process","2379-190X","1-4244-0469-X","10.1109/ICASSP.2006.1660100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1660100","","Speech analysis","Gaussian processes;natural languages;speech coding;speech recognition;synchronisation","lip desynchronization analysis;hand desynchronization analysis;French cued speech;automatic temporal segmentation;hand flow;cued speech gesture phonetic translation;Gaussian modeling;cued speech speaker","","5","","8","","24 Jul 2006","","","IEEE","IEEE Conferences"
"ERS SAR and SPOT images automatic registration in a multichannel consensual segmentation scheme","O. Thepaut; K. Kpalma; J. Ronsin","Lab. ARTIST, Inst. Nat. des Sci. Appliques, Rennes, France; NA; NA","IGARSS '98. Sensing and Managing the Environment. 1998 IEEE International Geoscience and Remote Sensing. Symposium Proceedings. (Cat. No.98CH36174)","6 Aug 2002","1998","2","","1040","1042 vol.2","This paper deals with an automatic registration method of multidate and multisensor remote sensing images as preprocessing in a multichannel consensual segmentation scheme. This two step method consists of a geometrical correction followed by a residual translation compensation performed by detecting the crosscorrelation maximum between multidate images. This second step is preceded by an edge detection for prefiltered ERS and SPOT multisensor images.","","0-7803-4403-0","10.1109/IGARSS.1998.699666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=699666","","Image segmentation;Geometry;Remote sensing;Heart rate variability;Automatic control;Earth;Interpolation;Image edge detection;Instruments;Microwave generation","geophysical techniques;geophysical signal processing;remote sensing;remote sensing by radar;synthetic aperture radar;spaceborne radar;radar imaging;image registration;sensor fusion;image segmentation;image sequences","geophysical measurement technique;land surface;terrain mapping;radar imaging;optical imaging;radar remote sensing;spaceborne radar;satellite remote sensing;image fusion;multitemporal images;image sequence;image registration;SAR;synthetic aperture radar;ERS;SPOT;automatic registration;multichannel consensual segmentation scheme;preprocessing;geometrical correction;residual translation compensation;cross correlation maximum;edge detection;multisensor image","","4","","4","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic detection of relevant head gestures in American Sign Language communication","U. M. Erdem; S. Sclaroff","Dept. of Comput. Sci., Boston Univ., MA, USA; Dept. of Comput. Sci., Boston Univ., MA, USA","Object recognition supported by user interaction for service robots","10 Dec 2002","2002","1","","460","463 vol.1","An automated system for detection of head movements is described The goal is to label relevant head gestures in video of American Sign Language (ASL) communication. In the system, a 3D head tracker recovers head rotation and translation parameters from monocular video. Relevant head gestures are then detected by analyzing the length and frequency of the motion signal's peaks and valleys. Each parameter is analyzed independently, due to the fact that a number of relevant head movements in ASL are associated with major changes around one rotational axis. No explicit training of the system is necessary Currently, the system can detect ""head shakes."" In experimental evaluation, classification performance is compared against ground-truth labels obtained from ASL linguists. Initial results are promising, as the system matches the linguists' labels in a significant number of cases.","1051-4651","0-7695-1695-X","10.1109/ICPR.2002.1044759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1044759","","Handicapped aids;Head;Humans;Video sequences;Computer science;Signal analysis;Frequency;Indexing;Production;Databases","image classification;gesture recognition;image motion analysis;image segmentation;parameter estimation;image sequences","automatic detection;American Sign Language communication;relevant head gestures;computer human interaction;gesture classification;visual motion;image indexing;video indexing;3D head tracker;head rotation;head translation;monocular video;head shakes;ground-truth labels","","17","","8","","10 Dec 2002","","","IEEE","IEEE Conferences"
"Automatic correction of motion artifacts in magnetic resonance images using an entropy focus criterion","D. Atkinson; D. L. G. Hill; P. N. R. Stoyle; P. E. Summers; S. F. Keevil","Radiol. Sci., Guys Hosp., London, UK; NA; NA; NA; NA","IEEE Transactions on Medical Imaging","6 Aug 2002","1997","16","6","903","910","Presents the use of an entropy focus criterion to enable automatic focusing of motion corrupted magnetic resonance images. The authors demonstrate the principle using illustrative examples from cooperative volunteers. Their technique can determine unknown patient motion or use knowledge of motion from other measures as a starting estimate. The motion estimate is used to compensate the acquired data and is iteratively refined using the image entropy. Entropy focuses the whole image principally by favoring the removal of motion induced ghosts and blurring from otherwise dark regions of the image. Using only the image data, and no special hardware or pulse sequences, the authors demonstrate correction for arbitrary rigid-body translational motion in the imaging plane and for a single rotation. Extension to three-dimensional (3-D) and more general motion should be possible. The algorithm is able to determine volunteer motion well. The mean absolute deviation between algorithm and navigator-echo-determined motion is comparable to the displacement step size used in the algorithm. Local deviations from the recorded motion or navigator-determined motion are explained and the authors indicate how enhanced focus criteria may be derived. In all cases they were able to compensate images for patient motion, reducing blurring and ghosting.","1558-254X","","10.1109/42.650886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=650886","","Magnetic resonance;Entropy;Focusing;Navigation;Hardware;Hospitals;Motion estimation;Magnetic resonance imaging;Image resolution;Image processing","entropy;medical image processing;biomedical NMR;biomechanics","motion artifacts automatic correction;magnetic resonance images;entropy focus criterion;unknown patient motion;image entropy;motion induced ghosts removal;blurring;arbitrary rigid-body translational motion;volunteer motion;navigator-echo-determined motion;image compensation;medical diagnostic imaging","Artifacts;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Motion","158","14","22","","6 Aug 2002","","","IEEE","IEEE Journals"
"Frequency-Warping Invariant Features for Automatic Speech Recognition","A. Mertins; J. Rademacher","Signal Processing Group, University of Oldenburg, Department of Physics, 26111 Oldenburg, Germany. Email: alfred.mertins@uni-oldenburg.de; Dept. of Phys., Oldenburg Univ.","2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings","24 Jul 2006","2006","5","","V","V","Based on the well-known relationship between vocal tract length (VTL) variation and linear frequency warping, we present a method for generating vocal tract length invariant (VTLI) features. These features are computed as translation invariant, correlation-type features in a log-frequency domain. In phoneme classification and recognition experiments on the TIMIT database, their discrimination capabilities and robustness to mismatches between training and test conditions turned out to be considerably better than for Mel-frequency cepstral coefficients (MFCCs). The best results are obtained when VTLI features and MFCCs are combined","2379-190X","1-4244-0469-X","10.1109/ICASSP.2006.1661453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1661453","","Frequency;Automatic speech recognition;Wavelet transforms;Continuous wavelet transforms;Discrete wavelet transforms;Fourier transforms;Robustness;Testing;Hidden Markov models;Bandwidth","cepstral analysis;correlation methods;frequency-domain analysis;signal classification;speech recognition;wavelet transforms","frequency-warping invariant features;automatic speech recognition;vocal tract length variation;linear frequency warping;translation invariant;correlation-type features;log-frequency domain;phoneme classification;recognition experiments;TIMIT database;discrimination capabilities;Mel-frequency cepstral coefficients","","8","","11","","24 Jul 2006","","","IEEE","IEEE Conferences"
"Automatic classification of shoeprints for use in forensic science based on the Fourier transform","C. Huynh; P. de Chazal; D. McErlean; R. B. Reilly; T. J. Hannigan; L. M. Fleury","Dept. of Electron. & Electr. Eng., Univ. Coll. Dublin, Ireland; Dept. of Electron. & Electr. Eng., Univ. Coll. Dublin, Ireland; Dept. of Electron. & Electr. Eng., Univ. Coll. Dublin, Ireland; Dept. of Electron. & Electr. Eng., Univ. Coll. Dublin, Ireland; NA; NA","Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429)","24 Nov 2003","2003","3","","III","569","This study developed a system of automatic classification of shoeprint images into groups belonging to the same sole pattern. When presented with an image of a new shoeprint the system displays a ranked sequence of shoeprint images from the database. The shoeprint images are ranked from best match to worst match in terms of the pattern of the shoeprint. For this study a database of 503 shoeprint images belonging to 139 pattern groups was established with each group containing 2 or more examples. The pattern grouping was performed by a panel of human experts. This designed system is a fully automatic method and functions with minimum user intervention. Tests of the system have shown that the first shoeprint image displayed is a correct match 54% of the time and that a correct match appears within the first 5% of displayed shoeprints 75% of the time. The system has translational and rotational invariance so that the spatial positioning of the new shoeprint images does not have to correspond with the spatial positioning of the shoeprint images of the database.","1522-4880","0-7803-7750-8","10.1109/ICIP.2003.1247308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1247308","","Forensics;Fourier transforms;Footwear;Image databases;Spatial databases;Layout;Pattern matching;Laboratories;Humans;Signal processing","image classification;image matching;Fourier transforms;image sequences;visual databases","automatic classification;forensic science;Fourier transform;shoeprint images;sole pattern;pattern grouping;rotational invariance;translational invariance;spatial positioning","","7","","7","","24 Nov 2003","","","IEEE","IEEE Conferences"
"An automatic prosody recognizer using a coupled multi-stream acoustic model and a syntactic-prosodic language model","S. Ananthakrishnan; S. S. Narayanan","Dept. of Electr. Eng., Univ. of Southern California, Los Angeles, CA, USA; Dept. of Electr. Eng., Univ. of Southern California, Los Angeles, CA, USA","Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.","9 May 2005","2005","1","","I/269","I/272 Vol. 1","Automatic detection and labeling of prosodic events in speech has received much attention from speech technologists and linguists ever since the introduction of annotation standards such as ToBI. Since prosody is intricately bound to the semantics of the utterance, recognition of prosodic events is important for spoken language applications such as automatic understanding and translation of speech. Moreover, corpora labeled with prosodic markers are essential for building speech synthesizers that use data-driven approaches to generate natural speech. In this paper, we build a prosody recognition system that detects stress and prosodic boundaries at the word and syllable level in American English using a coupled hidden Markov model (CHMM) to model multiple, asynchronous acoustic feature streams and a syntactic-prosodic model that captures the relationship between the syntax of the utterance and its prosodic structure. Experiments show that the recognizer achieves about 75% agreement on stress labeling and 88% agreement on boundary labeling at the syllable level.","2379-190X","0-7803-8874-7","10.1109/ICASSP.2005.1415102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1415102","","Labeling;Natural languages;Automatic speech recognition;Speech analysis;Acoustic signal detection;Stress;Hidden Markov models;Event detection;Speech synthesis;Synthesizers","hidden Markov models;speech recognition;natural languages","coupled hidden Markov model;automatic prosody recognizer;coupled multiple-stream acoustic model;syntactic-prosodic language model;prosodic event labeling;annotation;utterance semantics;spoken language processing;stress labeling;boundary labeling;speech translation;speech comprehension;data-driven speech synthesizers;word stress detection;syllable level prosodic boundaries;CHMM;asynchronous acoustic feature streams","","23","1","10","","9 May 2005","","","IEEE","IEEE Conferences"
"Towards automatic building extraction: Variational level set model using prior shape knowledge","J. Yang; Y. Wang","Northwest Institute of Nuclear Technology, Xi'an, China; Northwest Institute of Nuclear Technology, Xi'an, China","2012 International Conference on Image Analysis and Signal Processing","31 Jan 2013","2012","","","1","6","A novel variational level set model for multiple-building extraction from a single remote image is proposed in this paper. Multi-competing shapes are considered together with the level set model, the curve evolution is constrained by the prior shape knowledge and the label function that dynamically indicates the region with which the prior shape should be compared. The building extraction is addressed through a level set image segmentation approach that involves the use of the label function, as well as the prior shape knowledge. In addition, the proposed model permits translation, scaling, and rotation of the prior shape. Experimental results and the qualitative and quantitative evaluations demonstrate the potential of the approach.","2156-0129","978-1-4673-2546-2","10.1109/IASP.2012.6424990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424990","level sets;prior shape knowledge;label;function;building detection;segmentation;variational;method","Shape;Level set;Buildings;Image segmentation;Noise;Remote sensing;Mathematical model","feature extraction;geophysical image processing;image segmentation;remote sensing;set theory;shape recognition","multiple-building extraction;variational level set model;prior shape knowledge;remote image;curve evolution;label function;level set image segmentation approach;prior shape translation;prior shape scaling;prior shape rotation;qualitative evaluation;quantitative evaluation;remote sensing image","","3","","16","","31 Jan 2013","","","IEEE","IEEE Conferences"
"A hybrid approach for Discourse Segment Detection in the automatic subtitle generation of computer science lecture videos","R. Sridhar; S. Aravind; H. Muneerulhudhakalvathi; M. Sibi Senthur","Department of Computer Science and Engineering, College of Engineering Guindy, Anna University, Chennai, India; Department of Computer Science and Engineering, College of Engineering Guindy, Anna University, Chennai, India; Department of Computer Science and Engineering, College of Engineering Guindy, Anna University, Chennai, India; Department of Computer Science and Engineering, College of Engineering Guindy, Anna University, Chennai, India","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","1 Dec 2014","2014","","","284","287","The aim of this paper is to develop an automatic subtitle generation system for computer science lecture videos. CMU Sphinx Speech API is used to accomplish speech recognition. The main challenge of this work, is to align the translated text with the video. Discourse Segment Detection (DSD) is the process of analyzing and identifying discourse boundaries in human speech. Discourse Segment Detection (DSD) is carried out that classifies word boundaries and groups words until a discourse break occurs. The approach that has been devised in this paper for DSD to identify word boundary is a hybrid approach combining acoustic and linguistic features from the speech. This helps to segment the text obtained from Speech Engine, group words that can be written to the subtitles file without violating the subtitle standards. The devised approach has shown an improved performance than the existing approach as the error has reduced from 30% to 18 %.","","978-1-4799-3080-7","10.1109/ICACCI.2014.6968422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968422","Discourse Segment Detection;Subtitles","Videos;Speech;Acoustics;Pragmatics;Automatic speech recognition;Computer science","computer aided instruction;computer science education;speech recognition;speech synthesis;video signal processing","hybrid discourse segment detection approach;automatic subtitle generation;computer science lecture videos;CMU Sphinx speech API;speech recognition;translated text;DSD;speech engine","","","","6","","1 Dec 2014","","","IEEE","IEEE Conferences"
"Automatic target recognition in laser radar imagery","M. Snorrason; H. Ruda; A. Caglayan","Charles River Anal., Cambridge, MA, USA; Charles River Anal., Cambridge, MA, USA; Charles River Anal., Cambridge, MA, USA","1995 International Conference on Acoustics, Speech, and Signal Processing","6 Aug 2002","1995","4","","2471","2474 vol.4","This paper presents an automatic target recognition (ATR) system for laser radar (LADAR) imagery, designed to classify objects at multiple levels of discrimination (target detection, classification, and recognition) from single LADAR images. Segmentation is performed in both the range and non-range LADAR channels and results combined to increase object detection rate or decrease false positive detection rate. Through use of the range data, object subimages are projected and rotated to canonical orientations, providing invariance to translation, scale and rotations in 3-D. Global features are extracted for rapid target detection and local receptive field features are computed for target recognition 100% detection and recognition rates are shown for a small set of real LADAR data.","1520-6149","0-7803-2431-5","10.1109/ICASSP.1995.480049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=480049","","Laser radar;Target recognition;Radar imaging;Object detection;Optical design;Image recognition;Image segmentation;Radar detection;Feature extraction;Data mining","optical radar;radar target recognition;radar detection;radar imaging;image classification;image segmentation;laser ranging;feature extraction","laser radar imagery;automatic target recognition;LADAR imagery;target detection;target classification;target recognition;image segmentation;LADAR channels;object detection rate;false positive detection rate;range data;object subimages;canonical orientations;translation invariance;scale invariance;rotation invariance;global features;feature extraction;local receptive field features;recognition rates","","1","","6","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic target recognition using higher order neural network","Liqing Wan; Longhe Sun","Electro-Opt. Equipment Res. Inst., Henan, China; Electro-Opt. Equipment Res. Inst., Henan, China","Proceedings of the IEEE 1996 National Aerospace and Electronics Conference NAECON 1996","6 Aug 2002","1996","1","","221","226 vol.1","Translational rotational scaling invariant (TRSI) pattern recognition is an important problem in the automatic target recognition (ATR) field. Recent research has shown that the higher order neural networks (HONN) have numerous advantages over other neural network approaches in respect of the object recognition with invariant of the object's position size, and in-plane rotation. The major limitation of HONNs is that the number of connected weights is too large to store on most machines. For N/spl times/N image, the memory needed to store the connections is proportional to N/sup 6/. This huge memory requirement limits the HONN's application to large scale images. In this paper, we have developed an integrated method which combines the bi-directional log-polar mapping and HONN pattern recognizer. It reduces the HONN memory requirement from O(N/sup 6/) to O(N/sup 2/). The proposed method has been successfully verified. Finally, the results are compared with those of coarse-coding method, traditional log-polar method.","0547-3578","0-7803-3306-3","10.1109/NAECON.1996.517646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=517646","","Target recognition;Neural networks;Pattern recognition;Layout;Image recognition;Object recognition;Image analysis;Nonlinear distortion;Sun;Large-scale systems","object recognition;multilayer perceptrons;neural net architecture;image classification;learning (artificial intelligence)","automatic target recognition;higher order neural network;translational rotational scaling invariant;pattern recognition;integrated method;bi-directional log-polar mapping;reduced memory requirement;geometric invariance;perceptron training rule;combinational explosion problem;object recognition;third-order network;pattern classification;neural network architecture;aircraft recognition","","1","","3","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Hidden Markov modelling for SAR automatic target recognition","C. Nilubol; Q. H. Pham; R. M. Mersereau; M. J. T. Smith; M. A. Clements","Center for Signal & Image Process., Georgia Inst. of Technol., Atlanta, GA, USA; NA; NA; NA; NA","Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)","6 Aug 2002","1998","2","","1061","1064 vol.2","This paper discusses the application of hidden Markov models (HMMs) to solve the translational and rotational invariant automatic target recognition (TRIATR) problem associated with SAR imagery. This approach is based on a cascade of these stages: preprocessing, feature extraction and selection, and classification. Preprocessing and feature extraction and selection involve successive applications of extraction operations from measurements of the Radon transform of target chips. The features which are invariant to changes in rotation, position and shifts, although not to changes in scale are optimized through the use of feature selection techniques. The classification stage successively takes as its inputs the multidimensional multiple observation sequences, parameterizes them statistically using continuous density models to capture target and background appearance variability, and thus results in the TRIATR-HMMs. Experimental results have demonstrated that the recognition rate is as high as 99% over both the training set and the testing set.","1520-6149","0-7803-4428-6","10.1109/ICASSP.1998.675451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=675451","","Hidden Markov models;Target recognition;Discrete Fourier transforms;Speech recognition;Signal processing;Image processing;Application software;Feature extraction;Semiconductor device measurement;Multidimensional systems","synthetic aperture radar;radar imaging;radar target recognition;hidden Markov models;feature extraction;image classification;Radon transforms","hidden Markov modelling;SAR automatic target recognition;hidden Markov models;rotational invariant automatic target recognition;SAR imagery;translational invariant automatic target recognition;preprocessing;feature extraction;feature selection;classification;measurements;Radon transform;target chips;position;shifts;multidimensional multiple observation sequences;continuous density models;background appearance variability;target appearance variability;experimental results;TRIATR-HMM;recognition rate;training set;testing set","","7","1","5","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic recalibration of the camera pose parameters in a vision system","B. Zhu","School of Information Engineering Nanjing University of Financial and Economics, Nanjing, China","2010 3rd International Congress on Image and Signal Processing","29 Nov 2010","2010","1","","400","403","In this work, we present a robust method for automatic recalibration camera pose via homography. And assuming that the intrinsic parameters of the camera are known from initial calibration, then we show that its translation vector and rotation matrix via Homography. In this method, computational cost is low compared with traditional methods. Some simulations and real data experiments show the accuracy and robustness of the proposed method to be improved over those of the standard method.","","978-1-4244-6516-3","10.1109/CISP.2010.5648006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648006","Recalibration;Camera Pose;Homographic matrix","Transmission line matrix methods;Cameras;Eigenvalues and eigenfunctions;Calibration;Equations;Machine vision;Symmetric matrices","computer vision;matrix algebra;vectors","vision system;camera pose parameter;automatic recalibration;homography;translation vector;rotation matrix","","","","13","","29 Nov 2010","","","IEEE","IEEE Conferences"
"Application of Conditional Adversarial Networks for Automatic Generation of MR-based Attenuation Map in PET/MR","L. Tao; X. Li; J. Fisher; C. S. Levin","Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, 94305, USA; Center for Gamma-Ray Imaging, University of Arizona, Tucson, AZ, 85721, USA; Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, 94305, USA; Molecular Imaging Instrumentation Laboratory, Stanford University, Stanford, CA, 94305, USA","2018 IEEE Nuclear Science Symposium and Medical Imaging Conference Proceedings (NSS/MIC)","5 Sep 2019","2018","","","1","3","Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients for attenuation correction in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this work, we study deep learning techniques to automatically generate attenuation maps directly from MR images, with focus on the head and neck areas. We use a generative adversarial network (GAN) in a conditional setting for this image translation task. GANs separate the deep learning network into a generator, which tries to generate fake examples, and a discriminator, which learns to distinguish between real and fake examples, and train the two networks simultaneously. The objective function of the conditional GAN is a combination of the generator loss, discriminator loss, and the L1 distance between the label image and the output image from the generator. Image pairs of PET/MR image (input) and corresponding PET/CT based 511 keV photon attenuation map (label) are used to train the network. The network is trained for 6k iterations. The generator loss is trained from 2.0 to 1.4. The discriminator loss is trained from 0.6 to 1.0. The L1 loss is trained from 0.2 to 0.1. In our previous work with a basic autoencoder network for the conversion from MR images to corresponding attenuation maps, if we convert the defined L2 loss for the network to the RMS pixel value prediction error, the autoencoder network, after training, has a pixel prediction error of around 0.2 when all pixel values are scaled from 0 to 1. In this study, the L1 loss also represents this pixel prediction error, which is around 0.1 after the network is trained. This indicates an average reduction of 50% of the pixel prediction error.","2577-0829","978-1-5386-8494-8","10.1109/NSSMIC.2018.8824444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824444","PET/MR;MR-based attenuation map;attenuation correction;deep learning;generative adversarial network (GAN);conditional GAN","Generators;Attenuation;Training;Gallium nitride;Generative adversarial networks;Bones;Head","biomedical MRI;bone;computerised tomography;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography","PET/CT based photon attenuation map;bone delineation;photon attenuation map;MR-based attenuation map;head and neck areas;PET image reconstruction;empirical attenuation coefficients;MR image segmentation;conditional adversarial networks;pixel prediction error;basic autoencoder network;image pairs;discriminator loss;generator loss;conditional GAN;fake examples;deep learning network;image translation task;generative adversarial network;electron volt energy 511.0 keV","","1","","5","","5 Sep 2019","","","IEEE","IEEE Conferences"
"A fully automatic calibration procedure for freehand 3D ultrasound","F. Rousseau; P. Hellier; C. Barillot","IRISA, Rennes I Univ., France; IRISA, Rennes I Univ., France; IRISA, Rennes I Univ., France","Proceedings IEEE International Symposium on Biomedical Imaging","7 Nov 2002","2002","","","985","988","Describes a novel method for calibration of freehand three-dimensional (3D) ultrasound. A position sensor is mounted on a conventional ultrasound probe, thus the set of B-scans can be localized in 3D, and can be compounded into a volume. The calibration process aims at determining the transformation (translations, rotations, scaling) between the coordinates system of images and the coordinate system of the localization system. In our study, the phantom used to calibrate the 3D ultrasound system is a plane. It provides in each image a strong, straight line. The calibration process is based on the set of lines in 2D images forming a plane in 3D. Points of interest are extracted from the ultrasound sequence. The eight parameters of the transformation are determined with an iterative algorithm which is based on the principle that correct registration between the plane and the points of interest provides correct calibration. Validation of this method has been performed on synthetic sequences. This calibration method is shown to be easy to perform, completely automatic and fast enough for clinical use.","","0-7803-7584-X","10.1109/ISBI.2002.1029428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029428","","Calibration;Ultrasonic imaging;Probes;Image reconstruction;Imaging phantoms;Mechanical systems;Phased arrays;Surface reconstruction;Iterative algorithms;Biomedical imaging","biomedical ultrasonics;calibration;biomedical transducers;ultrasonic transducers;phantoms;image sequences;medical image processing;iterative methods;image registration;image reconstruction","freehand 3D ultrasound;fully automatic calibration procedure;free-hand three-dimensional ultrasound;position sensor;conventional ultrasound probe;B-scans;volume;calibration process;transformation;translations;rotations;scaling;coordinates system of images;localization system;phantom;plane;strong straight line;set of lines;2D images;ultrasound sequence;iterative algorithm;correct registration;points of interest;synthetic sequences;clinical use","","9","1","8","","7 Nov 2002","","","IEEE","IEEE Conferences"
"An intelligent system for automatic fire detection in forests","V. Cappellini; L. Mattii; A. Mecocci","Florence Univ., Italy; Florence Univ., Italy; Florence Univ., Italy","Third International Conference on Image Processing and its Applications, 1989.","6 Aug 2002","1989","","","563","570","The use of smoke-analysis is described for fire detection by means of suitably placed standard TV cameras. The paper first gives the smoke detection technique and discusses a method to obtain a suitable description of the scene. Then the intelligent part of the system is described and the methods and heuristics to speed-up the decision process discussed. The sub-system devoted to the translation of the sensor data into a form suitable for logic manipulation is then presented. Finally some results and future trends are reported.<>","","","10.1007/3-540-51815-0_67","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=132192","","Machine vision;Detectors;Fires;Forestry;Knowledge based systems","computer vision;detectors;fires;forestry;knowledge based systems","intelligent system;automatic fire detection;forests;smoke-analysis;standard TV cameras;smoke detection;heuristics;decision process;sub-system;translation;sensor data;logic manipulation;future trends","","10","","","","6 Aug 2002","","","IET","IET Conferences"
"A preliminary study on topical model for multi-domain speech recognition via word embedding vector","J. Moon; S. Yun; D. Lee; S. Kim","ICT-Computer Software, University of Science and Technology; Speech Intelligence Research Group, Electronics and Telecommunications Research Institute, Daejeon, Republic of Korea; ICT-Computer Software, University of Science and Technology; Speech Intelligence Research Group, Electronics and Telecommunications Research Institute, Daejeon, Republic of Korea","2019 34th International Technical Conference on Circuits/Systems, Computers and Communications (ITC-CSCC)","12 Aug 2019","2019","","","1","4","In this paper, we suggest a basic topical model(TM) framework to adapt speech recognition system to multi-domain and prevent topical errors. This paper employs the cosine similarities between target and context words at a spoken utterance as the topical model parameters. The TM is applied to frames having a large number of candidate words at lattice network, and it adjusts the ranking of candidate words by adding it to total cost estimated from acoustic model(AM) and language model(LM). To cover multidomain, the word embedding was trained with 5.5 billion text corpus from multi-domain. As an acoustic model and a language model, DNN-HMM and N - gram were selected. 501 sentences (10,054 words) includes 35 topics were used as an evaluation data set. As a result, the best performances were obtained by our approach, and the performance of WERR was increased up to about 4% compared with N-gram based model. The WERR increased above 10% when the word errors were correctly detected. The results show this suggestion has a possibility of adapting a model to multi-domains without sub-topic models.","","978-1-7281-3271-6","10.1109/ITC-CSCC.2019.8793299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793299","Artificial Intelligence;Deep Learning;Automatic Speech Translation;Automatic Speech Recognition;Natural Language Processing;Language Model","Adaptation models;Hidden Markov models;Lattices;Acoustics;Data models;Automatic speech recognition","hidden Markov models;natural language processing;neural nets;speech recognition;text analysis;vectors;word processing","multidomain speech recognition;speech recognition system;cosine similarities;context words;spoken utterance;topical model parameters;lattice network;acoustic model;N-gram based model;word embedding vector","","","","19","","12 Aug 2019","","","IEEE","IEEE Conferences"
"German Sign Language Translation using 3D Hand Pose Estimation and Deep Learning","S. Mohanty; S. Prasad; T. Sinha; B. N. Krupa","PES University,Dept. of Electronics and Communication Engineering,Bengaluru,India; PES University,Dept. of Electronics and Communication Engineering,Bengaluru,India; PES University,Dept. of Electronics and Communication Engineering,Bengaluru,India; PES University,Dept. of Electronics and Communication Engineering,Bengaluru,India","2020 IEEE REGION 10 CONFERENCE (TENCON)","22 Dec 2020","2020","","","773","778","Sign language is the primary medium of communication for the majority of the world's population suffering from disabling hearing loss that creates a barrier between the hearing and the hearing-impaired people. In this paper, sign language translation is undertaken for German Sign Language (GSL) characters from a single image by leveraging the technique of 3D object detection. We make use of a three-network architecture that performs segmentation, keypoint localization, and elevation from a two-dimensional plane to the three-dimensional space, from a single RGB image containing the signed gesture. Thirty gestures have been used and the best results were obtained using a combination of pose representation coordinates, joint angles, and pool layer features of AlexNet for classification. The system gives a character error rate of 0.29, a reduction of error rate by 12.12% when compared to the state-of-the-art approach.","2159-3450","978-1-7281-8455-5","10.1109/TENCON50793.2020.9293763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9293763","Keypoints;Scoremaps;Support Vector Machine;AlexNet;Inception-v3;Pose Representation;Neural Network;Transfer Learning","Three-dimensional displays;Assistive technology;Gesture recognition;Two dimensional displays;Solid modeling;Network architecture;Training","deep learning (artificial intelligence);image classification;image colour analysis;image segmentation;object detection;pose estimation;sign language recognition;stereo image processing","single RGB image;signed gesture;character error rate;3D hand pose estimation;deep learning;disabling hearing loss;hearing-impaired people;3D object detection;three-network architecture;two-dimensional plane;three-dimensional space;German sign language characters;German sign language translation;keypoint localization;pose representation coordinates;joint angles;AlexNet pool layer features","","","","17","","22 Dec 2020","","","IEEE","IEEE Conferences"
"Novel methodology for Kannada Braille to speech translation using image processing on FPGA","S. R. Rupanagudi; S. Huddar; V. G. Bhat; S. S. Patil; Bhaskar M. K.","WorldServe Education, Bangalore, India; WorldServe Education, Bangalore, India; WorldServe Education, Bangalore, India; WorldServe Education, Bangalore, India; Department of Telecommunication, Atria Institute of Technology, Bangalore, India","2014 International Conference on Advances in Electrical Engineering (ICAEE)","19 Jun 2014","2014","","","1","6","With the introduction and popularization of text to speech convertors, a huge drop in literacy rates is being seen amongst the visually impaired. Also, since Braille is not well known to the masses, communication by the visually impaired with the outside world becomes an arduous task. A lot of research is being carried out in conversion of English text to Braille but not many concentrate on the alternative i.e. conversion of Braille to regional languages. In order to address this issue, in this paper we introduce a novel methodology to convert Braille characters representing the Kannada Language (a popular language of southern part of India), captured by a camera, into Kannada text or speech. An automated thresholding algorithm for segmentation of the Braille dots along with a novel algorithm for identification of the characters has been explained. All algorithms were designed and developed for a Xilinx Spartan 3E FPGA and were executed in real time. An accuracy of over 94% was achieved in Braille segmentation and detection. The algorithm for identification of the Kannada Braille character was found to be four times faster than many existing methodologies, on the FPGA.","","978-1-4799-3543-7","10.1109/ICAEE.2014.6838445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6838445","Automatic thresholding;Bharthi Braille;Braille to speech;FPGA;Histogram;Image processing;Kannada Braille;Visually impaired","Field programmable gate arrays;Speech;Lighting;Image segmentation;Histograms;Speech processing","field programmable gate arrays;image segmentation;speech synthesis","Kannada-Braille-to-speech translation;image processing;text-to-speech convertors;literacy rates;visually impaired people;regional languages;Kannada language;south India;cameras;Kannada text;Kannada speech;automated thresholding algorithm;Braille dot segmentation;Xilinx Spartan 3E FPGA;Braille segmentation;Kannada Braille character identification","","11","","24","","19 Jun 2014","","","IEEE","IEEE Conferences"
"Prediction of Low-Kev Monochromatic Images From Polyenergetic CT Scans For Improved Automatic Detection of Pulmonary Embolism","C. Seibold; M. A. Fink; C. Goos; H. -U. Kauczor; H. -P. Schlemmer; R. Stiefelhagen; J. Kleesiek","Karlsruhe Institute of Technology,Institute of Anthropomatics & Robotics,Germany; University Hospital Heidelberg,Department of Diagnostic and Interventional Radiology,Germany; Karlsruhe Institute of Technology,Institute of Anthropomatics & Robotics,Germany; University Hospital Heidelberg,Department of Diagnostic and Interventional Radiology,Germany; German Cancer Research Center,Heidelberg,Germany; Karlsruhe Institute of Technology,Institute of Anthropomatics & Robotics,Germany; German Cancer Research Center,Heidelberg,Germany","2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)","25 May 2021","2021","","","1017","1020","Detector-based spectral computed tomography is a recent dual-energy CT (DECT) technology that offers the possibility of obtaining spectral information. From this spectral data, different types of images can be derived, amongst others virtual monoenergetic (monoE) images. MonoE images potentially exhibit decreased artifacts, improve contrast, and overall contain lower noise values, making them ideal candidates for better delineation and thus improved diagnostic accuracy of vascular abnormalities.In this paper, we are training convolutional neural networks (CNN) that can emulate the generation of monoE images from conventional single energy CT acquisitions. For this task, we investigate several commonly used image-translation methods. We demonstrate that these methods while creating visually similar outputs, lead to a poorer performance when used for automatic classification of pulmonary embolism (PE). We expand on these methods through the use of a multi-task optimization approach, under which the networks achieve improved classification as well as generation results, as reflected by PSNR and SSIM scores. Further, evaluating our proposed framework on a subset of the RSNAPE challenge data set shows that we are able to improve the Area under the Receiver Operating Characteristic curve (AuROC) in comparison to a naïve classification approach from 0.8142 to 0.8420.","1945-8452","978-1-6654-1246-9","10.1109/ISBI48211.2021.9433966","Helmholtz Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433966","Image-to-Image Translation;Spectral Computer Tomography;Domain Adaptation;Pulmonary Embolism Diagnosis","Training;Computed tomography;Pulmonary diseases;Receivers;Feature extraction;Convolutional neural networks;Task analysis","computerised tomography;image reconstruction;medical image processing;neural nets","lower noise values;convolutional neural networks;monoE images;conventional single energy CT acquisitions;image-translation methods;automatic classification;pulmonary embolism;multitask optimization approach;low-kev monochromatic images;polyenergetic CT scans;improved automatic detection;detector-based spectral computed tomography;dual-energy CT technology;spectral information;spectral data;others virtual monoenergetic images;MonoE images","","","","10","","25 May 2021","","","IEEE","IEEE Conferences"
"Machine vision processing/selection in printed circuit board manufacturing","A. Trivedi; Paul Cheng-Hsin Liu","Center for Adv. Electron. Manuf., North Carolina A&T State Univ., Greensboro, NC, USA; NA","Proceedings The Twenty-Ninth Southeastern Symposium on System Theory","6 Aug 2002","1997","","","190","194","Machine vision applications are common in the manufacturing of PWBs. Machine vision systems are used in the placement of surface mount devices with lead pitch of 0.025 inch or less. There are two steps in the successful placement of surface mount devices, as follows: (a) The location of the device on the vacuum tip which picks and places the device must be determined. Translation and rotational offsets are the difference between the device and the vacuum tip center and rotation. The part orientation (translational offset) and orientation angle (rotational offset) must be known for accurate placement. (b) The location offiducials (reference marks incorporated into the circuit pattern) must also be determined.","0094-2898","0-8186-7873-9","10.1109/SSST.1997.581605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581605","","Machine vision;Printed circuits;Manufacturing processes;Assembly;Conducting materials;Surface-mount technology;Lead;Integrated circuit interconnections;Sheet materials;Wiring","computer vision;printed circuit manufacture;surface mount technology;assembling","machine vision processing/selection;printed circuit board manufacturing;surface mount devices;vacuum tip;location offiducials;rotational offsets;part orientation;translational offset;reference marks;0.025 in","","2","","2","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Developing Automatic Markerless Sign Language Gesture Tracking and Recognition System","S. Demidenko; M. Ooi; R. Akmeliawati; G. S. Gupta; Y. C. Kuang; D. Bailey; S. Khan; N. Gamage; S. Bilal","Sunway University,School of Science & Technology,Bandar Sunway,Malaysia; University of Waikato,School of Engineering,Hamilton,New Zealand; University of Adelaide,School of Mechanical Engineering,Adelaide,Australia; Massey University,School of Food and Advanced Technology,Palmerston North,New Zealand; University of Waikato,School of Engineering,Hamilton,New Zealand; Massey University,School of Food and Advanced Technology,Palmerston North,New Zealand; Massey University,School of Food and Advanced Technology,Palmerston North,New Zealand; Wirecard Asia,Singapore,Republic of Singapore; School of Information Technology,Whitireia,Porirua,New Zealand","2019 IEEE International Symposium on Haptic, Audio and Visual Environments and Games (HAVE)","5 Dec 2019","2019","","","1","6","Machine-based interpretations of sign language hand postures and gestures have long been a specialized research topic in human-computer interaction. Involvement of human subjects and multi-faceted nature of the problem requiring expertise in a multitude of disciplines make sign language interpretation an exigent research problem. Purpose of this paper is to present the experience and findings of international collaborative research conducted over several years on vision-based sign language translation. At first, this paper extends a discussion on sign language and its variations, machine-based translation and its significance. Secondly, a discussion on how three main tasks within the machine translation, namely: a) hand localization and tracking, b) hand posture interpretation, and c) hand gesture interpretation, can be addressed. Finally, research challenges, possible approaches, and future extensions are discussed.","","978-1-7281-2355-4","10.1109/HAVE.2019.8921358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8921358","sign language;image processing;markerless tracking;recognition;hand posture;hand gesture","Assistive technology;Gesture recognition;Feature extraction;Trajectory;Image color analysis;Head;Torso","computer vision;gesture recognition;human computer interaction;sign language recognition","sign language interpretation;vision-based sign language translation;machine-based translation;hand localization;automatic markerless sign language gesture tracking;machine-based interpretations;sign language hand postures;human-computer interaction","","","","17","","5 Dec 2019","","","IEEE","IEEE Conferences"
"A Portable Wireless DSP System for a Brain Machine Interface","S. Darmanjian; S. Morrison; B. Dang; K. Gugel; J. Principe","Comput. NeuroEngineering Lab., Florida Univ., Gainesville, FL; NA; NA; NA; NA","Conference Proceedings. 2nd International IEEE EMBS Conference on Neural Engineering, 2005.","18 Apr 2005","2005","","","112","115","In this paper, we present a design for a wearable DSP system that is capable of processing various neural-to-motor translation algorithms. The system first acquires the neural data through a high speed data bus in order to train and evaluate our prediction models. Then via a widely used protocol, the low-bandwidth output trajectory is wirelessly transmitted to a simulated robot arm. This system has been built and successfully tested with real data","1948-3554","0-7803-8710-4","10.1109/CNE.2005.1419566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419566","","Digital signal processing;Hardware;Trajectory;Algorithm design and analysis;Predictive models;Robots;Brain computer interfaces;Computer interfaces;Wearable computers;Laboratories","bioelectric phenomena;biomechanics;brain;digital signal processing chips;handicapped aids;medical robotics;medical signal processing;neurophysiology;wearable computers","portable wireless DSP system;brain machine interface;neural-to-motor translation algorithms;neural data;prediction models;low-bandwidth output trajectory;simulated robot arm","","1","","10","","18 Apr 2005","","","IEEE","IEEE Conferences"
"Toward New Retail: A Benchmark Dataset for Smart Unmanned Vending Machines","H. Zhang; D. Li; Y. Ji; H. Zhou; W. Wu; K. Liu","Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Southeast University, Nanjing, China; Chongqing University, Chongqing, China","IEEE Transactions on Industrial Informatics","21 Sep 2020","2020","16","12","7722","7731","Deep learning is a popular direction in computer vision and digital image processing. It is widely utilized in many fields, such as robot navigation, intelligent video surveillance, industrial inspection, and aerospace. With the extensive use of deep learning techniques, classification and object detection algorithms have been rapidly developed. In recent years, with the introduction of the concept of “unmanned retail,” object detection, and image classification play a central role in unmanned retail applications. However, open-source datasets of traditional classification and object detection have not yet been optimized for application scenarios of unmanned retail. Currently, classification and object detection datasets do not exist that focus on unmanned retail solely. Therefore, in order to promote unmanned retail applications by using deep learning-based classification and object detection, in this article we collected more than 30 000 images of unmanned retail containers using a refrigerator affixed with different cameras under both static and dynamic recognition environments. These images were categorized into ten kinds of beverages. After manual labeling, images in our constructed dataset contained 155 153 instances, each of which was annotated with a bounding box. We performed extensive experiments on this dataset using ten state-of-the-art deep learning-based models. Experimental results indicate great potential of using these deep learning-based models for real-world smart unmanned vending machines.","1941-0050","","10.1109/TII.2019.2954956","National Key Research and Development Program of China Stem Cell and Translational Research; National Natural Science Foundation of China; Shenzhen Science and Technology Program; National Natural Science Foundation of China; Fund of ZTE Corporation; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908822","Unmanned retail;vending machine;object detection;benchmark dataset","Object detection;Cameras;Computer vision;Computational modeling;Cloud computing;Sensors;Benchmark testing","computer vision;image classification;learning (artificial intelligence);neural nets;object detection;public domain software;retail data processing;service industries;vending machines","benchmark dataset;digital image processing;image classification;unmanned retail applications;open-source datasets;object detection datasets;deep learning-based classification;unmanned retail containers;computer vision;smart unmanned vending machines;dynamic recognition environment;static recognition environment;refrigerator","","10","","35","IEEE","21 Nov 2019","","","IEEE","IEEE Journals"
"Automatic 3-D Breath-Hold Related Motion Correction of Dynamic Multislice MRI","A. Elen; J. Hermans; J. Ganame; D. Loeckx; J. Bogaert; F. Maes; P. Suetens","Department of Electrical Engineering (ESAT/PSI), K.U.Leuven, Medical Imaging Research Center, UZ Gasthuisberg, Leuven, Belgium; Department of Electrical Engineering (ESAT/PSI), K.U.Leuven, Medical Imaging Research Center, UZ Gasthuisberg, Leuven, Belgium; Department of Cardiology, K.U.Leuven, UZ Gasthuisberg, Leuven, Belgium; Department of Electrical Engineering (ESAT/PSI), K.U.Leuven, Medical Imaging Research Center, UZ Gasthuisberg, Leuven, Belgium; Department of Radiology, K.U.Leuven, UZ Gasthuisberg, Leuven, Belgium; Department of Electrical Engineering (ESAT/PSI), K.U.Leuven, Medical Imaging Research Center, UZ Gasthuisberg, Leuven, Belgium; Department of Electrical Engineering (ESAT/PSI), K.U.Leuven, Medical Imaging Research Center, UZ Gasthuisberg, Leuven, Belgium","IEEE Transactions on Medical Imaging","1 Mar 2010","2010","29","3","868","878","Magnetic resonance (MR) cine images are often used to clinically assess left ventricular cardiac function. In a typical study, multiple 2-D long axis (LA) and short axis (SA) cine images are acquired, each in a different breath-hold. Differences in lung volume during breath-hold and overall patient motion distort spatial alignment of the images thus complicating spatial integration of all image data in three dimensions. We present a fully automatic postprocessing approach to correct these slice misalignments. The approach is based on the constrained optimization of the intensity similarity of intersecting image lines after the automatic definition of a region of interest. It uses all views and all time frames simultaneously. Our method models both in-plane and out-of-plane translations and full 3-D rotations, can be applied retrospectively and does not require a cardiac wall segmentation. The method was validated on both healthy volunteer and patient data with simulated misalignments, as well as on clinical multibreath-hold patient data. For the simulated data, subpixel accuracy could be obtained using translational correction. The possibilities and limitations of rotational correction were investigated and discussed. For the clinical multibreath-hold patient data sets, the median discrepancy between manual SA and LA contours was reduced from 2.83 to 1.33 mm using the proposed correction method. We have also shown the usefulness of the correction method for functional analysis on clinical image data. The same clinical multibreath-hold data sets were resegmented after positional correction, taking newly available complementary information of intersecting slices into account, further reducing the median discrepancy to 0.43 mm. This is due to the integration of the 2-D slice information into 3-D space.","1558-254X","","10.1109/TMI.2009.2039145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5423305","Cardiac function analysis;multislice magnetic resonance imaging (MRI);three-dimensional (3-D) spatial correction","Magnetic resonance imaging;Biomedical imaging;Image analysis;Spatial resolution;Position measurement;Time measurement;Lungs;Constraint optimization;Image segmentation;Functional analysis","biomedical MRI;cardiology;image motion analysis;image segmentation;medical image processing","automatic 3-D breath-hold related motion correction;dynamic multislice MRI;left ventricular cardiac function;multiple 2-D long axis cine images;multiple 2-D short axis cine images;image data spatial integration;lung volume;image spatial alignment;in-plane translations;out-of-plane translations;full 3-D rotations;positional correction;segmentation","Algorithms;Computer Simulation;Humans;Image Processing, Computer-Assisted;Lung;Lung;Magnetic Resonance Imaging, Cine;Models, Cardiovascular;Motion;Reproducibility of Results;Respiration;Statistics, Nonparametric;Ventricular Function, Left","23","","18","","1 Mar 2010","","","IEEE","IEEE Journals"
"A bootstrapping approach for SLU portability to a new language by inducting unannotated user queries","T. Misu; E. Mizukami; H. Kashioka; S. Nakamura; H. Li","National Institute of Information and Communications Technology (NICT), Japan; National Institute of Information and Communications Technology (NICT), Japan; National Institute of Information and Communications Technology (NICT), Japan; National Institute of Information and Communications Technology (NICT), Japan; Institute for Infocomm Research, Singapore","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4961","4964","This paper proposes a bootstrapping method of constructing a new spoken language understanding (SLU) system in a target language by utilizing statistical machine translation given an SLU module in some source language. The main challenge in this work is to induct unannotated automatic speech recognition results of user queries in the source language collected through a spoken dialog system, which is under public test. In order to select candidate expressions from among erroneous translation results stemming from problems with speech recognition and machine translation, we use back-translation results to check whether the translation result maintains the semantic meaning of the original sentence. We demonstrate that the proposed scheme can effectively prefer suitable sentences for inclusion in the training data as well as help improve the SLU module for the target language.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6289033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6289033","Spoken language understanding;Language portability;Statistical machine translation","Training data;Manuals;Training;Tagging;Semantics;Automatic speech recognition;Data models","speech recognition","bootstrapping approach;SLU portability;inducting unannotated user queries;spoken language understanding module;statistical machine translation;SLU module;unannotated automatic speech recognition;source language;spoken dialog system","","4","","12","","30 Aug 2012","","","IEEE","IEEE Conferences"
"Image Captioning in Turkish Language","B. D. Yılmaz; A. E. Demir; E. B. Sönmez; T. Yıldız","Istanbul Bilgi University,Dept. Computer Engineering,Istanbul,Turkey; Istanbul Bilgi University,Dept. Computer Engineering,Istanbul,Turkey; Istanbul Bilgi University,Dept. Computer Engineering,Istanbul,Turkey; Istanbul Bilgi University,Dept. Computer Engineering,Istanbul,Turkey","2019 Innovations in Intelligent Systems and Applications Conference (ASYU)","2 Jan 2020","2019","","","1","5","Image captioning is one of the everlasting challenging tasks in the field of artificial intelligence that requires computer vision and natural language processing. Plenty of salient works have been proposed throughout the time for English language however, the number of studies in Turkish language is still too limited. This paper couples an encoder CNN -the component that is responsible for extracting the features of the given images-, with a decoder RNN -the component that is responsible for generating captions using the given inputs-to generate Turkish captions within human gold-standards. We conducted the experiments using the most common evaluation metrics such as BLEU, METEOR, ROUGE and CIDEr. Results show that the performance of the proposed model is satisfactory in both qualitative and quantitatively evaluations. A Web App is already deployed to allow volunteers to contribute to the improvements of the Turkish captioned dataset.","","978-1-7281-2868-9","10.1109/ASYU48272.2019.8946358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946358","Turkish image captioning;computer vision;natural language processing;machine learning;deep neural networks;CNN;RNN","Databases;Measurement;Feature extraction;Recurrent neural networks;Task analysis;Computer vision;Decoding","artificial intelligence;computer vision;convolutional neural nets;feature extraction;image retrieval;Internet;language translation;natural language processing;recurrent neural nets;text analysis;text detection","image captioning;Turkish language;artificial intelligence;computer vision;natural language processing;English language;CNN encoder;RNN decoder;Turkish captions;human gold-standards;Web App;feature extraction","","","","15","","2 Jan 2020","","","IEEE","IEEE Conferences"
"Automated Lung Segmentation and Image Quality Assessment for Clinical 3-D/4-D-Computed Tomography","J. Wei; G. Li","Department of Computer Science, City College of New York, New York, NY, USA; Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, USA","IEEE Journal of Translational Engineering in Health and Medicine","20 May 2017","2014","2","","1","10","4-D-computed tomography (4DCT) provides not only a new dimension of patient-specific information for radiation therapy planning and treatment, but also a challenging scale of data volume to process and analyze. Manual analysis using existing 3-D tools is unable to keep up with vastly increased 4-D data volume, automated processing and analysis are thus needed to process 4DCT data effectively and efficiently. In this paper, we applied ideas and algorithms from image/signal processing, computer vision, and machine learning to 4DCT lung data so that lungs can be reliably segmented in a fully automated manner, lung features can be visualized and measured on the fly via user interactions, and data quality classifications can be computed in a robust manner. Comparisons of our results with an established treatment planning system and calculation by experts demonstrated negligible discrepancies (within ±2%) for volume assessment but one to two orders of magnitude performance enhancement. An empirical Fourier-analysis-based quality measure-delivered performances closely emulating human experts. Three machine learners are inspected to justify the viability of machine learning techniques used to robustly identify data quality of 4DCT images in the scalable manner. The resultant system provides a toolkit that speeds up 4-D tasks in the clinic and facilitates clinical research to improve current clinical practice.","2168-2372","","10.1109/JTEHM.2014.2381213","National Institutes of Health; City College of New York Seed; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6994263","Biomedical image processing;Image analysis;Classification algorithms;Morphological operations;Machine learning algorithms;Data visualization;Computed tomography;Biomedical image processing;image analysis;classification algorithms;morphological operations;machine learning algorithms;data visualization;computed tomography","Lungs;Image segmentation;Computed tomography;Noise;Three-dimensional displays;Discrete cosine transforms;Tumors","computerised tomography;image segmentation;learning (artificial intelligence);lung;medical image processing","clinical practice;scalable manner;machine learning techniques;machine learners;volume assessment;robust manner;data quality classifications;user interactions;fully automated manner;4DCT lung data;machine learning;computer vision;image/signal processing;automated processing;4-D data volume;data volume;radiation therapy planning;patient-specific information;4D computed tomography;3D computed tomography;image quality assessment;automated lung segmentation","","7","1","35","","19 Dec 2014","","","IEEE","IEEE Journals"
"Cardiac-DeepIED: Automatic Pixel-Level Deep Segmentation for Cardiac Bi-Ventricle Using Improved End-to-End Encoder-Decoder Network","X. Du; S. Yin; R. Tang; Y. Zhang; S. Li","School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; School of Computer Science and Technology, Anhui University, Hefei, China; Department of Medical Imaging, Western University, London, ON, Canada","IEEE Journal of Translational Engineering in Health and Medicine","27 Mar 2019","2019","7","","1","10","Accurate segmentation of cardiac bi-ventricle (CBV) from magnetic resonance (MR) images has a great significance to analyze and evaluate the function of the cardiovascular system. However, the complex structure of CBV image makes fully automatic segmentation as a well-known challenge. In this paper, we propose an improved end-to-end encoder-decoder network for CBV segmentation from the pixel level view (Cardiac-DeepIED). In our framework, we explicitly solve the high variability of complex cardiac structures through an improved encoder-decoder architecture which consists of Fire dilated modules and D-Fire dilated modules. This improved encoder-decoder architecture has the advantages of being capable of obtaining semantic task-aware representation and preserving fine-grained information. In addition, our method can dynamically capture potential spatiotemporal correlations between consecutive cardiac MR images through specially designed convolutional long-term and short-term memory structure; it can simulate spatiotemporal contexts between consecutive frame images. The combination of these modules enables the entire network to get an accurate, robust segmentation result. The proposed method is evaluated on the 145 clinical subjects with leave-one-out cross-validation. The average dice metric (DM) is up to 0.96 (left ventricle), 0.89 (myocardium), and 0.903 (right ventricle). The performance of our method outperforms state-of-the-art methods. These results demonstrate the effectiveness and advantages of our method for CBV regions segmentation at the pixel-level. It also reveals the proposed automated segmentation system can be embedded into the clinical environment to accelerate the quantification of CBV and expanded to volume analysis, regional wall thickness analysis, and three LV dimensions analysis.","2168-2372","","10.1109/JTEHM.2019.2900628","National Natural Science Foundation of China; Natural Science Foundation of Anhui Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651295","CBV segmentation;deep learning;encoder-decoder;magnetic resonance images","Image segmentation;Convolution;Fires;Semantics;Decoding;Standards;Task analysis","biomedical MRI;brain;cardiology;cardiovascular system;diseases;image coding;image segmentation;medical image processing;spatiotemporal phenomena","Cardiac-DeepIED;automatic pixel-level deep segmentation;cardiac bi-ventricle;improved end-to-end encoder-decoder network;magnetic resonance images;CBV image;fully automatic segmentation;CBV segmentation;pixel level view;complex cardiac structures;improved encoder-decoder architecture;semantic task-aware representation;preserving fine-grained information;consecutive cardiac MR images;specially designed convolutional long-term;short-term memory structure;consecutive frame images;robust segmentation result;CBV regions segmentation;automated segmentation system;Bi","","5","","39","","24 Feb 2019","","","IEEE","IEEE Journals"
"CHOBS: Color Histogram of Block Statistics for Automatic Bleeding Detection in Wireless Capsule Endoscopy Video","T. Ghosh; S. A. Fattah; K. A. Wahid","Department of Electrical Electronic Engineering, Pabna University of Science and Technology, Pabna, Bangladesh; Department of Electrical Electronic Engineering, Pabna University of Science and Technology, Pabna, Bangladesh; Department of Electrical Electronic Engineering, Pabna University of Science and Technology, Pabna, Bangladesh","IEEE Journal of Translational Engineering in Health and Medicine","12 Feb 2018","2018","6","","1","12","Wireless capsule endoscopy (WCE) is the most advanced technology to visualize whole gastrointestinal (GI) tract in a non-invasive way. But the major disadvantage here, it takes long reviewing time, which is very laborious as continuous manual intervention is necessary. In order to reduce the burden of the clinician, in this paper, an automatic bleeding detection method for WCE video is proposed based on the color histogram of block statistics, namely CHOBS. A single pixel in WCE image may be distorted due to the capsule motion in the GI tract. Instead of considering individual pixel values, a block surrounding to that individual pixel is chosen for extracting local statistical features. By combining local block features of three different color planes of RGB color space, an index value is defined. A color histogram, which is extracted from those index values, provides distinguishable color texture feature. A feature reduction technique utilizing color histogram pattern and principal component analysis is proposed, which can drastically reduce the feature dimension. For bleeding zone detection, blocks are classified using extracted local features that do not incorporate any computational burden for feature extraction. From extensive experimentation on several WCE videos and 2300 images, which are collected from a publicly available database, a very satisfactory bleeding frame and zone detection performance is achieved in comparison to that obtained by some of the existing methods. In the case of bleeding frame detection, the accuracy, sensitivity, and specificity obtained from proposed method are 97.85%, 99.47%, and 99.15%, respectively, and in the case of bleeding zone detection, 95.75% of precision is achieved. The proposed method offers not only low feature dimension but also highly satisfactory bleeding detection performance, which even can effectively detect bleeding frame and zone in a continuous WCE video data.","2168-2372","","10.1109/JTEHM.2017.2756034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268657","Bleeding detection;bleeding zone;color histogram;feature extraction;wireless capsule endoscopy","Image color analysis;Hemorrhaging;Feature extraction;Histograms;Indexes;Endoscopes;Wireless sensor networks","biomedical optical imaging;diseases;endoscopes;feature extraction;image classification;image colour analysis;image motion analysis;image texture;medical image processing;motion compensation;principal component analysis;video signal processing","color planes;bleeding frame;bleeding detection performance;color histogram;color texture feature;continuous WCE video data;frame detection;feature extraction;zone detection;principal component analysis;color histogram pattern;feature reduction technique;index value;RGB color space;local block features;local statistical features;individual pixel values;GI tract;capsule motion;WCE image;single pixel;CHOBS;automatic bleeding detection method;continuous manual intervention;gastrointestinal tract;wireless capsule endoscopy;block statistics","","5","","29","","24 Jan 2018","","","IEEE","IEEE Journals"
"A Multi-Anatomical Retinal Structure Segmentation System for Automatic Eye Screening Using Morphological Adaptive Fuzzy Thresholding","J. Almotiri; K. Elleithy; A. Elleithy","Computer Science Department, University of Bridgeport, Bridgeport, CT, USA; Computer Science Department, University of Bridgeport, Bridgeport, CT, USA; Computer Science Department, William Paterson University, Wayne, NJ, USA","IEEE Journal of Translational Engineering in Health and Medicine","4 Jun 2018","2018","6","","1","23","Eye exam can be as efficacious as physical one in determining health concerns. Retina screening can be the very first clue for detecting a variety of hidden health issues including pre-diabetes and diabetes. Through the process of clinical diagnosis and prognosis; ophthalmologists rely heavily on the binary segmented version of retina fundus image; where the accuracy of segmented vessels, optic disc, and abnormal lesions extremely affects the diagnosis accuracy which in turn affect the subsequent clinical treatment steps. This paper proposes an automated retinal fundus image segmentation system composed of three segmentation subsystems follow same core segmentation algorithm. Despite of broad difference in features and characteristics; retinal vessels, optic disc, and exudate lesions are extracted by each subsystem without the need for texture analysis or synthesis. For sake of compact diagnosis and complete clinical insight, our proposed system can detect these anatomical structures in one session with high accuracy even in pathological retina images. The proposed system uses a robust hybrid segmentation algorithm combines adaptive fuzzy thresholding and mathematical morphology. The proposed system is validated using four benchmark datasets: DRIVE and STARE (vessels), DRISHTI-GS (optic disc), and DIARETDB1 (exudates lesions). Competitive segmentation performance is achieved, outperforming a variety of up-to-date systems and demonstrating the capacity to deal with other heterogeneous anatomical structures.","2168-2372","","10.1109/JTEHM.2018.2835315","University of Bridgeport, Bridgeport, CT, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360472","Retina screening;retinopathy;retinal vessels segmentation;optic disc segmentation;retinal exudate segmentation;fuzzy systems;fuzzy C-means;adaptive local thresholding;morphological operations","Retina;Image segmentation;Anatomical structure;Diabetes;Retinopathy;Optical imaging;Biomedical optical imaging","blood vessels;diseases;eye;feature extraction;image segmentation;mathematical morphology;medical image processing","retinal vessels extraction;clinical treatment steps;DRIVE;retina fundus image binary segmentation;clinical prognosis;morphological adaptive fuzzy thresholding;automatic eye screening;vessels;prediabetes;eye exam;hybrid segmentation algorithm;clinical diagnosis;retina screening;multianatomical retinal structure segmentation system;heterogeneous anatomical structures;DIARETDB1;DRISHTI-GS;STARE;mathematical morphology;adaptive fuzzy thresholding;pathological retina images;exudate lesions;optic disc;core segmentation algorithm;automated retinal fundus image segmentation system","","","","72","","17 May 2018","","","IEEE","IEEE Journals"
"Automatic Lung Segmentation With Juxta-Pleural Nodule Identification Using Active Contour Model and Bayesian Approach","H. Chung; H. Ko; S. J. Jeon; K. -H. Yoon; J. Lee","Department of Biomedical Engineering, Wonkwang University College of Medicine, Iksan, South Korea; Department of Biomedical Engineering, Wonkwang University College of Medicine, Iksan, South Korea; Department of Radiology, Wonkwang University College of Medicine, Iksan, South Korea; Department of Radiology, Wonkwang University College of Medicine, Iksan, South Korea; Department of Biomedical Engineering, Wonkwang University College of Medicine, Iksan, South Korea","IEEE Journal of Translational Engineering in Health and Medicine","11 Jun 2018","2018","6","","1","13","Objective: chest computed tomography (CT) images and their quantitative analyses have become increasingly important for a variety of purposes, including lung parenchyma density analysis, airway analysis, diaphragm mechanics analysis, and nodule detection for cancer screening. Lung segmentation is an important prerequisite step for automatic image analysis. We propose a novel lung segmentation method to minimize the juxta-pleural nodule issue, a notorious challenge in the applications. Method: we initially used the Chan-Vese (CV) model for active lung contours and adopted a Bayesian approach based on the CV model results, which predicts the lung image based on the segmented lung contour in the previous frame image or neighboring upper frame image. Among the resultant juxta-pleural nodule candidates, false positives were eliminated through concave points detection and circle/ellipse Hough transform. Finally, the lung contour was modified by adding the final nodule candidates to the area of the CV model results. Results: to evaluate the proposed method, we collected chest CT digital imaging and communications in medicine images of 84 anonymous subjects, including 42 subjects with juxta-pleural nodules. There were 16873 images in total. Among the images, 314 included juxta-pleural nodules. Our method exhibited a disc similarity coefficient of 0.9809, modified hausdorff distance of 0.4806, sensitivity of 0.9785, specificity of 0.9981, accuracy of 0.9964, and juxta-pleural nodule detection rate of 96%. It outperformed existing methods, such as the CV model used alone, the normalized CV model, and the snake algorithm. Clinical impact: the high accuracy with the juxta-pleural nodule detection in the lung segmentation can be beneficial for any computer aided diagnosis system that uses lung segmentation as an initial step.","2168-2372","","10.1109/JTEHM.2018.2837901","Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Science, ICT & Future Planning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8361032","Active contour;lung segmentation;chest CT images;computer aided diagnosis;juxta-pleural nodule","Lung;Image segmentation;Computed tomography;Bayes methods;Level set;Brain modeling;DICOM","Bayes methods;cancer;computerised tomography;feature extraction;Hough transforms;image classification;image segmentation;lung;medical image processing","modified hausdorff distance;computer aided diagnosis system;circle/ellipse Hough transform;concave points detection;collected chest CT digital imaging;clinical impact;normalized CV model;juxta-pleural nodule detection rate;medicine images;final nodule candidates;resultant juxta-pleural nodule candidates;previous frame image;segmented lung contour;lung image;CV model results;active lung contours;Chan-Vese model;juxta-pleural nodule issue;novel lung segmentation method;automatic image analysis;important prerequisite step;diaphragm mechanics analysis;airway analysis;lung parenchyma density analysis;chest computed tomography images;Bayesian approach;active contour model;juxta-pleural nodule identification;automatic lung segmentation","","8","","47","","18 May 2018","","","IEEE","IEEE Journals"
"Automatic Detection of Compensation During Robotic Stroke Rehabilitation Therapy","Y. X. Zhi; M. Lukasik; M. H. Li; E. Dolatabadi; R. H. Wang; B. Taati","Toronto Rehabilitation Institute—University Health Network, Toronto, ON, Canada; Toronto Rehabilitation Institute—University Health Network, Toronto, ON, Canada; Toronto Rehabilitation Institute—University Health Network, Toronto, ON, Canada; Toronto Rehabilitation Institute—University Health Network, Toronto, ON, Canada; Toronto Rehabilitation Institute—University Health Network, Toronto, ON, Canada; Toronto Rehabilitation Institute—University Health Network, Toronto, ON, Canada","IEEE Journal of Translational Engineering in Health and Medicine","25 Jan 2018","2018","6","","1","7","Robotic stroke rehabilitation therapy can greatly increase the efficiency of therapy delivery. However, when left unsupervised, users often compensate for limitations in affected muscles and joints by recruiting unaffected muscles and joints, leading to undesirable rehabilitation outcomes. This paper aims to develop a computer vision system that augments robotic stroke rehabilitation therapy by automatically detecting such compensatory motions. Nine stroke survivors and ten healthy adults participated in this study. All participants completed scripted motions using a table-top rehabilitation robot. The healthy participants also simulated three types of compensatory motions. The 3-D trajectories of upper body joint positions tracked over time were used for multiclass classification of postures. A support vector machine (SVM) classifier detected lean-forward compensation from healthy participants with excellent accuracy (AUC = 0.98, F1 = 0.82), followed by trunk-rotation compensation (AUC = 0.77, F1 = 0.57). Shoulder-elevation compensation was not well detected (AUC = 0.66, F1 = 0.07). A recurrent neural network (RNN) classifier, which encodes the temporal dependency of video frames, obtained similar results. In contrast, F1-scores in stroke survivors were low for all three compensations while using RNN: lean-forward compensation (AUC = 0.77, F1 = 0.17), trunk-rotation compensation (AUC = 0.81, F1 = 0.27), and shoulder-elevation compensation (AUC = 0.27, F1 = 0.07). The result was similar while using SVM. To improve detection accuracy for stroke survivors, future work should focus on predefining the range of motion, direct camera placement, delivering exercise intensity tantamount to that of real stroke therapies, adjusting seat height, and recording full therapy sessions.","2168-2372","","10.1109/JTEHM.2017.2780836","Natural Sciences and Engineering Research Council of Canada; Toronto Rehabilitation Institute—University Health Network; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8214256","Motion compensation;posture classification;rehabilitation robotics;stroke rehabilitation","Medical treatment;Robot kinematics;Shoulder;Tracking;Muscles;Robot sensing systems","biomechanics;biomedical optical imaging;image classification;image sensors;medical disorders;medical image processing;medical robotics;motion compensation;motion control;muscle;patient rehabilitation;patient treatment;recurrent neural nets;robot vision;support vector machines;trajectory control;video coding","automatic compensation detection;robotic stroke rehabilitation therapy;therapy delivery;muscles;computer vision system;compensatory motions;stroke survivors;scripted motions;table-top rehabilitation robot;3D trajectories;upper body joint positions;multiclass posture classification;support vector machine classifier;SVM classifier;lean-forward compensation;AUC;trunk-rotation compensation;recurrent neural network;RNN classifier;temporal dependency;video frames;F1-scores;shoulder-elevation compensation;range-of-motion;direct camera placement;exercise intensity delivery","","4","","28","","15 Dec 2017","","","IEEE","IEEE Journals"
"Machine learning identification of diabetic retinopathy from fundus images","N. Gurudath; M. Celenk; H. B. Riley","School of Electrical Engineering and Computer Science Stocker Center, Ohio University Athens, OH 45701 USA; School of Electrical Engineering and Computer Science Stocker Center, Ohio University Athens, OH 45701 USA; School of Electrical Engineering and Computer Science Stocker Center, Ohio University Athens, OH 45701 USA","2014 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)","8 Jan 2015","2014","","","1","7","Diabetic retinopathy may potentially lead to blindness without early detection and treatment. In this research, an approach to automate the identification of the presence of diabetic retinopathy from color fundus images of the retina has been proposed. Classification of an input fundus image into one of the three classes, healthy/normal, Non-Proliferative Diabetic Retinopathy (NPDR) and Proliferative Diabetic Retinopathy (PDR) has been achieved. Blood vessel segmentation from the input image is achieved by Gaussian filtering. An adaptive, input - driven approach is considered for the mask generation and thresholding is accomplished using local entropy. The processed image obtained is characterized by second order textural feature, contrast, in four different orientations- 0°, 45°, 90° and 135° and structural features namely, fractal dimension and lacunarity. The research incorporates a three layered artificial neural network (ANN) and support vector machines (SVM) to classify the retinal images. The efficiency of the proposed approach has been evaluated on a set of 106 images from the DRIVE and DIARETB1 databases. The experimental results indicate that this method can produce a 97.2% and 98.1% classification accuracy using ANN and SVM respectively invariant of rotation, translation and scaling in input retinal images as opposed to a fixed mask based on the matched filter method.","","978-1-4799-8184-7","10.1109/SPMB.2014.7002949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7002949","Diabetic retinopathy;fundus images;Gaussian filtering;texture;contrast;fractal dimension;lacunarity;machine learning;artificial neural network;support vector machines","Fractals;Diabetes;Retinopathy;Retina;Biomedical imaging;Blood vessels;Artificial neural networks","biomedical optical imaging;blood vessels;entropy;eye;feature extraction;Gaussian processes;image classification;image segmentation;image texture;learning (artificial intelligence);matched filters;medical image processing;neural nets;support vector machines","machine learning identification;blindness;color fundus images;nonproliferative diabetic retinopathy;healthy-normal diabetic retinopathy;NPDR;blood vessel segmentation;Gaussian filtering;mask generation;local entropy;second order textural feature;structural features;fractal dimension;fractal lacunarity;artificial neural network;support vector machines;retinal image classification;DRIVE databases;DIARETB1 databases;SVM;ANN;matched filter method;image processing","","12","","28","","8 Jan 2015","","","IEEE","IEEE Conferences"
"Lung Diseases Classification based on Machine Learning Algorithms and Performance Evaluation","B. M. Boban; R. K. Megalingam","Amrita Vishwa Vidyapeetham,Department Of Electronics and Communication Engineering,Amritapuri,India; Amrita Vishwa Vidyapeetham,Department Of Electronics and Communication Engineering,Amritapuri,India","2020 International Conference on Communication and Signal Processing (ICCSP)","1 Sep 2020","2020","","","0315","0320","Machine learning (ML) is a significant subset of Artificial Intelligence (AI) that plays a key role in medical diagnosis. The advantage of AI is they can automatically learn, extract and translate the features from data sets such as images, text or video, without introducing traditional hand-coded code or rules. This paper focuses on recognizing and classifying lung diseases by ML algorithms. It includes 400 lung disease images (i.e. CT scan images) including bronchitis, emphysema, pleural effusion, cancer, and normal. The input image is analyzed, categorized and classified using ML algorithms such as the MLP, KNN and SVM classifier. After feature extraction, the output is segmented and compares the classifier's accuracy. When a CT scan image was given to a classifier as an input, it contains irrelevant information. For the selection of the most relevant features (i.e. for extracting characteristics) here Gray Level Co-occurrence Matrix (GLCM) is used. For MLP, this classifier acquires 98% accuracy, for SVM accuracy is 70.45% and for KNN accuracy is 99.2%. These classifiers will help the doctors to prescribe the most effective treatment for a patient.","","978-1-7281-4988-2","10.1109/ICCSP48568.2020.9182324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9182324","Machine learning (ML);Artificial Intelligence (AI);Gray-Level Co-occurrence Matrix (GLCM);Multilayer perceptron (MLP);K-nearest neighbors (KNN);Support vector machine(SVM)","Lung;Support vector machines;Feature extraction;Computed tomography;Cancer;Neurons","cancer;computerised tomography;feature extraction;feature selection;image classification;image segmentation;image texture;learning (artificial intelligence);lung;medical image processing;multilayer perceptrons;support vector machines","gray level cooccurrence matrix;feature selection;image segmentation;MLP;cancer;emphysema;bronchitis;lung disease images;KNN;SVM;classifier;feature extraction;pleural effusion;CT scan images;artificial intelligence;machine learning algorithms;lung diseases classification","","","","16","","1 Sep 2020","","","IEEE","IEEE Conferences"
"Screening for Atrial Fibrillation During Automatic Blood Pressure Measurements","A. Lowe; T. H. Oh; R. Stewart","Institute of Biomedical Technologies, Auckland University of Technology, Auckland, New Zealand; Cardiothoracic Surgical Unit, Auckland City Hospital, Auckland, New Zealand; Auckland District Health Board Cardiology, Auckland City Hospital, Auckland, New Zealand","IEEE Journal of Translational Engineering in Health and Medicine","25 Oct 2018","2018","6","","1","7","Atrial fibrillation (Afib) contributes significantly to overall cardiovascular risk. Widespread screening for Afib in primary care is sometimes performed by palpation, but suffers from low accuracy and is dependent on clinician experience. Algorithms implemented on oscillometric blood pressure devices can detect Afib with high sensitivity and specificity, but information on factors affecting accuracy is scant. Concurrent diagnostic electrocardiogram (ECG) and oscillometry were measured in participants in ECG clinics at two sites. Root mean squared successive difference (RMSSD) and irregularity index (Irrx) were calculated from oscillometric data and used to train logistic regression classifiers. Monte Carlo cross validation with 20 splits was performed to estimate confidence intervals for mean sensitivity and specificity, with various weightings, in the absence or presence of ectopics, and with or without repeated measurements. 707 measurements, including 168 Afib, were collected from 569 participants with mean (standard deviation) age of 63 (16) years. Sensitivity/specificity of RMSSD and Irrx were 0.982/0.908 and 0.986/0.960 respectively when ectopics were included. Excluding ectopics from the data improved specificity by up to 5%. Nevertheless, based on this performance and after accounting for prevalence of Afib in the population aged over 60 years, and estimated costs of healthcare, oscillometric screening for Afib in this age group could return a positive net health-economic benefit.","2168-2372","","10.1109/JTEHM.2018.2869609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486979","Atrial fibrillation;biomedical signal processing;medical conditions;medical tests;oscillometric blood pressure","Electrocardiography;Biomedical measurement;Atmospheric measurements;Particle measurements;Indexes;Pulse measurements;Blood pressure","blood;blood pressure measurement;cardiovascular system;diseases;electrocardiography;medical signal processing;Monte Carlo methods;regression analysis;signal classification","logistic regression classifiers;root mean squared successive difference;electrocardiogram;cardiovascular risk;oscillometric screening;ectopics;Monte Carlo cross validation;oscillometric data;irregularity index;ECG clinics;oscillometric blood pressure devices;automatic blood pressure measurements;atrial fibrillation","","","","29","","9 Oct 2018","","","IEEE","IEEE Journals"
"Accurate and Fully Automatic Hippocampus Segmentation Using Subject-Specific 3D Optimal Local Maps Into a Hybrid Active Contour Model","D. Zarpalas; P. Gkontra; P. Daras; N. Maglaveras","Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Laboratory of Medical Informatics, the Medical School, Aristotle University of Thessaloniki, Thessaloniki, Greece","IEEE Journal of Translational Engineering in Health and Medicine","30 Jun 2014","2014","2","","1","16","Assessing the structural integrity of the hippocampus (HC) is an essential step toward prevention, diagnosis, and follow-up of various brain disorders due to the implication of the structural changes of the HC in those disorders. In this respect, the development of automatic segmentation methods that can accurately, reliably, and reproducibly segment the HC has attracted considerable attention over the past decades. This paper presents an innovative 3-D fully automatic method to be used on top of the multiatlas concept for the HC segmentation. The method is based on a subject-specific set of 3-D optimal local maps (OLMs) that locally control the influence of each energy term of a hybrid active contour model (ACM). The complete set of the OLMs for a set of training images is defined simultaneously via an optimization scheme. At the same time, the optimal ACM parameters are also calculated. Therefore, heuristic parameter fine-tuning is not required. Training OLMs are subsequently combined, by applying an extended multiatlas concept, to produce the OLMs that are anatomically more suitable to the test image. The proposed algorithm was tested on three different and publicly available data sets. Its accuracy was compared with that of state-of-the-art methods demonstrating the efficacy and robustness of the proposed method.","2168-2372","","10.1109/JTEHM.2014.2297953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6702429","Hippocampus segmentation;hybrid active contour model (ACM);multi-atlas;prior knowledge;local weighting scheme;optimal local maps (OLMs)","Active contours;Image segmentation;Biomedical imaging;Image edge detection;Solid modeling;Active appearance model;Hippocampus;Algorithm design and analysis","biomedical MRI;brain;image segmentation;medical disorders;medical image processing;optimisation","magnetic resonance imaging;data sets;extended multiatlas concept;optimization scheme;training images;multiatlas concept;brain disorder prevention;brain disorder follow-up;brain disorder diagnosis;structural integrity;hybrid active contour model;subject-specific 3D optimal local maps;fully automatic hippocampus segmentation;accurate automatic hippocampus segmentation","","12","","80","","30 Jun 2014","","","IEEE","IEEE Journals"
"Automatic Diagnosis of Alzheimer’s Disease Using Neural Network Language Models","J. Fritsch; S. Wankerl; E. Nöth","Idiap Research Institute, Martigny, Switzerland; Friedrich-Alexander-University, Erlangen-Nuremberg, Germany; Friedrich-Alexander-University, Erlangen-Nuremberg, Germany","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","5841","5845","In today's aging society, the number of neurodegenerative diseases such as Alzheimer's disease (AD) increases. Reliable tools for automatic early screening as well as monitoring of AD patients are necessary. For that, semantic deficits have been shown to be useful indicators. We present a way to significantly improve the method introduced by Wankerl et al. [1]. The purely statistical approach of n-gram language models (LMs) is enhanced by using the rwthlm toolkit to create neural network language models (NNLMs) with Long Short Term-Memory (LSTM) cells. The prediction is solely based on evaluating the perplexity of transliterations of descriptions of the Cookie Theft picture from DementiaBank's Pitt Corpus. Each transliteration is evaluated on LMs of both control and Alzheimer speakers in a leave-one-speaker-out cross-validation scheme. The resulting perplexity values reveal enough discrepancy to classify patients on just those two values with an accuracy of 85.6% at equal-error-rate.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682690","Alzheimer’s disease;automatic diagnosis;language models;neural network language models;Long-Short Term Memory","Alzheimer's disease;Artificial neural networks;Biological system modeling;Vocabulary;Linguistics","diseases;medical diagnostic computing;natural language processing;neurophysiology;recurrent neural nets;speech recognition;statistical analysis","automatic early screening;semantic deficits;useful indicators;purely statistical approach;n-gram language models;neural network language models;automatic diagnosis;aging society;neurodegenerative diseases;reliable tools;LM;Alzheimers disease;long short term-memory cells;cookie theft picture;DementiaBank Pitt Corpus","","2","","18","","17 Apr 2019","","","IEEE","IEEE Conferences"
"Human Visual System-Based Fundus Image Quality Assessment of Portable Fundus Camera Photographs","S. Wang; K. Jin; H. Lu; C. Cheng; J. Ye; D. Qian","Institute of VLSI Design, Zhejiang University, Hangzhou, China; Department of Ophthalmology, the Second Affiliated Hospital, College of Medicine, Zhejiang University, Hangzhou, China; Institute of Translational Medicine, College of Medicine, Zhejiang University, Hangzhou, China; Med-imaging Integrated Solution Inc., Hsinchu, Taiwan; Department of Ophthalmology, the Second Affiliated Hospital, College of Medicine, Zhejiang University, Hangzhou, China; Institute of Translational Medicine, College of Medicine, Zhejiang University, Hangzhou, China","IEEE Transactions on Medical Imaging","31 Mar 2016","2016","35","4","1046","1055","Telemedicine and the medical “big data” era in ophthalmology highlight the use of non-mydriatic ocular fundus photography, which has given rise to indispensable applications of portable fundus cameras. However, in the case of portable fundus photography, non-mydriatic image quality is more vulnerable to distortions, such as uneven illumination, color distortion, blur, and low contrast. Such distortions are called generic quality distortions. This paper proposes an algorithm capable of selecting images of fair generic quality that would be especially useful to assist inexperienced individuals in collecting meaningful and interpretable data with consistency. The algorithm is based on three characteristics of the human visual system-multi-channel sensation, just noticeable blur, and the contrast sensitivity function to detect illumination and color distortion, blur, and low contrast distortion, respectively. A total of 536 retinal images, 280 from proprietary databases and 256 from public databases, were graded independently by one senior and two junior ophthalmologists, such that three partial measures of quality and generic overall quality were classified into two categories. Binary classification was implemented by the support vector machine and the decision tree, and receiver operating characteristic (ROC) curves were obtained and plotted to analyze the performance of the proposed algorithm. The experimental results revealed that the generic overall quality classification achieved a sensitivity of 87.45% at a specificity of 91.66%, with an area under the ROC curve of 0.9452, indicating the value of applying the algorithm, which is based on the human vision system, to assess the image quality of non-mydriatic photography, especially for low-cost ophthalmological telemedicine applications.","1558-254X","","10.1109/TMI.2015.2506902","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349228","Human visual system;machine learning;portable fundus photography;quality assessment","Retina;Distortion;Image quality;Cameras;Biomedical imaging;Photography;Lighting","biomedical optical imaging;decision trees;eye;medical image processing;photography;sensitivity analysis;support vector machines;telemedicine","human visual system;fundus image quality assessment;portable fundus camera photographs;support vector machine;decision tree;receiver operating characteristic curves;ROC curve;low-cost ophthalmological telemedicine applications","Algorithms;Diagnostic Techniques, Ophthalmological;Fundus Oculi;Humans;Image Processing, Computer-Assisted;Machine Learning;ROC Curve","37","","35","","8 Dec 2015","","","IEEE","IEEE Journals"
"Automatic Quantitative Analysis of Human Respired Carbon Dioxide Waveform for Asthma and Non-Asthma Classification Using Support Vector Machine","O. P. Singh; R. Palaniappan; M. Malarvili","Bio-Signal Processing Research Group, Faculty of Biosciences and Medical Engineering, Universiti Teknologi Malaysia, Johor Bahru, Malaysia; Data Science (E-Health) Research Group, School of Computing, University of Kent, Medway, U.K.; Bio-Signal Processing Research Group, Faculty of Biosciences and Medical Engineering, Universiti Teknologi Malaysia, Johor Bahru, Malaysia","IEEE Access","18 Oct 2018","2018","6","","55245","55256","Currently, carbon dioxide (CO2) waveforms measured by capnography are used to estimate respiratory rate and end-tidal CO2 (EtCO2) in the clinic. However, the shape of the CO2 signal carries significant diagnostic information about the asthmatic condition. Previous studies have shown a strong correlation between various features that quantitatively characterize the shape of CO2 signal and are used to discriminate asthma from non-asthma using pulmonary function tests, but no reliable progress was made, and no translation into clinical practice has been achieved. Therefore, this paper reports a relatively simple signal processing algorithm for automatic differentiation of asthma and non-asthma. CO2 signals were recorded from 30 non-asthmatic and 43 asthmatic patients. Each breath cycle was decomposed into subcycles, and features were computationally extracted. Thereafter, feature selection was performed using the area (Az) under the receiver operating characteristics curve analysis. A classification was performed via a leave-oneout cross-validation procedure by employing a support vector machine. Our results show maximum screening capabilities for upward expiration (AR1), downward inspiration (AR2), and the sum of AR1 and AR2, with an Az of 0.892, 0.803, and 0.793, respectively. The proposed method obtained an average accuracy of 94.52%, sensitivity of 97.67%, and specificity of 90% for discrimination of asthma and non-asthma. The proposed method allows for automatic classification of asthma and non-asthma condition by analyzing the shape of the CO2 waveform. The developed method may possibly be incorporated in real-time for assessment and management of the asthmatic conditions.","2169-3536","","10.1109/ACCESS.2018.2871091","Ministry of Higher Education, Malaysia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8468979","Area;quantitative;feature;classifier;asthma","Respiratory system;Feature extraction;Real-time systems;Shape;Carbon dioxide;Support vector machines;Correlation","biomedical measurement;diseases;feature extraction;grey systems;lung;medical disorders;medical signal processing;patient diagnosis;patient monitoring;pneumodynamics;support vector machines","support vector machine;automatic classification;asthmatic condition;automatic quantitative analysis;human respired carbon dioxide waveform;nonasthma classification;signal processing algorithm;characteristics curve analysis","","2","","48","CCBY","20 Sep 2018","","","IEEE","IEEE Journals"
"A novel 6-degree of freedom man-machine interface using MEMS sensors and computer vsion","H. F. Ko; K. H. Wong","Department of Computer Science and Engineering, The Chinese University of Hong Kong, HSH Engineering Building, CUHK, Shatin, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, HSH Engineering Building, CUHK, Shatin, Hong Kong","2015 IEEE 12th International Conference on Networking, Sensing and Control","4 Jun 2015","2015","","","344","349","Developing user friendly Human-Computer-Interfaces is important in many areas of computer applications. Traditional input devices such as the keyboard and mouse are designed for general purpose computers only, they are not likely to provide intuitive user experiences for multi-dimensional input entry. Therefore, we design and implement an embedded system for users to interactive with a computer in 6-degree of freedom. The system can capture the 3-D rotation and 3-D translation motions which are the basic 6-D motion parameters. In our proposed approach, the translational motion is estimated using a computer vision method by tracking the speckled pattern generated by a laser beam on a rough surface. The orientation information is estimated by measuring the gravitational and magnetic fields of the Earth. A gyroscope is included together with the use of Kalman filter to improve the dynamic responses. A prototype is built to evaluate the performance of the proposed tracking method. Experiments have been carried out which show that the approach can work with good accuracy and the whole system can be built using off-the-shelf low cost components.","","978-1-4799-8069-7","10.1109/ICNSC.2015.7116060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116060","Human Computer Interaction;Motion sensing;Accelerometer;Gyroscope;Compass;Kalman filter","Gyroscopes;Accelerometers;Optical sensors;Tracking;Kalman filters;Compass","computer vision;embedded systems;gyroscopes;human computer interaction;Kalman filters;microsensors;motion estimation","6-degree of freedom man-machine interface;MEMS sensors;computer vsion;human-computer-interfaces;multidimensional input entry;embedded system;3D rotation motion;3D translation motion;translational motion estimation;speckled pattern tracking;laser beam;orientation information estimation;gravitational field measurement;magnetic field measurement;Earth;gyroscope;Kalman filter;dynamic responses;performance evaluation;off-the-shelf low cost components","","","","12","","4 Jun 2015","","","IEEE","IEEE Conferences"
"Early Detection of Acute Chest Syndrome Through Electronic Recording and Analysis of Auscultatory Percussion","B. Allen; R. Molokie; T. J. Royston","Richard and Loan Hill Department of Bioengineering, University of Illinois at Chicago, Chicago, IL, USA; Department of Medicine, University of Illinois at Chicago, Chicago, IL, USA; Richard and Loan Hill Department of Bioengineering, University of Illinois at Chicago, Chicago, IL, USA","IEEE Journal of Translational Engineering in Health and Medicine","7 Oct 2020","2020","8","","1","8","Acute chest syndrome (ACS) is the leading cause of death among people with sickle cell disease. ACS is clinically defined and diagnosed by the presence of a new pulmonary infiltrate on chest imaging with accompanying fever and respiratory symptoms like hypoxia, tachypnea, and shortness of breath. However, the characteristic chest x-ray (CXR) findings necessary for a clinical diagnosis of ACS can be difficult to detect, as is determining which patient needs a CXR. This makes early detection difficult; but it is critical in order to limit ACS severity and subsequent fatalities. This research project looks to apply percussion and auscultation techniques that can provide an immediate diagnosis of acute pulmonary conditions by using an automated standard percussive input and electronic auscultation for computational analysis of the measured signal. Measurements on sickle cell patients having ACS, vaso-occlusive crisis (VOC), and regular clinic visits (healthy) were recorded and analyzed. Average intensity of sound transmission through the chest and lungs was determined in the ACS and healthy subject groups, revealing an average of 10-14 dB decrease in sound intensity in the ACS group compared to the healthy group. A random under-sampling boosted tree classification model identified with 94% accuracy the positive ACS and healthy observations. The analysis also revealed unique measurable changes in a small number of cases clinically classified as complicated VOC, which later developed into ACS. This suggests the developed approach may also have early predictive capability, identifying patients at risk for developing ACS prior to current clinical practice.","2168-2372","","10.1109/JTEHM.2020.3027802","NIH through the University of Illinois at Chicago Clinical and Translational Science Award; Pilot Grant Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210002","Acoustic;stethoscope;percussion;lung;acute chest syndrome;sickle cell disease;machine learning;diagnosis","Diseases;Lung;Hospitals;Chirp;Stethoscope;Pain;Clinical diagnosis","bioacoustics;biomedical measurement;diseases;electronic health records;lung;medical disorders;medical signal processing","respiratory symptoms;fever;random under-sampling boosted tree classification model;sound transmission intensity;regular clinic visits;vaso-occlusive crisis;computational analysis;characteristic chest X-ray findings;auscultatory percussion;electronic recording;early detection;clinical diagnosis;chest imaging;sickle cell disease;acute chest syndrome;positive ACS;ACS group;lungs;sickle cell patients;electronic auscultation;automated standard percussive input;acute pulmonary conditions","","","","17","CCBY","30 Sep 2020","","","IEEE","IEEE Journals"
"Feature Extraction Based Machine Learning for Human Burn Diagnosis From Burn Images","D. P. Yadav; A. Sharma; M. Singh; A. Goyal","Department of Computer Engineering & Applications, GLA University, Mathura, India; Department of Computer Engineering & Applications, GLA University, Mathura, India; School of Technology Studies, Endicott College of International Studies, Woosong University, Daejeon, South Korea; Department of Electrical Engineering and Computer Science, Texas A&M University–Kingsville, Kingsville, TX, USA","IEEE Journal of Translational Engineering in Health and Medicine","30 Jul 2019","2019","7","","1","7","Burn is one of the serious public health problems. Usually, burn diagnoses are based on expert medical and clinical experience and it is necessary to have a medical or clinical expert to conduct an examination in restorative clinics or at emergency rooms in hospitals. But sometimes a patient may have a burn where there is no specialized facility available, and in such a case a computerized automatic burn assessment tool may aid diagnosis. Burn area, depth, and location are the critical factors in determining the severity of burns. In this paper, a classification model to diagnose burns is presented using automated machine learning. The objective of the research is to develop the feature extraction model to classify the burn. The proposed method based on support vector machine (SVM) is evaluated on a standard data set of burns-BIP_US database. Training is performed by classifying images into two classes, i.e., those that need grafts and those that are non-graft. The 74 images of test data set are tested with the proposed SVM based method and according to the ground truth, the accuracy of 82.43% was achieved for the SVM based model, which was higher than the 79.73% achieved in past work using the multidimensional scaling analysis (MDS) approach.","2168-2372","","10.1109/JTEHM.2019.2923628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8766148","Image preprocessing;burn;classification;graft;SVM","Skin;Image color analysis;Support vector machines;Surgery;Biomedical imaging;Wounds","feature extraction;image classification;learning (artificial intelligence);medical image processing;support vector machines","public health problems;SVM based model;support vector machine;automated machine learning;computerized automatic burn assessment tool;restorative clinics;clinical expert;medical expert;burn diagnoses;burn images;human burn diagnosis;feature extraction based machine","","7","","20","CCBY","18 Jul 2019","","","IEEE","IEEE Journals"
"Robust Sparse Representation and Multiclass Support Matrix Machines for the Classification of Motor Imagery EEG Signals","I. Razzak; I. A. Hameed; G. Xu","University of Technology, Sydney, NSW, Australia; University of Technology, Sydney, NSW, Australia; University of Technology, Sydney, NSW, Australia","IEEE Journal of Translational Engineering in Health and Medicine","25 Oct 2019","2019","7","","1","8","Background: EEG signals are extremely complex in comparison to other biomedical signals, thus require an efficient feature selection as well as classification approach. Traditional feature extraction and classification methods require to reshape the data into vectors that results in losing the structural information exist in the original featured matrix. Aim: The aim of this work is to design an efficient approach for robust feature extraction and classification for the classification of EEG signals. Method: In order to extract robust feature matrix and reduce the dimensionality of from original epileptic EEG data, in this paper, we have applied robust joint sparse PCA (RJSPCA), Outliers Robust PCA (ORPCA) and compare their performance with different matrix base feature extraction methods, followed by classification through support matrix machine. The combination of joint sparse PCA with robust support matrix machine showed good generalization performance for classification of EEG data due to their convex optimization. Results: A comprehensive experimental study on the publicly available EEG datasets is carried out to validate the robustness of the proposed approach against outliers. Conclusion: The experiment results, supported by the theoretical analysis and statistical test, show the effectiveness of the proposed framework for solving classification of EEG signals.","2168-2372","","10.1109/JTEHM.2019.2942017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854827","Brain-computer interfaces;Electroencephalography (EEG);Principal component Analysis (PCA);Brain disorder","Electroencephalography;Feature extraction;Principal component analysis;Sparse matrices;Dimensionality reduction;Loading;Electromagnetics","electroencephalography;feature extraction;medical signal processing;principal component analysis;signal classification;support vector machines","convex optimization;outliers robust PCA;robust sparse representation;EEG data;robust support matrix machine;extraction methods;robust joint sparse PCA;feature extraction;signal classification;efficient feature selection;biomedical signals;motor imagery EEG signals;multiclass support matrix machines","","2","","45","CCBY","2 Oct 2019","","","IEEE","IEEE Journals"
"High Precision Digitization of Paper-Based ECG Records: A Step Toward Machine Learning","M. Baydoun; L. Safatly; O. K. Abou Hassan; H. Ghaziri; A. El Hajj; H. Isma’eel","Beirut Research and Innovation Center, Beirut, Lebanon; Electrical and Computer Engineering Department, American University of Beirut, Beirut, Lebanon; Internal Medicine Department, American University of Beirut, Beirut, Lebanon; Beirut Research and Innovation Center, Beirut, Lebanon; Electrical and Computer Engineering Department, American University of Beirut, Beirut, Lebanon; Internal Medicine Department, American University of Beirut, Beirut, Lebanon","IEEE Journal of Translational Engineering in Health and Medicine","22 Nov 2019","2019","7","","1","8","Introduction: The electrocardiogram (ECG) plays an important role in the diagnosis of heart diseases. However, most patterns of diseases are based on old datasets and stepwise algorithms that provide limited accuracy. Improving diagnostic accuracy of the ECG can be done by applying machine learning algorithms. This requires taking existing scanned or printed ECGs of old cohorts and transforming the ECG signal to the raw digital (time (milliseconds), voltage (millivolts)) form. Objectives: We present a MATLAB-based tool and algorithm that converts a printed or scanned format of the ECG into a digitized ECG signal. Methods: 30 ECG scanned curves are utilized in our study. An image processing method is first implemented for detecting the ECG regions of interest and extracting the ECG signals. It is followed by serial steps that digitize and validate the results. Results: The validation demonstrates very high correlation values of several standard ECG parameters: PR interval 0.984 +/-0.021 (p-value <; 0.001), QRS interval 1+/- SD (p-value <; 0.001), QT interval 0.981 +/- 0.023 p-value <; 0.001, and RR interval 1 +/- 0.001 p-value <; 0.001. Conclusion: Digitized ECG signals from existing paper or scanned ECGs can be obtained with more than 95% of precision. This makes it possible to utilize historic ECG signals in machine learning algorithms to identify patterns of heart diseases and aid in the diagnostic and prognostic evaluation of patients with cardiovascular disease.","2168-2372","","10.1109/JTEHM.2019.2949784","MPP Fund at the American University of Beirut Medical Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894038","Electrocardiogram;digitization;Matlab tool;image processing","Electrocardiography;Tools;Standards;Machine learning;Machine learning algorithms;Matlab;Licenses","cardiovascular system;diseases;electrocardiography;learning (artificial intelligence);medical signal detection;medical signal processing","cardiovascular disease;MATLAB-based tool;electrocardiogram;standard ECG parameters;digitized ECG signal;step toward machine learning;paper-based ECG records;high precision digitization;heart diseases;machine learning algorithms;historic ECG signals","","1","","27","CCBY","7 Nov 2019","","","IEEE","IEEE Journals"
"Simultaneous Tracking of Cardiorespiratory Signals for Multiple Persons Using a Machine Vision System With Noise Artifact Removal","A. Al-Naji; J. Chahl","School of Engineering, University of South Australia, Mawson Lakes, SA, Australia; School of Engineering, University of South Australia, Mawson Lakes, SA, Australia","IEEE Journal of Translational Engineering in Health and Medicine","13 Oct 2017","2017","5","","1","10","Most existing non-contact monitoring systems are limited to detecting physiological signs from a single subject at a time. Still, another challenge facing these systems is that they are prone to noise artifacts resulting from motion of subjects, facial expressions, talking, skin tone, and illumination variations. This paper proposes an efficient non-contact system based on a digital camera to track the cardiorespiratory signal from a number of subjects (up to six persons) at the same time with a new method for noise artifact removal. The proposed system relied on the physiological and physical effects as a result of the activity of the cardiovascular and respiratory systems, such as skin color changes and head motion. Since these effects are imperceptible to the human eye and highly affected by the noise variations, we used advanced signal and video processing techniques, including developing video magnification technique, complete ensemble empirical mode decomposition with adaptive noise, and canonical correlation analysis to extract the heart rate and respiratory rate from multiple subjects under the noise artifact assumptions. The experimental results of the proposed system had a significant correlation (Pearson's correlation coefficient = 0.9994, Spearman correlation coefficient = 0.9987, and root mean square error = 0.32) when compared with the conventional contact methods (pulse oximeter and piezorespiratory belt), which makes the proposed system a promising candidate for novel applications.","2168-2372","","10.1109/JTEHM.2017.2757485","Defence Science and Technology Group’s Tyche Program on trusted autonomy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053769","Cardiorespiratory signal;camera imaging-based methods;imaging photoplethysmography (iPPG);video magnification technique;complete ensemble EMD with adaptive noise;canonical correlation analysis;graphical user interface","Face;Biomedical monitoring;Skin;Image color analysis;Webcams;Forehead","cameras;cardiology;computer vision;image denoising;medical image processing;pneumodynamics;video signal processing","canonical correlation analysis;adaptive noise;complete ensemble empirical mode decomposition;video magnification technique;video processing techniques;head motion;skin color changes;digital camera;noncontact system;noise artifact removal;machine vision system;multiple persons;cardiorespiratory signals;simultaneous tracking","","3","","82","OAPA","29 Sep 2017","","","IEEE","IEEE Journals"
"Software Vulnerability Detection Using Deep Neural Networks: A Survey","G. Lin; S. Wen; Q. -L. Han; J. Zhang; Y. Xiang","School of Information Engineering, Sanming University, Fujian, Sanming, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia","Proceedings of the IEEE","28 Sep 2020","2020","108","10","1825","1848","The constantly increasing number of disclosed security vulnerabilities have become an important concern in the software industry and in the field of cybersecurity, suggesting that the current approaches for vulnerability detection demand further improvement. The booming of the open-source software community has made vast amounts of software code available, which allows machine learning and data mining techniques to exploit abundant patterns within software code. Particularly, the recent breakthrough application of deep learning to speech recognition and machine translation has demonstrated the great potential of neural models' capability of understanding natural languages. This has motivated researchers in the software engineering and cybersecurity communities to apply deep learning for learning and understanding vulnerable code patterns and semantics indicative of the characteristics of vulnerable code. In this survey, we review the current literature adopting deep-learning-/neural-network-based approaches for detecting software vulnerabilities, aiming at investigating how the state-of-the-art research leverages neural techniques for learning and understanding code semantics to facilitate vulnerability discovery. We also identify the challenges in this new field and share our views of potential research directions.","1558-2256","","10.1109/JPROC.2020.2993293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108283","Cybersecurity;deep neural network (DNN);machine learning (ML);representation learning;software vulnerability","Computer security;Semantics;Feature extraction;Open source software;Neural networks;Deep learning;Computer bugs","data mining;language translation;learning (artificial intelligence);natural languages;neural nets;public domain software;security of data;software engineering;speech recognition","neural network-based approaches;natural languages;vulnerability discovery;code semantics;vulnerable code patterns;cybersecurity communities;software engineering;machine translation;speech recognition;deep learning;data mining techniques;machine learning;software code;open-source software community;software industry;security vulnerabilities;deep neural networks;software vulnerability detection","","6","","128","IEEE","4 Jun 2020","","","IEEE","IEEE Journals"
"Study on real-time correction method of die declination angles in LED sorting system","T. Wu; B. Li; L. Wang; Y. Huang","National Numerical Control System Engineering Research Center of Huazhong University of Science and Technology, Wuhan, 430074, China; National Numerical Control System Engineering Research Center of Huazhong University of Science and Technology, Wuhan, 430074, China; National Numerical Control System Engineering Research Center of Huazhong University of Science and Technology, Wuhan, 430074, China; National Numerical Control System Engineering Research Center of Huazhong University of Science and Technology, Wuhan, 430074, China","2010 IEEE International Conference on Mechatronics and Automation","7 Oct 2010","2010","","","1443","1448","All dies are required to keep consistent angles in LED chip sorting. Usually, rectification orient real-time vision is used for this but it may decrease production efficiency greatly, because one correction may need several adjustments. Considering that all dies to be sorted are adhesive to wafer tape, each is rotated, locations of all other dies change. It is a problem for following positioning and sorting. To solve this problem, an on-line angle correction method basing on translation compensation is put forward. In this way, angle deviations of dies are transformed as translations of recycling bins. Each die declination can be corrected without any rotations. It does little impact on sorting efficiency because the translation of recycling bins acts concurrently with the die ejection and transporting, etc. Techniques on figuring out displacement compensations are discussed in detail. In addition, SPN model of declination angle correction is built to analyze its influence to production rate. With theoretical analysis and experimental results, it is revealed that sort efficiency may decrease by only 3% for angle correction. Such effect is perfectly acceptable.","2152-744X","978-1-4244-5140-1","10.1109/ICMA.2010.5589001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5589001","declination angle;correction;parallel scheduling;machine vision;translation compensation;stochastic Petri net","Recycling;Sorting;Lenses;Markov processes;Clocks;Mouth;Computer numerical control","computer vision;materials handling;microassembling;Petri nets;scheduling;semiconductor industry;semiconductor technology","die declination angles;LED sorting system;LED chip sorting;rectification orient realtime vision;online angle correction method;translation compensation;recycling bins;displacement compensations;declination angle correction;stochastic Petri nets","","3","","14","","7 Oct 2010","","","IEEE","IEEE Conferences"
"Punctuated transcription of multi-genre broadcasts using acoustic and lexical approaches","O. Klejch; P. Bell; S. Renals","Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK; Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK; Centre for Speech Technology Research, University of Edinburgh, EH8 9AB, UK","2016 IEEE Spoken Language Technology Workshop (SLT)","9 Feb 2017","2016","","","433","440","In this paper we investigate the punctuated transcription of multi-genre broadcast media. We examine four systems, three of which are based on lexical features, the fourth of which uses acoustic features by integrating punctuation into the speech recognition acoustic models. We also explore the combination of these component systems using voting and log-linear interpolation. We performed experiments on the English language MGB Challenge data, which comprises about 1,600h of BBC television recordings. Our results indicate that a lexical system, based on a neural machine translation approach is significantly better than other systems achieving an F-Measure of 62.6% on reference text, with a relative degradation of 19% on ASR output. Our analysis of the results in terms of specific punctuation indicated that using longer context improves the prediction of question marks and acoustic information improves prediction of exclamation marks. Finally, we show that even though the systems are complementary, their straightforward combination does not yield better F-measures than a single system using neural machine translation.","","978-1-5090-4903-5","10.1109/SLT.2016.7846300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846300","punctuation;speech recognition;neural machine translation;rich transcription","Acoustics;Context;Predictive models;Hidden Markov models;Labeling;Interpolation;Context modeling","acoustic signal processing;interpolation;neural nets;speech recognition;text analysis","exclamation mark prediction;acoustic information;question mark prediction;specific punctuation;ASR output;relative degradation;reference text;F-measure;neural machine translation approach;BBC television recordings;English language MGB Challenge data;log-linear interpolation;voting;speech recognition acoustic model;acoustic features;lexical features;multigenre broadcast media;acoustic approach;punctuated transcription","","6","","34","","9 Feb 2017","","","IEEE","IEEE Conferences"
"ASR error segment localization for spoken recovery strategy","F. Béchet; B. Favre","LIF/CNRS, Aix-Marseille University, 163 avenue de Luminy, France; LIF/CNRS, Aix-Marseille University, 163 avenue de Luminy, France","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","21 Oct 2013","2013","","","6837","6841","Even though small ASR errors might not impact downstream processes that make use of the transcript, larger error segments like those generated by OOVs can have a considerable impact on applications such as speech-to-speech translation and can eventually lead to communication failure between users of the system. This work focuses on error detection in ASR output targeted towards significant error segments that can be recovered using a dialog system. We propose a CRF system trained to recognize error segments with ASR confidence-based, lexical and syntactic features. The most significant error segment is passed to a dialog system for interactive recovery in which rephrased words are reinserted in the original. 22% of utterances can be fully recovered and an interesting by-product is that rewriting error segments as a single token reduces WER by 17% on an adverse corpus.","2379-190X","978-1-4799-0356-6","10.1109/ICASSP.2013.6638986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638986","Automatic Speech Recognition;Confidence Measure;Error Detection;Speech to Speech translation","Speech;Syntactics;Speech recognition;Error correction;Lattices;Computational linguistics;Measurement uncertainty","speech recognition","ASR error segment localization;spoken recovery strategy;downstream process;speech-to-speech translation;communication failure;error detection;CRF system;dialog system;interactive recovery;automatic speech recognition","","7","","20","","21 Oct 2013","","","IEEE","IEEE Conferences"
"Real-Time American Sign Language Recognition Using Skin Segmentation and Image Category Classification with Convolutional Neural Network and Deep Learning","S. Shahriar; A. Siddiquee; T. Islam; A. Ghosh; R. Chakraborty; A. I. Khan; C. Shahnaz; S. A. Fattah","Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, 1205, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, 1205, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, 1205, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, 1205, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, 1205, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, 1205, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, 1205, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, 1205, Bangladesh","TENCON 2018 - 2018 IEEE Region 10 Conference","24 Feb 2019","2018","","","1168","1171","A real-time sign language translator is an important milestone in facilitating communication between the deaf community and the general public. We hereby present the development and implementation of an American Sign Language (ASL) fingerspelling translator based on skin segmentation and machine learning algorithms. We present an automatic human skin segmentation algorithm based on color information. The YCbCr color space is employed because it is typically used in video coding and provides an effective use of chrominance information for modeling the human skin color. We model the skin-color distribution as a bivariate normal distribution in the CbCr plane. The performance of the algorithm is illustrated by simulations carried out on images depicting people of different ethnicity. Then Convolutional Neural Network (CNN) is used to extract features from the images and Deep Learning Method is used to train a classifier to recognize Sign Language.","2159-3450","978-1-5386-5457-6","10.1109/TENCON.2018.8650524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8650524","sign language;skin segmentation;machine learning;YCbCr color space;gesture recognition","Image color analysis;Skin;Feature extraction;Gesture recognition;Assistive technology;Image segmentation;Colored noise","convolutional neural nets;feature extraction;gesture recognition;image classification;image colour analysis;image segmentation;learning (artificial intelligence);normal distribution;real-time systems;sign language recognition;skin","real-time American Sign Language recognition;Deep Learning Method;bivariate normal distribution;skin-color distribution;human skin color;chrominance information;YCbCr color space;color information;automatic human skin segmentation algorithm;machine learning algorithms;American Sign Language fingerspelling translator;general public;deaf community;real-time sign language translator;Convolutional Neural Network;image category classification","","5","","18","","24 Feb 2019","","","IEEE","IEEE Conferences"
"OCRMPD: OCR system for Myanmar printed document image with a novel segmentation method and hierarchical classification scheme","H. P. P. Win; P. T. T. Khine; K. N. N. Tun","University of Computer Studies, Yangon, Myanmar; University of Computer Studies, Yangon, Myanmar; University of Computer Studies, Yangon, Myanmar","2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing","20 Oct 2011","2011","","","285","291","As large quantity of document images is getting archived by the digital libraries, an efficient strategy that can convert Myanmar document image into machine understandable text format is needed. And Myanmar language contains many words, and most of them are similar, especially for small fonts, the accuracy of the Optical Character Recognition, OCR system for Myanmar may be low. Therefore, this paper designs an OCR system for Myanmar Printed Document (OCRMPD) with several proposed methods that can automatically convert Myanmar printed text to machine understandable text. In order to get more accurate system, enhance the input image by removing noise and making some correction on variants. A method for isolation of the character image is proposed by using connected component analysis for wrongly segmented characters produced by projection only. Finally, hierarchical mechanism is used for SVM classifier for recognition of the character image. The proposed algorithms have been tested on a variety of Myanmar printed documents and the results of the experiments indicate that the methods can increase the segmentation accuracy as well as recognition rates.","","978-1-4577-1481-8","10.1109/ICCP.2011.6047882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6047882","machine printed;OCR;OCRMPD;Myanmar scripts;Support vector machine","Optical character recognition software;Feature extraction;Accuracy;Image segmentation;Support vector machines;Character recognition;Text recognition","digital libraries;document image processing;image classification;image denoising;image segmentation;language translation;natural language processing;optical character recognition;support vector machines;text analysis","character image recognition;SVM classifier;noise removal;Myanmar printed text conversion;optical character recognition;Myanmar language;machine understandable text format;digital library;hierarchical classification scheme;image segmentation method;OCR system for Myanmar printed document image;OCRMPD image","","1","","27","","20 Oct 2011","","","IEEE","IEEE Conferences"
"Visual-speech to text conversion applicable to telephone communication for deaf individuals","P. Heracleous; H. Ishiguro; N. Hagita","Intelligent Robotics and Communication Laboratories, Advanced Telecommunications Research Institute International, 2-2-2 Hikaridai Seika-cho, Soraku-gun, Kyoto-fu, Japan 619-0288; Intelligent Robotics and Communication Laboratories, Advanced Telecommunications Research Institute International, 2-2-2 Hikaridai Seika-cho, Soraku-gun, Kyoto-fu, Japan 619-0288; Intelligent Robotics and Communication Laboratories, Advanced Telecommunications Research Institute International, 2-2-2 Hikaridai Seika-cho, Soraku-gun, Kyoto-fu, Japan 619-0288","2011 18th International Conference on Telecommunications","20 Jun 2011","2011","","","130","133","The access to communication technologies has become essential for the handicapped people. This study introduces the initial step of an automatic translation system able to translate visual speech used by deaf individuals to text, or auditory speech. A such a system would enable deaf users to communicate with each other and with normal-hearing people through telephone networks or through Internet by only using telephone devices equipped with simple cameras. In particular, this paper introduces automatic recognition and conversion to text of Cued Speech for French. Cued speech is a visual mode used for communication in the deaf society. Using hand shapes placed in different positions near the face as a complement to lipreading, all the sounds of a spoken language can be visually distinguished and perceived. Experimental results show high recognition rates for both isolated word and continuous phoneme recognition experiments in Cued Speech for French.","","978-1-4577-0024-8","10.1109/CTS.2011.5898904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5898904","Telephone communication;deaf;French Cued Speech;HMM;concatenative feature fusion;automatic recognition","Shape;Speech recognition;Speech;Hidden Markov models;Accuracy;Lips;Visualization","handicapped aids;Internet telephony;natural language processing;speech processing;speech recognition;speech synthesis;telephone equipment","visual-speech to text conversion;telephone communication technology;handicapped people;automatic translation system;auditory speech;deaf user;normal hearing people;telephone network;Internet;telephone device;automatic recognition;Cued speech;lip reading;spoken language;isolated word recognition rate;continuous phoneme recognition","","4","","12","","20 Jun 2011","","","IEEE","IEEE Conferences"
"Early and late integration of audio features for automatic video description","C. Hori; T. Hori; T. K. Marks; J. R. Hershey","Mitsubishi Electric Research Laboratories (MERL), 201 Broadway, Cambridge, MA 02139, USA; Mitsubishi Electric Research Laboratories (MERL), 201 Broadway, Cambridge, MA 02139, USA; Mitsubishi Electric Research Laboratories (MERL), 201 Broadway, Cambridge, MA 02139, USA; Mitsubishi Electric Research Laboratories (MERL), 201 Broadway, Cambridge, MA 02139, USA","2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","25 Jan 2018","2017","","","430","436","This paper presents our approach to improve video captioning by integrating audio and video features. Video captioning is the task of generating a textual description to describe the content of a video. State-of-the-art approaches to video captioning are based on sequence-to-sequence models, in which a single neural network accepts sequential images and audio data, and outputs a sequence of words that best describe the input data in natural language. The network thus learns to encode the video input into an intermediate semantic representation, which can be useful in applications such as multimedia indexing, automatic narration, and audio-visual question answering. In our prior work, we proposed an attention-based multi-modal fusion mechanism to integrate image, motion, and audio features, where the multiple features are integrated in the network. Here, we apply hypothesis-level integration based on minimum Bayes-risk (MBR) decoding to further improve the caption quality, focusing on well-known evaluation metrics (BLEU and METEOR scores). Experiments with the YouTube2Text and MSR-VTT datasets demonstrate that combinations of early and late integration of multimodal features significantly improve the audio-visual semantic representation, as measured by the resulting caption quality. In addition, we compared the performance of our method using two different types of audio features: MFCC features, and the audio features extracted using SoundNet, which was trained to recognize objects and scenes from videos using only the audio signals.","","978-1-5090-4788-8","10.1109/ASRU.2017.8268968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268968","video description;audio feature;SoundNet;MFCC;encoder-decoder;deep learning","Feature extraction;Decoding;Generators;Task analysis;Measurement;Mel frequency cepstral coefficient;Fuses","audio signal processing;audio-visual systems;Bayes methods;feature extraction;indexing;language translation;learning (artificial intelligence);natural language processing;neural nets;object recognition;speech recognition;text analysis;video signal processing","video captioning;sequence-to-sequence models;single neural network;audio data;video input;audio-visual question answering;audio features;multiple features;hypothesis-level integration;multimodal features;audio-visual semantic representation;MFCC features;videos;audio signals;automatic video description;integrating audio;caption quality;natural language;multimedia indexing;automatic narration","","6","","24","","25 Jan 2018","","","IEEE","IEEE Conferences"
"Using Automatic Speech Recognition and Speech Synthesis to Improve the Intelligibility of Cochlear Implant users in Reverberant Listening Environments","K. Chu; L. Collins; B. Mainsah","Duke University,Department of Electrical and Computer Engineering,Durham,NC,USA; Duke University,Department of Electrical and Computer Engineering,Durham,NC,USA; Duke University,Department of Electrical and Computer Engineering,Durham,NC,USA","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","6929","6933","Cochlear implant (CI) users experience substantial difficulties in understanding reverberant speech. A previous study proposed a strategy that leverages automatic speech recognition (ASR) to recognize reverberant speech and speech synthesis to translate the recognized text into anechoic speech. However, the strategy was trained and tested on the same reverberant environment, so it is unknown whether the strategy is robust to unseen environments. Thus, the current study investigated the performance of the previously proposed algorithm in multiple unseen environments. First, an ASR system was trained on anechoic and reverberant speech using different room types. Next, a speech synthesizer was trained to generate speech from the text predicted by the ASR system. Experiments were conducted in normal hearing listeners using vocoded speech, and the results showed that the strategy improved speech intelligibility in previously unseen conditions. These results suggest that the ASR-synthesis strategy can potentially benefit CI users in everyday reverberant environments.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9054450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054450","Automatic speech recognition;Cochlear implants;Reverberation;Speech synthesis","Cochlear implants;Text recognition;Auditory system;Acoustics;User experience;Speech synthesis;Automatic speech recognition","cochlear implants;hearing;medical signal processing;reverberation;speech intelligibility;speech processing;speech recognition;speech synthesis;vocoders","reverberant listening environments;cochlear implant users;speech intelligibility;vocoded speech;speech synthesizer;ASR system;anechoic speech;automatic speech recognition;reverberant speech","","","","32","","9 Apr 2020","","","IEEE","IEEE Conferences"
"MASKER: Adaptive Mobile Security Enhancement against Automatic Speech Recognition in Eavesdropping","F. Yu; Z. Xu; C. Liu; X. Chen","George Mason University, Fairfax, Virginia; George Mason University, Fairfax, Virginia; George Mason University, Fairfax, Virginia; George Mason University, Fairfax, Virginia","2019 56th ACM/IEEE Design Automation Conference (DAC)","22 Aug 2019","2019","","","1","6","Benefited from recent artificial intelligence evolution, Automatic Speech Recognition (ASR) technology has achieved enormous performance improvement and wider application. Unfortunately, ASR is also heavily leveraged by speech eavesdropping, where ASR is used to translate large volume of intercepted vocal speech into text content, causing considerable information leakage. In this work, we propose MASKER - a mobile security enhancement solution to protect the mobile speech data from ASR in eavesdropping. By identifying ASR models' ubiquitous vulnerability, MASKER is designed to generate human imperceptible adversarial noises into the real-time speech on the mobile device (e.g. phone call and voice message). Even the speech data is exposed to eavesdropping during data transmission, the adversarial noises can effectively perturb the ASR process with significant Word Error Rate (WER). Meanwhile, MASKER is further optimized for mobile user perception quality and enhanced for environmental noises adaptation. Moreover, MASKER has outstanding computation efficiency for mobile system integration. Experiments show that, MASKER can achieve security enhancement with an average WER of 84.55% for ASR perturbation, 32% noise reduction for user perception quality and 16× faster processing speed compared to the state-of-the-art method.","0738-100X","978-1-4503-6725-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807068","Automatic Speech Recognition;Adversarial Example","Mel frequency cepstral coefficient;Eavesdropping;Speech processing;Perturbation methods;Automatic speech recognition;Computational modeling","learning (artificial intelligence);mobile computing;speech recognition;telecommunication security","artificial intelligence evolution;automatic speech recognition technology;information leakage;adaptive mobile security enhancement;text content;word error rate;WER;mobile user perception quality;environmental noises adaptation;MASKER;mobile system integration;speech eavesdropping;intercepted vocal speech;mobile speech data;human imperceptible adversarial noises;data transmission;ASR process","","","","23","","22 Aug 2019","","","IEEE","IEEE Conferences"
"A Novel Method of Language Modeling for Automatic Captioning in TC Video Teleconferencing","X. Zhang; Y. Zhao; L. Schopp","Dept. of Comput. Sci., Missouri Univ., Columbia, MO; Dept. of Comput. Sci., Missouri Univ., Columbia, MO; NA","IEEE Transactions on Information Technology in Biomedicine","30 Apr 2007","2007","11","3","332","337","We are developing an automatic captioning system for teleconsultation video teleconferencing (TC-VTC) in telemedicine, based on large vocabulary conversational speech recognition. In TC-VTC, doctors' speech contains a large number of infrequently used medical terms in spontaneous styles. Due to insufficiency of data, we adopted mixture language modeling, with models trained from several datasets of medical and nonmedical domains. This paper proposes novel modeling and estimation methods for the mixture language model (LM). Component LMs are trained from individual datasets, with class n-gram LMs trained from in-domain datasets and word n-gram LMs trained from out-of-domain datasets, and they are interpolated into a mixture LM. For class LMs, semantic categories are used for class definition on medical terms, names, and digits. The interpolation weights of a mixture LM are estimated by a greedy algorithm of forward weight adjustment (FWA). The proposed mixing of in-domain class LMs and out-of-domain word LMs, the semantic definitions of word classes, as well as the weight-estimation algorithm of FWA are effective on the TC-VTC task. As compared with using mixtures of word LMs with weights estimated by the conventional expectation-maximization algorithm, the proposed methods led to a 21% reduction of perplexity on test sets of five doctors, which translated into improvements of captioning accuracy","1558-0032","","10.1109/TITB.2006.885549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4167905","Automatic speech recognition;mixture language model (LM);telemedicine;teleconsultation (TC);video teleconferencing","Teleconferencing;Natural languages;Telemedicine;Automatic speech recognition;Vocabulary;Speech recognition;Parameter estimation;Predictive models;Probability;Interpolation","expectation-maximisation algorithm;greedy algorithms;interpolation;linguistics;natural language processing;speech recognition;teleconferencing;telemedicine","mixture language modeling;teleconsultation video teleconferencing;automatic captioning system;telemedicine;large vocabulary conversational speech recognition;medical terms;semantic categories;greedy algorithm;forward weight adjustment;semantic definitions;word classes;weight-estimation algorithm;expectation-maximization algorithm;captioning accuracy;interpolation weights","Artificial Intelligence;Documentation;Language;Natural Language Processing;Pattern Recognition, Automated;Remote Consultation;Speech Recognition Software;Video Recording;Videoconferencing;Vocabulary, Controlled","5","1","13","","30 Apr 2007","","","IEEE","IEEE Journals"
"Automatic Image Registration Through Image Segmentation and SIFT","H. Goncalves; L. Corte-Real; J. A. Goncalves","Departamento de Geociências, Ambiente e Ordenamento do Território, Faculdade de Ciências, Universidade do Porto, Porto, Portugal; Departamento de Engenharia Electrotécnica e de Computadores, Faculdade de Engenharia, Universidade do Porto, Porto, Portugal; Departamento de Geociências, Ambiente e Ordenamento do Território, Faculdade de Ciências, Universidade do Porto, Porto, Portugal","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2011","2011","49","7","2589","2600","Automatic image registration (AIR) is still a present challenge for the remote sensing community. Although a wide variety of AIR methods have been proposed in the last few years, there are several drawbacks which avoid their common use in practice. The recently proposed scale invariant feature transform (SIFT) approach has already revealed to be a powerful tool for the obtention of tie points in general image processing tasks, but it has a limited performance when directly applied to remote sensing images. In this paper, a new AIR method is proposed, based on the combination of image segmentation and SIFT, complemented by a robust procedure of outlier removal. This combination allows for an accurate obtention of tie points for a pair of remote sensing images, being a powerful scheme for AIR. Both synthetic and real data have been considered in this work for the evaluation of the proposed methodology, comprising medium and high spatial resolution images, and single-band, multispectral, and hyperspectral images. A set of measures which allow for an objective evaluation of the geometric correction process quality has been used. The proposed methodology allows for a fully automatic registration of pairs of remote sensing images, leading to a subpixel accuracy for the whole considered data set. Furthermore, it is able to account for differences in spectral content, rotation, scale, translation, different viewpoint, and change in illumination.","1558-0644","","10.1109/TGRS.2011.2109389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734838","Automatic image registration (AIR);image segmentation;optical images;scale invariant feature transform (SIFT)","Image segmentation;Remote sensing;Satellites;Feature extraction;Histograms;Pixel;Image registration","geophysical image processing;image registration;image resolution;image segmentation;remote sensing","automatic image registration;image segmentation;SIFT;remote sensing images;scale invariant feature transform approach;image processing;outlier removal procedure;high spatial resolution images;hyperspectral images;multispectral images;single-band images","","130","","32","","17 Mar 2011","","","IEEE","IEEE Journals"
"Automatic space calibration of a medical robot system","G. Liu; C. Li; G. Li; X. Yu; L. Li","State Key Laboratory of Robotic Technology and System, Harbin Institute of Technology, Harbin, Heilongjiang Province 150080, China; State Key Laboratory of Robotic Technology and System, Harbin Institute of Technology, Harbin, Heilongjiang Province 150080, China; State Key Laboratory of Robotic Technology and System, Harbin Institute of Technology, Harbin, Heilongjiang Province 150080, China; State Key Laboratory of Robotic Technology and System, Harbin Institute of Technology, Harbin, Heilongjiang Province 150080, China; School of Electrical Engineering and Automation, Harbin Institute of Technology, Harbin, Heilongjiang Province 150080, China","2016 IEEE International Conference on Mechatronics and Automation","5 Sep 2016","2016","","","465","470","In order to build an uniform coordinate system for a medical robot and its vision system in surgery, an automatic space calibration method is proposed based on the rotational and translational movements of the robot. With this method, both the robot and the NDI vision system can be calibrated to the patient's bones contour, which matches the virtual model built from the CT images. Thus surgical operation can be carried out by the robot following the preoperative design based on the virtual model. The calibration accuracy directly determines the precision of surgical operation. Experiments were carried out to evaluate the effectiveness of this calibration method, and the result showed that the position and posture error can meet the operation requirements. Also the calibration can be executed automatically to decrease the human effect and increase the reliability and stability.","2152-744X","978-1-5090-2396-7","10.1109/ICMA.2016.7558608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558608","Space calibration;Virtual model;Automatic calibration;Preoperative design;Error analysis","Calibration;Robot kinematics;Biomedical optical imaging;Medical robotics;Adaptive optics;Nonlinear optics","computer vision;computerised tomography;medical image processing;medical robotics","medical robot system;uniform coordinate system;automatic space calibration method;NDI vision system;CT images;virtual model;surgical operation","","1","","10","","5 Sep 2016","","","IEEE","IEEE Conferences"
"Evaluating Open-source Toolkits for Automatic Speech Recognition of South African Languages","A. Naidoo; M. Tsoeu","Dept. of Electrical Engineering, University of Cape Town, Cape Town, South Africa; Dept. of Electrical Engineering, University of Cape Town, Cape Town, South Africa","2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa (SAUPEC/RobMech/PRASA)","2 May 2019","2019","","","160","165","Automatic speech recognition is a critical component of human language technologies. It concerns the translation of speech into textual data which can be processed by computers. Thus, it offers the creation of an intimate link allowing humans to interact with machines on a completely natural level. A variety of open-source toolkits exist for the development of these systems. These toolkits have been successfully implemented and tested for use on well-resourced languages. However, the same level of testing has not been performed for South African languages. This investigation sets out to evaluate popular open-source tools for South African languages and identify optimal toolkit configurations for each language and toolkit. The NCHLT corpora were used to set up automatic speech recognition systems for English and isiXhosa using Kaldi, CMU Sphinx, and HTK. The word error rates achieved during this investigation showed that the best configurations from this investigation achieved better performance than those which were reported by the developers of the NCHLT corpus.","","978-1-7281-0369-3","10.1109/RoboMech.2019.8704774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704774","automatic speech recognition;under-resourced;evaluation;languages;isiXhosa;English","Hidden Markov models;Acoustics;Tools;Feature extraction;Decoding;Open source software;Speech recognition","natural language processing;public domain software;speech recognition","open-source toolkits;South African languages;human language technologies;completely natural level;well-resourced languages;open-source tools;optimal toolkit configurations;automatic speech recognition systems;NCHLT corpora;isiXhosa;Kaldi;CMU Sphinx;HTK","","","","13","","2 May 2019","","","IEEE","IEEE Conferences"
"Multi-Instrument Automatic Music Transcription With Self-Attention-Based Instance Segmentation","Y. -T. Wu; B. Chen; L. Su","Department of Computer Science and Information Engineering, National Taiwan Normal University, Taiepi, Taiwan; Department of Computer Science and Information Engineering, National Taiwan Normal University, Taiepi, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","27 Oct 2020","2020","28","","2796","2809","Multi-instrument automatic music transcription (AMT) is a critical but less investigated problem in the field of music information retrieval (MIR). With all the difficulties faced by traditional AMT research, multi-instrument AMT needs further investigation on high-level music semantic modeling, efficient training methods for multiple attributes, and a clear problem scenario for system performance evaluation. In this article, we propose a multi-instrument AMT method, with signal processing techniques specifying pitch saliency, novel deep learning techniques, and concepts partly inspired by multi-object recognition, instance segmentation, and image-to-image translation in computer vision. The proposed method is flexible for all the sub-tasks in multi-instrument AMT, including multi-instrument note tracking, a task that has rarely been investigated before. State-of-the-art performance is also reported in the sub-task of multi-pitch streaming.","2329-9304","","10.1109/TASLP.2020.3030482","MOST Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222310","Automatic music transcription;deep learning;multi-pitch estimation;multi-pitch streaming;self-attention","Instruments;Task analysis;Music;Multiple signal classification;Hidden Markov models;Speech processing;Deep learning","computer vision;image segmentation;information retrieval;learning (artificial intelligence);music;object recognition","MIR;self-attention-based instance segmentation;multiinstrument automatic music transcription;multipitch streaming;multiinstrument note tracking;multiobject recognition;multiinstrument AMT method;training methods;traditional AMT research;music information retrieval","","","","67","IEEE","13 Oct 2020","","","IEEE","IEEE Journals"
"Automatic Spelling Correction for ASR Corpus in Traditional Chinese Language using Seq2Seq Models","Y. -C. Chao; C. -H. Chang","National Central University,Computer Science & Information Engineering,Taiwan; National Central University,Computer Science & Information Engineering,Taiwan","2020 International Computer Symposium (ICS)","23 Feb 2021","2020","","","553","558","The goal of Automatic Speech Recognition (ASR) service is to translate spoken language into text. There exist many factors that will degrade the performance of the ASR system, such as environmental noise, human pronunciation, etc. This research focuses on automatic spelling correction for traditional Chinese corpus generated by ASR systems. We show that a Sequence to Sequence (Seq2Seq) neural network model with attention mechanism could be improved by adding pointer network with auxiliary phoneme features of the input word sequence.","","978-1-7281-9255-0","10.1109/ICS51289.2020.00113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359053","automatic speech recognition, spelling check, sequence to sequence model, pointer network","Integrated circuits;Working environment noise;Neural networks;Training data;Computer architecture;Data models;Task analysis","natural language processing;neural nets;speech recognition","automatic spelling correction;ASR corpus;Chinese language;Seq2Seq models;Automatic Speech Recognition service;spoken language;ASR system;environmental noise;human pronunciation;Chinese corpus;sequence to sequence neural network model;auxiliary phoneme features;input word sequence","","","","9","","23 Feb 2021","","","IEEE","IEEE Conferences"
"Unsupervised vocabulary selection for real-time speech recognition of lectures","P. Maergner; A. Waibel; I. Lane","Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","4417","4420","In this work, we propose a novel method for vocabulary selection to automatically adapt automatic speech recognition systems to the diverse topics that occur in educational and scientific lectures. Utilizing materials that are available before the lecture begins, such as lecture slides, our proposed framework iteratively searches for related documents on the web and generates a lecture-specific vocabulary based on the resulting documents. In this paper, we propose a novel method for vocabulary selection where we first collect documents similar to an initial seed document and then rank the resulting vocabulary based on a score which is calculated using a combination of word features. This is a critical component for adaptation that has typically been overlooked in prior works. On the inter ACT German-English simultaneous lecture translation system our proposed approach significantly improved vocabulary coverage, reducing the out-of-vocabulary rate, on average by 57.0% and up to 84.9%, compared to a lecture-independent baseline. Furthermore, our approach reduced the word error rate, by 12.5% on average and up to 25.3%, compared to a lecture-independent baseline.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6288899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6288899","Vocabulary selection;automatic speech recognition;language model adaptation","Vocabulary;Adaptation models;Real time systems;Speech;Accuracy;Automatic speech recognition","educational administrative data processing;natural languages;speech recognition;vocabulary","unsupervised vocabulary selection;real-time speech recognition;automatic speech recognition systems;educational lectures;scientific lectures;lecture-specific vocabulary;ACT German-English simultaneous lecture translation system;lecture-independent baseline","","7","","7","","30 Aug 2012","","","IEEE","IEEE Conferences"
"HAIRIS: A Method for Automatic Image Registration Through Histogram-Based Image Segmentation","H. Gonçalves; J. A. Gonçalves; L. Corte-Real","Departamento de Geociências, Ambiente e Ordenamento do Território, Faculdade de Ciências, Universidade do Porto, Porto, Portugal; Departamento de Geociências, Ambiente e Ordenamento do Território, Faculdade de Ciências, Universidade do Porto, Porto, Portugal; Departamento de Engenharia Electrotécnica e de Computadores, Faculdade de Engenharia, Universidade do Porto","IEEE Transactions on Image Processing","14 Feb 2011","2011","20","3","776","789","Automatic image registration is still an actual challenge in several fields. Although several methods for automatic image registration have been proposed in the last few years, it is still far from a broad use in several applications, such as in remote sensing. In this paper, a method for automatic image registration through histogram-based image segmentation (HAIRIS) is proposed. This new approach mainly consists in combining several segmentations of the pair of images to be registered, according to a relaxation parameter on the histogram modes delineation (which itself is a new approach), followed by a consistent characterization of the extracted objects-through the objects area, ratio between the axis of the adjust ellipse, perimeter and fractal dimension-and a robust statistical based procedure for objects matching. The application of the proposed methodology is illustrated to simulated rotation and translation. The first dataset consists in a photograph and a rotated and shifted version of the same photograph, with different levels of added noise. It was also applied to a pair of satellite images with different spectral content and simulated translation, and to real remote sensing examples comprising different viewing angles, different acquisition dates and different sensors. An accuracy below 1° for rotation and at the subpixel level for translation were obtained, for the most part of the considered situations. HAIRIS allows for the registration of pairs of images (multitemporal and multisensor) with differences in rotation and translation, with small differences in the spectral content, leading to a subpixel accuracy.","1941-0042","","10.1109/TIP.2010.2076298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5570999","Histogram;image registration;image segmentation;matching;Wiener filtering","Image segmentation;Image registration;Remote sensing;Histograms;Satellites;Noise;Degradation","image registration;image segmentation;object detection;remote sensing;statistical analysis","automatic image registration;histogram-based image segmentation;relaxation parameter;histogram modes delineation;object area;fractal dimension;statistical procedure;object matching;rotation;translation;photograph;added noise;satellite images;remote sensing;viewing angles;acquisition dates;sensors;HAIRIS;spectral content;subpixel accuracy","","47","","45","","13 Sep 2010","","","IEEE","IEEE Journals"
"Rule-Based Pronunciation Models to Handle OOV Words for Indonesian Automatic Speech Recognition System","F. Y. Putri; D. Hoesen; D. P. Lestari","Prosa.ai,Bandung,Indonesia; Prosa.ai,Bandung,Indonesia; Institut Teknologi Bandung,Department of Informatics,Bandung,Indonesia","2019 5th International Conference on Science in Information Technology (ICSITech)","10 Feb 2020","2019","","","246","251","A representative pronunciation dictionary becomes a necessity to cover large vocabulary in many domains. While creating a hand designed pronunciation dictionary in an extensive corpus is expensive and time-consuming, a rule-based pronunciation generation method is considerably effective to be applied in Indonesian language due to simpler graphemetophoneme mapping. In this paper, we propose a rule based G2P translation including an efficient way to handle abbreviation and word splitting. The accuracy of the proposed G2P translation achieved 93.65% for Indonesian word entries, and 87.29% for the overall entries that contain abbreviation and words from other language. The addition of rule based G2P translated OOV words to the dictionary has comparable speech recognition system performance with the hand designed dictionary. With the same amount of OOV addition, the speech recognition system performance is reduced by only 0.27 percent when using the rule-based G2P translated dictionary compared to the hand designed dictionary. From the experiment results, generating a dictionary for the OOV words with the proposed G2P mapping is considerably efficient without a big gap in the speech recognition performance score while reducing great amount of time and resources.","","978-1-7281-2380-6","10.1109/ICSITech46713.2019.8987472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8987472","pronunciation dictionary;rule-based;Indonesian;OOV;ASR","","dictionaries;natural language processing;speech recognition;vocabulary","graphemetophoneme mapping;OOV addition;comparable speech recognition system performance;Indonesian word entries;word splitting;abbreviation;Indonesian language;rule-based pronunciation generation method;extensive corpus;hand designed pronunciation dictionary;representative pronunciation dictionary;Indonesian automatic speech recognition system;rule-based pronunciation models;speech recognition performance score;OOV words;hand designed dictionary;rule-based G2P translated dictionary","","","","12","","10 Feb 2020","","","IEEE","IEEE Conferences"
"Mechatronic Implementation and Trajectory Tracking Validation of a BCI-based Human-wheelchair Interface","J. -W. Chen; C. -J. Wu; Y. -T. Lin; Y. -C. Kuo; C. -H. Kuo","National Taiwan University of Science and Technology,Department of Electrical Engineering,Taipei,Taiwan,106; National Taiwan University of Science and Technology,Department of Electrical Engineering,Taipei,Taiwan,106; National Taiwan University of Science and Technology,Department of Electrical Engineering,Taipei,Taiwan,106; National Taiwan University of Science and Technology,Department of Electrical Engineering,Taipei,Taiwan,106; National Taiwan University of Science and Technology,Department of Electrical Engineering,Taipei,Taiwan,106","2020 8th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)","15 Oct 2020","2020","","","304","309","This paper presents a mechatronic P300-based brain computer interface (BCI) for wheelchair control applications. A translucent visual stimulus panel (TVSP) is set up in front of the wheelchair to provide an intuitive P300 visual stimulus operation as well as to realize the see-through scene during operating wheelchairs. In this research, a micro projector is utilized to produce flickering visual stimuli on the display board which is 35cm away from the user. To improve the information transfer rate (ITR), a spatial filter based on Canonical Correlation Analysis (CCA) and Support Vector Machine (SVM) were also applied to this work to improve the performance of BCI classification. The result of experiments showed that the proposed BCI is with 88.2% in accuracy and 22.97 bits/min information transfer rate in average received from ten subjects. In ground truth experiments of practical trajectory tracking, the root mean squared error (RMSE) of P300 BCI are 12.11cm in “U” trajectory test.","2155-1782","978-1-7281-5907-2","10.1109/BioRob49111.2020.9224373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224373","Brain-machine interfaces;Human-machine interfaces;Algorithms and machine learning","Brain-computer interfaces;Visualization;Training;Spatial filters;Electroencephalography;Correlation;Wheelchairs","bioelectric potentials;brain-computer interfaces;electroencephalography;handicapped aids;mechatronics;medical signal processing;spatial filters;support vector machines;visual evoked potentials;wheelchairs","trajectory tracking validation;BCI-based human-wheelchair interface;mechatronic P300-based brain computer interface;wheelchair control applications;translucent visual stimulus panel;intuitive P300 visual stimulus operation;microprojector;display board;canonical correlation analysis;support vector machine;BCI classification;P300 BCI;trajectory test;root mean squared error;flickering visual stimuli;spatial filter;information transfer rate;distance 35.0 cm","","","","13","","15 Oct 2020","","","IEEE","IEEE Conferences"
"MaRePhoR — An open access machine-readable phonetic dictionary for Romanian","Ş. Toma; A. Stan; M. Pura; T. Bârsan","Computer Science Department, Military Technical Academy, Bucharest, Romania; Communications Department, Technical University of Cluj-Napoca, Romania; Computer Science Department, Military Technical Academy, Bucharest, Romania; Computer Science Department, Military Technical Academy, Bucharest, Romania","2017 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)","27 Jul 2017","2017","","","1","6","This paper introduces a novel open access resource, the machine-readable phonetic dictionary for Romanian - MaRePhoR. It contains over 70,000 word entries, and their manually performed phonetic transcription. The paper describes the dictionary format and statistics, as well as an initial use of the phonetic transcription entries by building a grapheme to phoneme converter based on decision trees. Various training strategies were tested enabling the correct selection of a final setup for our predictor. The best results showed that using the dictionary as training data, an accuracy of over 99% can be achieved.","","978-1-5090-6497-7","10.1109/SPED.2017.7990435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990435","phonetic dictionary;phoneme;grapheme;phonetic transcription;open access","Dictionaries;Decision trees;Open Access;Stress;Classification algorithms;Prediction algorithms;Training","decision trees;language translation;natural language processing;public domain software;speech recognition","MaRePhoR;open access machine-readable phonetic dictionary;open access resource;Romanian language;manually performed phonetic transcription;dictionary format;statistics;phonetic transcription entries;grapheme to phoneme converter;decision trees;training strategies","","2","","21","","27 Jul 2017","","","IEEE","IEEE Conferences"
"Noise Adaptive Training for Robust Automatic Speech Recognition","O. Kalinli; M. L. Seltzer; J. Droppo; A. Acero","R&D Group, Sony Comput. Entertainment of America, Foster City, CA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA","IEEE Transactions on Audio, Speech, and Language Processing","30 Aug 2010","2010","18","8","1889","1901","In traditional methods for noise robust automatic speech recognition, the acoustic models are typically trained using clean speech or using multi-condition data that is processed by the same feature enhancement algorithm expected to be used in decoding. In this paper, we propose a noise adaptive training (NAT) algorithm that can be applied to all training data that normalizes the environmental distortion as part of the model training. In contrast to feature enhancement methods, NAT estimates the underlying “pseudo-clean” model parameters directly without relying on point estimates of the clean speech features as an intermediate step. The pseudo-clean model parameters learned with NAT are later used with vector Taylor series (VTS) model adaptation for decoding noisy utterances at test time. Experiments performed on the Aurora 2 and Aurora 3 tasks demonstrate that the proposed NAT method obtain relative improvements of 18.83% and 32.02%, respectively, over VTS model adaptation.","1558-7924","","10.1109/TASL.2010.2040522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439889","Model adaptation;noise adaptive training;robust speech recognition;vector Taylor series (VTS)","Noise robustness;Automatic speech recognition;Network address translation;Acoustic noise;Speech enhancement;Decoding;Adaptation model;Speech processing;Working environment noise;Training data","series (mathematics);speech coding;speech enhancement;speech recognition","noise adaptive training;automatic speech recognition;feature enhancement;decoding;environmental distortion;pseudoclean model parameters;vector Taylor series model;NAT method","","51","4","30","","29 Mar 2010","","","IEEE","IEEE Journals"
"A Progressively-Trained Scale-Invariant and Boundary-Aware Deep Neural Network for the Automatic 3D Segmentation of Lung Lesions","B. Zhou; R. Crawford; B. Dogdas; G. Goldmacher; A. Chen","Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Image Data Analytics, Merck & Co., Inc., West Point, PA, USA; Image Data Analytics, Merck & Co., Inc., Rahway, NJ, USA; Translational BiomarkersMerck & Co., Inc.West Point, PA, USA; Image Data AnalyticsMerck & Co., Inc.West Point, PA, USA","2019 IEEE Winter Conference on Applications of Computer Vision (WACV)","7 Mar 2019","2019","","","1","10","Volumetric segmentation of lesions on CT scans is important for many types of analysis, including lesion growth kinetic modeling in clinical trials and machine learning of radiomic features. Manual segmentation is laborious, and impractical for large-scale use. For routine clinical use, and in clinical trials that apply the Response Evaluation Criteria In Solid Tumors (RECIST), clinicians typically outline the boundaries of a lesion on a single slice to extract diameter measurements. In this work, we have collected a large-scale database, named LesionVis, with pixel-wise manual 2D lesion delineations on the RECIST-slices. To extend the 2D segmentations to 3D, we propose a volumetric progressive lesion segmentation (PLS) algorithm to automatically segment the 3D lesion volume from 2D delineations using a scale-invariant and boundary-aware deep convolutional network (SIBA-Net). The SIBA-Net copes with the size transition of a lesion when the PLS progresses from the RECIST-slice to the edge-slices, as well as when performing longitudinal assessment of lesions whose size change over multiple time points. The proposed PLS-SiBA-Net (P-SiBA) approach is assessed on the lung lesion cases from LesionVis. Our experimental results demonstrate that the P-SiBA approach achieves mean Dice similarity coefficients (DSC) of 0.81, which significantly improves 3D segmentation accuracy compared with the approaches proposed previously (highest mean DSC at 0.78 on LesionVis). In summary, by leveraging the limited 2D delineations on the RECIST-slices, P-SiBA is an effective semi-supervised approach to produce accurate lesion segmentations in 3D.","1550-5790","978-1-7281-1975-5","10.1109/WACV.2019.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8659070","","Lesions;Two dimensional displays;Three-dimensional displays;Image segmentation;Computed tomography;Training;Training data","computerised tomography;image segmentation;lung;medical image processing;neural nets;supervised learning;tumours","PLS-SiBA-Net approach;P-SiBA;3D segmentation accuracy;RECIST-slice;lesion growth kinetic modeling;clinical trials;manual segmentation;single slice;large-scale database;volumetric progressive lesion segmentation algorithm;SIBA-Net copes;edge-slices;lung lesion;boundary-aware deep neural network;progressively-trained scale-invariant deep neural network;automatic 3D segmentation;CT scans;computed tomography;pixel-wise manual 2D lesion delineations;longitudinal assessment;LesionVis;Dice similarity coefficients;machine learning;effective semisupervised approach;response evaluation criteria in solid tumors","","1","","26","","7 Mar 2019","","","IEEE","IEEE Conferences"
"Fast and Fully Automatic Left Ventricular Segmentation and Tracking in Echocardiography Using Shape-Based B-Spline Explicit Active Surfaces","J. Pedrosa; S. Queirós; O. Bernard; J. Engvall; T. Edvardsen; E. Nagel; J. D’hooge","Laboratory on Cardiovascular Imaging and Dynamics, KU Leuven, Leuven, Belgium; Laboratory on Cardiovascular Imaging and Dynamics, KU Leuven, Leuven, Belgium; CREATIS, Centre national de la recherche scientifique, Inserm, INSA-Lyon, University of Lyon, University of Lyon 1, Villeurbanne, France; Department of Clinical Physiology and the Department of Medical and Health Sciences, Linköping University, Linköping, Sweden; Center for Heart Failure Research, University of Oslo, Oslo, Norway; DZHK Centre for Cardiovascular Imaging, Institute for Experimental and Translational Cardiovascular Imaging, University Hospital Frankfurt/Main, Frankfurt, Germany; Laboratory on Cardiovascular Imaging and Dynamics, KU Leuven, Leuven, Belgium","IEEE Transactions on Medical Imaging","27 Oct 2017","2017","36","11","2287","2296","Cardiac volume/function assessment remains a critical step in daily cardiology, and 3-D ultrasound plays an increasingly important role. Fully automatic left ventricular segmentation is, however, a challenging task due to the artifacts and low contrast-to-noise ratio of ultrasound imaging. In this paper, a fast and fully automatic framework for the full-cycle endocardial left ventricle segmentation is proposed. This approach couples the advantages of the B-spline explicit active surfaces framework, a purely image information approach, to those of statistical shape models to give prior information about the expected shape for an accurate segmentation. The segmentation is propagated throughout the heart cycle using a localized anatomical affine optical flow. It is shown that this approach not only outperforms other state-of-the-art methods in terms of distance metrics with a mean average distances of 1.81±0.59 and 1.98±0.66 mm at end-diastole and end-systole, respectively, but is computationally efficient (in average 11 s per 4-D image) and fully automatic.","1558-254X","","10.1109/TMI.2017.2734959","European Research Council under the European Union’s Seventh Framework Program (HeartMAPAS; FP7/2007-2013/ERC); FCT Fundação para a Ciência e a Tecnologia, Portugal, in the scope of the Ph.D. Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000408","3-D echocardiography;left ventricle segmentation;B-spline explicit active surfaces;statistical shape model;localized anatomical affine optical flow","Shape;Image segmentation;Splines (mathematics);Three-dimensional displays;Imaging;Ultrasonic imaging;Minimization","echocardiography;image segmentation;medical image processing;splines (mathematics);ultrasonic imaging","fully automatic left ventricular segmentation;left ventricular tracking;echocardiography;shape-based B-spline explicit active surfaces;cardiac volume-function assessment;cardiology;3D ultrasound imaging;endocardial left ventricle segmentation;heart cycle;localized anatomical affine optical flow","Echocardiography;Heart Ventricles;Humans;Image Processing, Computer-Assisted;Models, Statistical","11","","33","Traditional","2 Aug 2017","","","IEEE","IEEE Journals"
"Automatic Extraction of Dermatological Parameters from Nevi Using an Inexpensive Smartphone Microscope: A Proof of Concept","K. M. Meiburger; F. Veronese; V. Tarantino; M. Salvi; M. Fadda; S. Seoni; E. Zavattaro; B. D. Santi; N. Michielli; P. Savoia; F. Molinari","PoliToBIOMed Lab, Politecnico di Torino, Torino, Italy; Dermatology Unit, Dept. of Health Science, University of Eastern Piedmont, Novara, Italy; Dermatology Unit, Dept. of Health Science, University of Eastern Piedmont, Novara, Italy; PoliToBIOMed Lab, Politecnico di Torino, Torino, Italy; PoliToBIOMed Lab, Politecnico di Torino, Torino, Italy; PoliToBIOMed Lab, Politecnico di Torino, Torino, Italy; Dermatology Unit, Dept. of Translational Medicine, University of Eastern Piedmont, Novara, Italy; PoliToBIOMed Lab, Politecnico di Torino, Torino, Italy; PoliToBIOMed Lab, Politecnico di Torino, Torino, Italy; Dermatology Unit, Dept. of Health Science, University of Eastern Piedmont, Novara, Italy; PoliToBIOMed Lab, Politecnico di Torino, Torino, Italy","2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","7 Oct 2019","2019","","","399","402","The evolution of smartphone technology has made their use more common in dermatological applications. Here we studied the feasibility of using an inexpensive smartphone microscope for the extraction of dermatological parameters and compared the results obtained with a portable dermoscope, commonly used in clinical practice. Forty-two skin lesions were imaged with both devices and visually analyzed by an expert dermatologist. The presence of a reticular pattern was observed in 22 dermoscopic images, but only in 10 smartphone images. The proposed paradigm segments the image and extracts texture features which are used to train and validate a neural network to classify the presence of a reticular pattern. Using 5-fold cross-validation, an accuracy of 100% and 95% was obtained with the dermoscopic and smartphone images, respectively. This approach can be useful for general practitioners and as a triage tool for skin lesion analysis.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8856720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8856720","","Image segmentation;Feature extraction;Artificial neural networks;Lesions;Microscopy;Melanoma","biomedical communication;biomedical optical imaging;cancer;feature extraction;image classification;image segmentation;image texture;medical image processing;skin;smart phones","skin lesion analysis;automatic dermatological parameter extraction;dermatological applications;smartphone microscope;texture feature extraction;smartphone images;dermoscopic images;reticular pattern;portable dermoscope","Dermoscopy;Humans;Nevus;Skin Diseases;Smartphone","","","12","","7 Oct 2019","","","IEEE","IEEE Conferences"
"TRACE: A Topological Graph Representation for Automatic Sulcal Curve Extraction","I. Lyu; S. H. Kim; N. D. Woodward; M. A. Styner; B. A. Landman","Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA; Department of Psychiatry, The University of North Carolina, Chapel Hill, NC, USA; Department of Psychiatry, Vanderbilt University, Nashville, TN, USA; Department of Psychiatry, The University of North Carolina, Chapel Hill, NC, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN, USA","IEEE Transactions on Medical Imaging","29 Jun 2018","2018","37","7","1653","1663","A proper geometric representation of the cortical regions is a fundamental task for cortical shape analysis and landmark extraction. However, a significant challenge has arisen due to the highly variable, convoluted cortical folding patterns. In this paper, we propose a novel topological graph representation for automatic sulcal curve extraction (TRACE). In practice, the reconstructed surface suffers from noise influences introduced during image acquisition/surface reconstruction. In the presence of noise on the surface, TRACE determines stable sulcal fundic regions by employing the line simplification method that prevents the sulcal folding pattern from being significantly smoothed out. The sulcal curves are then traced over the connected graph in the determined regions by the Dijkstra's shortest path algorithm. For validation, we used the state-of-the-art surface reconstruction pipelines on a reproducibility data set. The experimental results showed higher reproducibility and robustness to noise in TRACE than the existing method (Li et al. 2010) with over 20% relative improvement in error for both surface reconstruction pipelines. In addition, the extracted sulcal curves by TRACE were well-aligned with manually delineated primary sulcal curves. We also provided a choice of parameters to control quality of the extracted sulcal curves and showed the influences of the parameter selection on the resulting curves.","1558-254X","","10.1109/TMI.2017.2787589","National Science Foundation; National Institutes of Health; Vanderbilt Institute for Clinical and Translational Research; National Center for Research Resources; National Center for Advancing Translational Sciences; National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8240660","Cortical surface;line simplification;shortest path;sulcal curve;topological graph;valley detection","Surface reconstruction;Shape;Robustness;Mathematical model;Image reconstruction;Pipelines","data acquisition;feature extraction;image reconstruction;image registration;medical image processing","image acquisition-surface reconstruction;topological-graph-representation-for-automatic-sulcal-curve-extraction;manually delineated primary sulcal curves;extracted sulcal curves;state-of-the-art surface reconstruction pipelines;sulcal folding pattern;stable sulcal fundic regions;noise influences;convoluted cortical folding patterns;landmark extraction;cortical shape analysis;cortical regions;TRACE","Algorithms;Brain Mapping;Cerebral Cortex;Female;Humans;Image Interpretation, Computer-Assisted;Magnetic Resonance Imaging;Male","2","","53","","27 Dec 2017","","","IEEE","IEEE Journals"
"Development of semi-automatic procedure for detection and tracking of fiducial markers for orofacial kinematics during natural feeding","F. Bunyak; N. Shiraishi; K. Palaniappan; T. E. Lever; L. Avivi-Arber; K. Takahashi","Department of Computer Science, University of Missouri, Columbia, 65211, United States of America; Division of Dysphagia Rehabilitation, Graduate School of Medical and Dental Sciences, Niigata University, Japan; Department of Computer Science, University of Missouri, Columbia, 65211, United States of America; Department of Otolaryngology-Head and Neck Surgery, University of Missouri School of Medicine, Columbia, 65212, USA; Department of Prosthodontics, Faculty of Dentistry, University of Toronto, ON M5G 1G6, Canada; Division of Dysphagia Rehabilitation, Graduate School of Medical and Dental Sciences, Niigata University, Japan","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","14 Sep 2017","2017","","","580","583","Feeding is a highly complex, essential behavior for survival in all species. Characterization of feeding behaviors has implications in basic science and translational medicine. We have been developing methods to study feeding behaviors using high speed videofluoroscopy (XROMM) in rats while self-feeding radiopaque flavored kibble. The rat is a popular model in translational medicine; however, it has not been studied using this methodology. Towards this goal, we surgically implanted radiopaque fiducial markers into the skull, mandible, and tongue of rats to enable motion tracking. We are developing computer vision tools to extract kinematics and behavioral features from XROMM videos to overcome barriers of current analysis methods. By understanding feeding dynamics, we will gain basic scientific knowledge and translational insights for feeding disorders caused by neurological conditions such as ALS, Parkinson's disease, and stroke.","1558-4615","978-1-5090-2809-2","10.1109/EMBC.2017.8036891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036891","","Rats;Kinematics;Surgery;Tracking;Tongue;Three-dimensional displays","computer vision;diagnostic radiography;diseases;feature extraction;kinematics;medical disorders;medical image processing;neurophysiology;prosthetics;video signal processing","semiautomatic procedure;fiducial marker tracking;orofacial kinematics;natural feeding;feeding behaviors;translational medicine;high speed videofluoroscopy;XROMM videos;self-feeding radiopaque flavored kibble;surgical implanted radiopaque fiducial markers;tongue;skull;mandible;motion tracking;fiducial marker detection;computer vision tools;feeding dynamics;feeding disorders;neurological conditions;ALS;Parkinson disease;stroke","Animals;Biomechanical Phenomena;Fiducial Markers;Motion;Rats;Tongue","","","13","","14 Sep 2017","","","IEEE","IEEE Conferences"
"Automatic correction of motion artifacts in 4D left ventricle model reconstructed from MRI","Y. Su; M. Tan; C. Lim; S. Teo; S. K. Selvaraj; M. Wan; L. Zhong; R. Tan","Institute of High Performance Computing, A*STAR, Singapore; Institute of High Performance Computing, A*STAR, Singapore; Institute of High Performance Computing, A*STAR, Singapore; Institute of High Performance Computing, A*STAR, Singapore; Institute of High Performance Computing, A*STAR, Singapore; Institute of High Performance Computing, A*STAR, Singapore; National Heart Centre, Singapore; National Heart Centre, Singapore","Computing in Cardiology 2014","19 Feb 2015","2014","","","705","708","This paper describes a computer method to correct the shape of three-dimensional (3D) left ventricle (LV) models created from magnetic resonance imaging (MRI) data that is affected by patient motion during scanning. Three-dimensional meshes of the LV endocardial and epicardial surfaces are created from border-delineated MRI data at every time frame of the cardiac cycle to generate a time-series model of the heart. A geometrically-based approach is used to achieve smooth epicardial shapes by iterative in-plane translation of vertices in the LV model. The Principal Curvatures of the LV epicardial surfaces across multiple time frames are used to construct a shape-based optimization objective function to restore the shape of the LV via a dual-resolution semi-rigid deformation process and a free-form geometric deformation process. A limited memory quasi-Newton algorithm, L-BFGS-B, is then used to solve the optimization problem. We tested our algorithm on 9 patient-specific models and it was able to correct motion artifacts without altering the general shape of the heart, such as its asymmetrical shape. The magnitudes of in-plane translations (Δx = 0.972±0.857 mm and Δy = 1.306±1.290 mm in the x- and y-directions, respectively) are also within the range of published experimental findings. The average computational time to correct each 4D model is 6 min 34 s.","2325-8853","978-1-4799-4347-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7043140","","Biomedical imaging;Optimization;Image restoration;Magnetic resonance imaging;Abstracts;Shape;Equations","biomedical MRI;cardiology;image reconstruction;medical image processing;motion compensation;optimisation","motion artifacts automatic correction;4D left ventricle model;MRI reconstruction;magnetic resonance imaging;patient motion;3D meshes;LV endocardial surfaces;LV epicardial surfaces;Principal Curvatures;dual resolution semirigid deformation process;free form geometric deformation process;limited memory quasiNewton algorithm;optimization problem","","","","9","","19 Feb 2015","","","IEEE","IEEE Conferences"
"Automatic Detection and Recognition of Shop Name in Outdoor Signboard Images","J. Park; G. Lee; A. Lai; E. Kim; J. Lim; S. Kim; H. Yang; S. Oh","School of Electronics & Computer Engineering, Chonnam National University, Korea, jhpark@chonnam.ac.kr; School of Electronics & Computer Engineering, Chonnam National University, Korea, gslee@chonnam.ac.kr; School of Electronics & Computer Engineering, Chonnam National University, Korea; School of Electronics & Computer Engineering, Chonnam National University, Korea, eckim@chonnam.ac.kr; School of Electronics & Computer Engineering, Chonnam National University, Korea, jslim@chonnam.ac.kr; School of Electronics & Computer Engineering, Chonnam National University, Korea, shkim@chonnam.ac.kr; School of Electronics & Computer Engineering, Chonnam National University, Korea, hjyang@chonnam.ac.kr; Multimedia Lab., Telecommunication R&D Center, Telecommunication Network Business, Samsung Electronics Co. LTD, Korea, sangwook.oh@samsung.com","2008 IEEE International Symposium on Signal Processing and Information Technology","6 Feb 2009","2008","","","111","116","In this paper, a system for automatic detection and recognition of Korean texts or shop names in outdoor signboard images is described. The system includes detection, binarization and extraction of text in a signboard image captured by a camera of a mobile phone for the recognition of the shop name. It can deal with different font styles and sizes as well as illumination changes. Individual characters detected by connected component analysis are recognized by using nonlinear mesh, in which feature vectors of vertical and horizontal components are extracted from the binarized image. Proposed methods have been applied to a Korean text translation system, which can automatically detect and recognize Korean texts and generate the translation result.","2162-7843","978-1-4244-3554-8","10.1109/ISSPIT.2008.4775652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4775652","Text detection;Text segmentation;Recognition;signboard image","Image recognition;Text recognition;Layout;Image segmentation;Data mining;Robustness;Mobile handsets;Lighting;Digital cameras;Support vector machines","image recognition;image segmentation;text analysis","automatic shop name detection;automatic shop name recognition;automatic Korean text detection;automatic Korean text recognition;outdoor signboard images;text extraction;text binarization;mobile phone camera;font styles;font sizes;illumination changes;connected component analysis;nonlinear mesh;feature vectors;horizontal components;vertical components;Korean text translation system","","4","1","11","","6 Feb 2009","","","IEEE","IEEE Conferences"
"Automatic 4-D Registration in Dynamic MR Renography","Ting Song; V. S. Lee; H. Rusinek; M. Kaur; A. F. Laine","Department of Biomedical Engineering, Columbia University, New York, NY 10027 USA. (Phone: 212-854-5996; fax: 212-854-5995; e-mail: ts2060@Columbia.edu).; NA; NA; NA; NA","2005 IEEE Engineering in Medicine and Biology 27th Annual Conference","10 Apr 2006","2005","","","3067","3070","Dynamic contrast-enhanced 4-D MR renography has the potential for broad clinical applications, but suffers from respiratory motion that limits analysis and interpretation. Since each examination yields at least over 10 20 serial 3-D images of the abdomen, manual registration is prohibitively labor-intensive. Besides in-plane motion and translation, out-of-plane motion and rotation are observed in the image series. In this paper, a novel robust and automated technique for removing out-of-plane translation and rotation with sub-voxel accuracy in 4-D dynamic MR images is presented. The method was evaluated on simulated motion data derived directly from a clinical patient's data. The method was also tested on 24 clinical patient kidney data sets. Registration results were compared with a mutual information method, in which differences between manually co-registered time-intensity curves and tested time-intensity curves were compared. Evaluation results showed that our method agreed well with these ground truth data","1558-4615","0-7803-8741-4","10.1109/IEMBS.2005.1617122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1617122","","Image edge detection;Anisotropic magnetoresistance;Noise reduction;Magnetic resonance imaging;Image segmentation;Image motion analysis;Testing;Mutual information;Image registration;Biomedical engineering","biomedical MRI;image motion analysis;image registration;kidney;medical image processing","automatic 4-D registration;dynamic contrast-enhanced 4-D MR renography;respiratory motion;in-plane motion;translation;out-of-plane motion;rotation;kidney;mutual information method;coregistered time-intensity curves","","2","","15","","10 Apr 2006","","","IEEE","IEEE Conferences"
"Performance assessment in clay pigeon shooting using machine vision for gaze detection","M. Micheli; S. Massardi; S. Morzenti; S. Pasinetti; C. Briamonte; M. Lancini","University of Brescia,Department of Mechanical and Industrial Engineering,Brescia,Italy; University of Brescia,Department of Mechanical and Industrial Engineering,Brescia,Italy; BRaIN Center, Fabbrica d'Armi P. Beretta S.p.A.,Gardone Val Trompia,Italy; University of Brescia,Department of Mechanical and Industrial Engineering,Brescia,Italy; Human and Health Sciences, Foro Italico University, Sapienza University,Department of Movement, Department of Translational Medicine,Roma,Italy; University of Brescia,Department of Mechanical and Industrial Engineering,Brescia,Italy","2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT","10 Jul 2020","2020","","","111","115","This study was a part of a wider project with the aim of describing the athletic gesture in the Olympic trap discipline. The study aimed to identify the most relevant ocular features and their contribution to athletic performance, developing solutions to monitor athlete's pupils. The results obtained are promising considering the particular athlete configuration and the uncontrolled conditions characterizing this shooting discipline, but improvements are needed to enhance the required accuracy.","","978-1-7281-4892-2","10.1109/MetroInd4.0IoT48571.2020.9138262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138262","clay pigeon shooting;gaze detection;quiet-eye;Olympic trap discipline","","computer vision;object detection;sport","athletic gesture;Olympic trap discipline;athletic performance;performance assessment;clay pigeon;machine vision;gaze detection;athlete configuration","","","","17","","10 Jul 2020","","","IEEE","IEEE Conferences"
"Mild Cognitive Impairment Diagnosis Using Extreme Learning Machine Combined With Multivoxel Pattern Analysis on Multi-Biomarker Resting-State FMRI","N. T. Duc; S. Ryu; M. Choi; M. N. Iqbal Qureshi; B. Lee","Department of Biomedical Science and Engineering (BMSE), Gwangju Institute of Science and Technology (GIST), South Korea; Department of Biomedical Science and Engineering (BMSE), Gwangju Institute of Science and Technology (GIST), South Korea; Department of Biomedical Science and Engineering (BMSE), Gwangju Institute of Science and Technology (GIST), South Korea; Translational Neuroimaging Laboratory, McGill University; Department of Biomedical Science and Engineering (BMSE), Gwangju Institute of Science and Technology (GIST), South Korea","2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","7 Oct 2019","2019","","","882","885","This paper proposed a classification framework that integrates hybrid multivoxel pattern analyses (MVPA) and extreme learning machine (ELM) for automated Mild Cognitive Impairment (MCI) diagnosis applied on concatenations of multi-biomarker resting-state fMRI. Given three-dimensional (3D) regional coherences and functional connectivity patterns measured during resting state, we performed 3D univariate t-tests to obtain initial univariate features which show the significant changes. To enhance discriminative patterns, we employed multivariate feature reductions using recursive feature elimination in combination with univariate t-test. The maximal amount of information changes were achieved by concatenations of multiple functional metrics. The classifications were performed by an ELM, and its efficiency was compared to SVMs. This study reported mean accuracies using 10-fold cross-validation, followed by permutation tests to assess the statistical significance of discriminative results. In diagnosis of MCI, the proposed method achieved a maximal accuracy of 97.86% (p<; 0.001) in ADNI2 cohort and thus has potentials to assist the clinicians in MCI diagnosis.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8857623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8857623","","Support vector machines;Three-dimensional displays;Training;Functional magnetic resonance imaging;Feature extraction;Neuroimaging;Pattern analysis","biomedical MRI;brain;cognition;feature extraction;image classification;learning (artificial intelligence);medical disorders;medical image processing;neurophysiology","MCI diagnosis;extreme learning machine;multivoxel pattern analysis;classification framework;automated Mild Cognitive Impairment diagnosis;multibiomarker resting-state fMRI;functional connectivity patterns;recursive feature elimination;univariate t-test;multivariate feature reduction","Biomarkers;Cognitive Dysfunction;Humans;Learning;Magnetic Resonance Imaging","","","9","","7 Oct 2019","","","IEEE","IEEE Conferences"
"Visualizing Functional Network Connectivity Difference between Healthy Control and Major Depressive Disorder Using an Explainable Machine-learning Method","J. Y. Chun; M. S. E. Sendi; J. Sui; D. Zhi; V. D. Calhoun","Georgia Institute of Technology,Department of Electrical and Computer Engineering,Atlanta,Georgia; Georgia Institute of Technology,Department of Electrical and Computer Engineering,Atlanta,Georgia; Georgia State University, Georgia Institute of Technology, Emory University,Tri-institutional Center for Translational Research in Neuroimaging and Data Science,Atlanta,Georgia; Chinese Academy of Sciences, and University of Chinese Academy of Sciences,Brainnetome Center and National Laboratory of Pattern Recognition, Institute of Automation,Beijing,China; Georgia Institute of Technology,Department of Electrical and Computer Engineering,Atlanta,Georgia","2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)","27 Aug 2020","2020","","","1424","1427","Major depressive disorder (MDD) is a complex mental disorder characterized by a persistent sad feeling and depressed mood. Recent studies reported differences between healthy control (HC) and MDD by looking to brain networks including default mode and cognitive control networks. More recently there has been interest in studying the brain using advanced machine learning-based classification approaches. However, interpreting the model used in the classification between MDD and HC has not been explored yet. In the current study, we classified MDD from HC by estimating whole-brain connectivity using several classification methods including support vector machine, random forest, XGBoost, and convolutional neural network. In addition, we leveraged the SHapley Additive exPlanations (SHAP) approach as a feature learning method to model the difference between these two groups. We found a consistent result among all classification method in regard of the classification accuracy and feature learning. Also, we highlighted the role of other brain networks particularly visual and sensory motor network in the classification between MDD and HC subjects.","2694-0604","978-1-7281-1990-8","10.1109/EMBC44109.2020.9175685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9175685","","Radio frequency;Support vector machines;Visualization;Functional magnetic resonance imaging;Machine learning;Hospitals;Brain modeling","biomedical MRI;brain;cognition;convolutional neural nets;image classification;learning (artificial intelligence);medical disorders;medical image processing;neurophysiology;support vector machines","Shapley additive explanations;HC subjects;sensory motor network;visual motor network;convolutional neural network;support vector machine;whole-brain connectivity;machine learning-based classification approaches;cognitive control networks;brain networks;depressed mood;complex mental disorder;MDD;depressive disorder;healthy control;functional network connectivity difference","Brain;Brain Mapping;Depressive Disorder, Major;Humans;Magnetic Resonance Imaging;Neural Pathways","","","20","","27 Aug 2020","","","IEEE","IEEE Conferences"
"Interpretable and stable prediction of schizophrenia on a large multisite dataset using machine learning with structured sparsity","A. de Pierrefeu; T. Löfstedt; C. Laidi; F. Hadj-Selem; M. Leboyer; P. Ciuciu; J. Houenou; E. Duchesnay","NeuroSpin, CEA, Paris-Saclay, France; Department of Radiation Sciences, Umeå University, Umeå, Sweden; NeuroSpin, CEA, Paris-Saclay, France; Energy Transition Institute: VeDeCoM, France; Institut National de la Santé et de la Recherche Médicale (INSERM), U955, Institut Mondor de Recherche Biomédicale, Psychiatrie Translationnelle, Créteil, France; NeuroSpin, CEA, Paris-Saclay, France; NeuroSpin, CEA, Paris-Saclay, France; NeuroSpin, CEA, Paris-Saclay, France","2018 International Workshop on Pattern Recognition in Neuroimaging (PRNI)","2 Aug 2018","2018","","","1","4","The use of machine-learning (ML) in neuroimaging offers new perspectives in early diagnosis and prognosis of brain diseases. Indeed, ML algorithms can jointly examine all brain features to capture complex relationships in the data in order to make inferences at a single-subject level. To deal with such high dimensional input and the associated risk of overfitting on the training data, a proper regularization (or feature selection) is required. Standard ℓ2-regularized predictors, such as Support Vector Machine, provide dense patterns of predictors. However, in the context of predictive disease signature discovery, it is now essential to understand the brain pattern that underpins the prediction. Despite ℓ1-regularized (sparse) has often been advocated as leading to more interpretable models, they generally lead to scattered and unstable patterns. We hypothesize that the integration of prior knowledge regarding the structure of the input images should improve the relevance and the stability of the predictive signature. Such structured sparsity can be obtained by combining together ℓ1 (possibly ℓ2) and Total variation (TV) penalties. We demonstrated the relevance of using ML with structured sparsity on a large multisite dataset of schizophrenia patients and controls. Using 3D maps of grey matter density, we obtained promising inter-site prediction performances. More importantly, we have uncovered a predictive signature of schizophrenia that is clinically interpretable and stable across resampling. This suggests that structured sparsity provides a major breakthrough over `off-the-shelf' algorithms to perform a robust selection of important brain regions in the context of biomarkers discovery.","","978-1-5386-6859-7","10.1109/PRNI.2018.8423946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8423946","","Predictive models;Magnetic resonance imaging;Support vector machines;Training;Machine learning;Feature extraction;Brain modeling","biomedical MRI;brain;diseases;learning (artificial intelligence);medical disorders;medical image processing;neurophysiology;support vector machines","interpretable prediction;stable prediction;structured sparsity;machine-learning;ML algorithms;schizophrenia patients;support vector machine;neuroimaging;disease signature discovery;ℓ2-regularized;ℓ1-regularized;TV penalties;schizophrenia controls;3D maps;total variation penalties;medical disorders;MRI;large multisite dataset;brain diseases diagnosis;brain diseases prognosis","","","","13","","2 Aug 2018","","","IEEE","IEEE Conferences"
"Diabetic Peripheral Neuropathy Risk Assessment using Digital Fundus Photographs and Machine Learning","J. Benson; T. Estrada; M. Burge; P. Soliz","VisionQuest Biomedical Inc.,Albuquerque,NM,USA,87106; The University of New Mexico,Albuquerque,NM,USA,87131; University of New Mexico’s Clinical and Translational Science,Albuquerque,NM,USA,87106; VisionQuest Biomedical Inc.,Albuquerque,NM,USA,87106","2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)","27 Aug 2020","2020","","","1988","1991","In this work, we demonstrate a novel approach to assessing the risk of Diabetic Peripheral Neuropathy (DPN) using only the retinal images of the patients. Our methodology consists of convolutional neural network feature extraction, dimensionality reduction and feature selection with random projections, combination of image features to case-level representations, and the training and testing of a support vector machine classifier. Using clinical diagnosis as ground truth for DPN, we achieve an overall accuracy of 89% on a held-out test set, with sensitivity reaching 78% and specificity reaching 95%.","2694-0604","978-1-7281-1990-8","10.1109/EMBC44109.2020.9175982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9175982","","Diabetes;Retina;Feature extraction;Support vector machines;Task analysis;Dimensionality reduction","biomedical optical imaging;diseases;eye;feature extraction;image classification;learning (artificial intelligence);medical image processing;neural nets;neurophysiology;support vector machines","diabetic peripheral neuropathy risk assessment;support vector machine classifier;case-level representations;image features;random projections;feature selection;dimensionality reduction;convolutional neural network feature extraction;retinal images;DPN;machine learning;digital fundus photographs","Diabetes Mellitus;Diabetic Neuropathies;Fundus Oculi;Humans;Machine Learning;Photography;Risk Assessment","","","24","","27 Aug 2020","","","IEEE","IEEE Conferences"
"Spiking neural network decoder for brain-machine interfaces","J. Dethier; V. Gilja; P. Nuyujukian; S. A. Elassaad; K. V. Shenoy; K. Boahen","Department of Bioengineering, Stanford University, Stanford, CA 94305, USA; Department of Computer Science and Stanford Institute for Neuro-Innovation and Translational Neuroscience, Stanford University, Stanford, CA 94305, USA; Department of Bioengineering and MSTP, Stanford University, Stanford, CA 94305, USA; Department of Bioengineering, Stanford University, Stanford, CA 94305, USA; Departments of Electrical Engineering and Bioengineering, and Neurosciences Program, Stanford University, Stanford, CA 94305, USA; Department of Bioengineering, Stanford University, Stanford, CA 94305, USA","2011 5th International IEEE/EMBS Conference on Neural Engineering","23 Jun 2011","2011","","","396","399","We used a spiking neural network (SNN) to decode neural data recorded from a 96-electrode array in premotor/motor cortex while a rhesus monkey performed a point-to-point reaching arm movement task. We mapped a Kalman-filter neural prosthetic decode algorithm developed to predict the arm's velocity on to the SNN using the Neural Engineering Framework and simulated it using Nengo, a freely available software package. A 20,000-neuron network matched the standard decoder's prediction to within 0.03% (normalized by maximum arm velocity). A 1,600-neuron version of this network was within 0.27%, and run in real-time on a 3GHz PC. These results demonstrate that a SNN can implement a statistical signal processing algorithm widely used as the decoder in high-performance neural prostheses (Kalman filter), and achieve similar results with just a few thousand neurons. Hardware SNN implementations - neuromorphic chips - may offer power savings, essential for realizing fully-implantable cortically controlled prostheses.","1948-3554","978-1-4244-4140-2","10.1109/NER.2011.5910570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5910570","","Neurons;Kalman filters;Decoding;Prosthetics;Neuroscience;Real time systems;Neuromorphics","biomechanics;brain-computer interfaces;Kalman filters;medical signal processing;neural nets;prosthetics","spiking neural network decoder;brain-machine interfaces;neural data;96-electrode array;premotor-motor cortex;rhesus monkey;point-to-point reaching arm movement task;Kalman-filter neural prosthetic decode algorithm;arm velocity;neural engineering framework;Nengo;statistical signal processing algorithm;neural prosthesis;neuromorphic chips","","3","","15","","23 Jun 2011","","","IEEE","IEEE Conferences"
"Digitalization of Malayalam Palmleaf Manuscripts Based on Contrast-Based Adaptive Binarization and Convolutional Neural Networks","D. Sudarsan; P. Vijayakumar; S. Biju; S. Sanu; S. K. Shivadas","Computer Science & Engineering, Muthoot Institute of Technology & Science, Ernakulam, India; Computer Science & Engineering, Muthoot Institute of Technology & Science, Ernakulam, India; Computer Science & Engineering, Muthoot Institute of Technology & Science, Ernakulam, India; Computer Science & Engineering, Muthoot Institute of Technology & Science, Ernakulam, India; Computer Science & Engineering, Muthoot Institute of Technology & Science, Ernakulam, India","2018 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)","18 Nov 2018","2018","","","1","4","The palm leaf manuscripts are an abundant source of knowledge, tradition and ancient culture. These scriptures are an unavoidable part of our rich culture and have to be preserved in the best possible way. But the information extraction from palm leaf is a tedious task due to various challenges such as noise enormous character set and the difficulty in reading and understanding the ancient Malayalam script. Handwriting recognition in Malayalam is a challenging and emerging area of pattern recognition. Our proposed system aims at extracting information from old palm leaves (thaaliyola) and translating the ancient Malayalam scripts to their current version based on contrast-based adaptive binarization and convolutional neural networks which simplifies the entire process by avoiding feature extraction. The proposed method is different from the conventional methods which require handcrafted features that are used for classification. Initially, the system is trained with a set of characters. This can be expanded to work with the remaining characters as well. The input will be images of Malayalam palmleaf manuscript and the expected output is their translated script. Our system aims to transform these scripts so as to make it accessible and useful to the current generation. The system will be trained using a number of samples to build a convolutional neural network using which the characters will be recognized.","","978-1-5386-3624-4","10.1109/WiSPNET.2018.8538588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538588","Convolutional neural networks;Machine learning;Contrast based adaptive binarization;Palm-leaf manuscript;Malayalam handwriting recognition","Image segmentation;Character recognition;Handwriting recognition;Convolutional neural networks;Hidden Markov models;Adaptive systems;Information retrieval","document image processing;feedforward neural nets;handwriting recognition;history;image classification;image retrieval;natural language processing;optical character recognition","contrast-based adaptive binarization;ancient culture;information extraction;noise enormous character set;pattern recognition;old palm leaves;translated script;thaaliyola;ancient Malayalam script translation;Malayalam palmleaf manuscripts digitalization;convolutional neural networks","","2","","21","","18 Nov 2018","","","IEEE","IEEE Conferences"
"In-Bed Pose Estimation: Deep Learning With Shallow Dataset","S. Liu; Y. Yin; S. Ostadabbas","Electrical and Computer Engineering Department, Augmented Cognition Laboratory, Northeastern University, Boston, MA, USA; Electrical and Computer Engineering Department, Augmented Cognition Laboratory, Northeastern University, Boston, MA, USA; Electrical and Computer Engineering Department, Augmented Cognition Laboratory, Northeastern University, Boston, MA, USA","IEEE Journal of Translational Engineering in Health and Medicine","31 Jan 2019","2019","7","","1","12","This paper presents a robust human posture and body parts detection method under a specific application scenario known as in-bed pose estimation. Although the human pose estimation for various computer vision (CV) applications has been studied extensively in the last few decades, the in-bed pose estimation using camera-based vision methods has been ignored by the CV community because it is assumed to be identical to the general purpose pose estimation problems. However, the in-bed pose estimation has its own specialized aspects and comes with specific challenges, including the notable differences in lighting conditions throughout the day and having pose distribution different from the common human surveillance viewpoint. In this paper, we demonstrate that these challenges significantly reduce the effectiveness of the existing general purpose pose estimation models. In order to address the lighting variation challenge, the infrared selective (IRS) image acquisition technique is proposed to provide uniform quality data under various lighting conditions. In addition, to deal with the unconventional pose perspective, a 2- end histogram of oriented gradient (HOG) rectification method is presented. The deep learning framework proves to be the most effective model in human pose estimation; however, the lack of large public dataset for in-bed poses prevents us from using a large network from scratch. In this paper, we explored the idea of employing a pre-trained convolutional neural network (CNN) model trained on large public datasets of general human poses and fine-tuning the model using our own shallow (limited in size and different in perspective and color) in-bed IRS dataset. We developed an IRS imaging system and collected IRS image data from several realistic life-size mannequins in a simulated hospital room environment. A pre-trained CNN called convolutional pose machine (CPM) was fine-tuned for in-bed pose estimation by re-training its specific intermediate layers. Using the HOG rectification method, the pose estimation performance of CPM improved significantly by 26.4% in the probability of correct key-point (PCK) criteria at PCK0.1 compared to the model without such rectification. Even testing with only well aligned in-bed pose images, our fine-tuned model still surpassed the traditionally tuned CNN by another 16.6% increase in pose estimation accuracy.","2168-2372","","10.1109/JTEHM.2019.2892970","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611350","Convolutional neural network (CNN);convolutional pose machine (CPM);histogram of oriented gradient (HOG);in-bed pose estimation;infrared selective (IRS)","Pose estimation;Lighting;Cameras;Image color analysis;Monitoring;Predictive models","cameras;computer vision;convolutional neural nets;infrared imaging;learning (artificial intelligence);object detection;pose estimation;probability","computer vision applications;camera-based vision methods;in-bed pose estimation;infrared selective image acquisition technique;deep learning framework;in-bed IRS dataset;IRS imaging system;IRS image data;HOG rectification method;in-bed pose images;pose estimation accuracy;shallow dataset;robust human posture;human surveillance;pretrained convolutional neural network model;pretrained CNN;convolutional pose machine;human poses;histogram of oriented gradient rectification method;body part detection method;PCK criteria;probability of correct key-point;IRS image acquisition technique;CPM","","5","","37","","13 Jan 2019","","","IEEE","IEEE Journals"
"Predictive Monitoring of Critical Cardiorespiratory Alarms in Neonates Under Intensive Care","R. Joshi; Z. Peng; X. Long; L. Feijs; P. Andriessen; C. Van Pul","Department of Family Care Solutions, Philips Research, Eindhoven, AZ, The Netherlands; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands; Department of Industrial Design, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands; Department of Neonatology, Máxima Medical Center, Veldhoven, DB, The Netherlands; Department of Clinical Physics, Máxima Medical Center, Veldhoven, DB, The Netherlands","IEEE Journal of Translational Engineering in Health and Medicine","8 Dec 2019","2019","7","","1","10","We aimed at reducing alarm fatigue in neonatal intensive care units by developing a model using machine learning for the early prediction of critical cardiorespiratory alarms. During this study in over 34,000 patient monitoring hours in 55 infants 278,000 advisory (yellow) and 70,000 critical (red) alarms occurred. Vital signs including the heart rate, breathing rate, and oxygen saturation were obtained at a sampling frequency of 1 Hz while heart rate variability was calculated by processing the ECG - both were used for feature development and for predicting alarms. Yellow alarms that were followed by at least one red alarm within a short post-alarm window constituted the case-cohort while the remaining yellow alarms constituted the control cohort. For analysis, the case and control cohorts, stratified by proportion, were split into training (80%) and test sets (20%). Classifiers based on decision trees were used to predict, at the moment the yellow alarm occurred, whether a red alarm(s) would shortly follow. The best performing classifier used data from the 2-min window before the occurrence of the yellow alarm and could predict 26% of the red alarms in advance (18.4s, median), at the expense of 7% additional red alarms. These results indicate that based on predictive monitoring of critical alarms, nurses can be provided a longer window of opportunity for preemptive clinical action. Further, such as algorithm can be safely implemented as alarms that are not algorithmically predicted can still be generated upon the usual breach of the threshold, as in current clinical practice.","2168-2372","","10.1109/JTEHM.2019.2953520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8910552","Alarm fatigue;medical devices;machine learning;NICU;patient monitoring;predictive monitoring;real-time monitoring","Pediatrics;Biomedical monitoring;Predictive models;Patient monitoring;Electrocardiography;Fatigue","decision trees;electrocardiography;learning (artificial intelligence);medical signal processing;paediatrics;patient monitoring;pneumodynamics;signal classification;signal sampling","decision trees;ECG;sampling frequency;oxygen saturation;breathing rate;machine learning;patient monitoring hours;critical alarms;control cohorts;short post-alarm window;red alarm;yellow alarm;heart rate variability;neonatal intensive care units;alarm fatigue;critical cardiorespiratory alarms;predictive monitoring;frequency 1.0 Hz;time 18.4 s","","","","33","CCBY","22 Nov 2019","","","IEEE","IEEE Journals"
"Histogram-Based Features Selection and Volume of Interest Ranking for Brain PET Image Classification","I. Garali; M. Adel; S. Bourennane; E. Guedj","Aix Marseille Univ, CNRS, Centrale Marseille, Institut Fresnel, Marseille, France; Aix Marseille Univ, CNRS, Centrale Marseille, Institut Fresnel, Marseille, France; Ecole Centrale Marseille, Institut Fresnel UMR-CNRS 7249, Marseille, France; Institut de Neurosciences de la Timone UMR-CNRS 7289, Aix-Marseille Université, Marseille, France","IEEE Journal of Translational Engineering in Health and Medicine","28 Mar 2018","2018","6","","1","12","Positron emission tomography (PET) is a molecular medical imaging modality which is commonly used for neurodegenerative diseases diagnosis. Computer-aided diagnosis, based on medical image analysis, could help quantitative evaluation of brain diseases such as Alzheimer's disease (AD). A novel method of ranking the effectiveness of brain volume of interest (VOI) to separate healthy control from AD brains PET images is presented in this paper. Brain images are first mapped into anatomical VOIs using an atlas. Histogram-based features are then extracted and used to select and rank VOIs according to the area under curve (AUC) parameter, which produces a hierarchy of the ability of VOIs to separate between groups of subjects. The top-ranked VOIs are then input into a support vector machine classifier. The developed method is evaluated on a local database image and compared to the known selection feature methods. Results show that using AUC outperforms classification results in the case of a two group separation.","2168-2372","","10.1109/JTEHM.2018.2796600","DHU-Imaging through the A * MIDEX Project; “Investissements d’Avenir” French Government Program; French National Research Agency; region PACA; Nicesoft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8318637","Machine learning;computer-aided diagnosis;first order statistics;feature selection;positron emission tomography;classification;Alzheimer’s disease","Feature extraction;Positron emission tomography;Support vector machines;Dementia;Brain","brain;diseases;feature extraction;feature selection;image classification;medical image processing;neurophysiology;positron emission tomography;support vector machines","local database image;support vector machine classifier;area under curve parameter;histogram-based feature extraction;brain volume-of-interest;Alzheimer's disease;brain diseases;medical image analysis;computer-aided diagnosis;neurodegenerative disease diagnosis;molecular medical imaging modality;positron emission tomography;brain PET image classification;volume-of-interest ranking;histogram-based feature selection","","7","","93","","16 Mar 2018","","","IEEE","IEEE Journals"
"Locally Linear Embedding and fMRI Feature Selection in Psychiatric Classification","G. Sidhu","Department of Computing Science, 1-337 Athabasca Hall, University of Alberta, Edmonton, Canada","IEEE Journal of Translational Engineering in Health and Medicine","2 Sep 2019","2019","7","","1","11","Background: Functional magnetic resonance imaging (fMRI) provides non-invasive measures of neuronal activity using an endogenous Blood Oxygenation-Level Dependent (BOLD) contrast. This article introduces a nonlinear dimensionality reduction (Locally Linear Embedding) to extract informative measures of the underlying neuronal activity from BOLD time-series. The method is validated using the Leave-One-Out-Cross-Validation (LOOCV) accuracy of classifying psychiatric diagnoses using resting-state and task-related fMRI. Methods: Locally Linear Embedding of BOLD time-series (into each voxel's respective tensor) was used to optimise feature selection. This uses Gauß' Principle of Least Constraint to conserve quantities over both space and time. This conservation was assessed using LOOCV to greedily select time points in an incremental fashion on training data that was categorised in terms of psychiatric diagnoses. Findings: The embedded fMRI gave highly diagnostic performances (> 80%) on eleven publicly-available datasets containing healthy controls and patients with either Schizophrenia, Attention-Deficit Hyperactivity Disorder (ADHD), or Autism Spectrum Disorder (ASD). Furthermore, unlike the original fMRI data before or after using Principal Component Analysis (PCA) for artefact reduction, the embedded fMRI furnished significantly better than chance classification (defined as the majority class proportion) on ten of eleven datasets. Interpretation: Locally Linear Embedding appears to be a useful feature extraction procedure that retains important information about patterns of brain activity distinguishing among psychiatric cohorts.","2168-2372","","10.1109/JTEHM.2019.2936348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8807145","Nonlinear;dimensionality reduction;image processing;machine learning;kernel methods;optimization;least squares;neurophysiology;evidence-based medicine;(computer-assisted) diagnosis;fMRI;method of image charges;integration;oscillations;theorema egregium","Functional magnetic resonance imaging;Principal component analysis;Feature extraction;Stress;Image reconstruction;Time measurement","biomedical MRI;blood;brain;feature extraction;feature selection;image classification;medical disorders;medical image processing;neurophysiology;principal component analysis;psychology;time series","fMRI feature selection;noninvasive measures;BOLD time-series;locally linear embedding;functional magnetic resonance imaging;neuronal activity;endogenous blood oxygenation-level dependent contrast;psychiatric classification;leave-one-out-cross-validation accuracy;resting-state fMRI;voxel respective tensor;Gauß principle of least constraint;schizophrenia;attention-deficit hyperactivity disorder;autism spectrum disorder;principal component analysis;PCA;artefact reduction;feature extraction;brain activity;psychiatric cohorts;original fMRI data;embedded fMRI;task-related fMRI;psychiatric diagnoses","","","","66","CCBY","20 Aug 2019","","","IEEE","IEEE Journals"
"An Open-Source Feature Extraction Tool for the Analysis of Peripheral Physiological Data","M. Nabian; Y. Yin; J. Wormwood; K. S. Quigley; L. F. Barrett; S. Ostadabbas","Electrical and Computer Engineering Department, Augmented Cognition Lab, Northeastern University, Boston, MA, USA; Electrical and Computer Engineering Department, Augmented Cognition Lab, Northeastern University, Boston, MA, USA; University of New Hampshire, Durham, NH, USA; Department of Psychology, Northeastern University, Boston, MA, USA; Department of Psychology, Northeastern University, Boston, MA, USA; Electrical and Computer Engineering Department, Augmented Cognition Lab, Northeastern University, Boston, MA, USA","IEEE Journal of Translational Engineering in Health and Medicine","9 Nov 2018","2018","6","","1","11","Electrocardiogram, electrodermal activity, electromyogram, continuous blood pressure, and impedance cardiography are among the most commonly used peripheral physiological signals (biosignals) in psychological studies and healthcare applications, including health tracking, sleep quality assessment, disease early-detection/diagnosis, and understanding human emotional and affective phenomena. This paper presents the development of a biosignal-specific processing toolbox (Bio-SP tool) for preprocessing and feature extraction of these physiological signals according to the state-of-the-art studies reported in the scientific literature and feedback received from the field experts. Our open-source Bio-SP tool is intended to assist researchers in affective computing, digital and mobile health, and telemedicine to extract relevant physiological patterns (i.e., features) from these biosignals semi-automatically and reliably. In this paper, we describe the successful algorithms used for signal-specific quality checking, artifact/noise filtering, and segmentation along with introducing features shown to be highly relevant to category discrimination in several healthcare applications (e.g., discriminating patterns associated with disease versus non-disease). Further, the Bio-SP tool is a publicly-available software written in MATLAB with a user-friendly graphical user interface (GUI), enabling future crowd-sourced modification to these tools. The GUI is compatible with MathWorks Classification Learner app for inference model development, such as model training, cross-validation scheme farming, and classification result computation.","2168-2372","","10.1109/JTEHM.2018.2878000","MathWorks; Army Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8510820","Affective computing;biosignal processing;blood pressure (BP);dimensionality reduction;electrocardiogram (ECG);electrodermal activity (EDA);electromyography (EMG);feature extraction;health informatics;impedance cardiography (ICG);machine learning;pattern recognition;quality checking","Feature extraction;Electrocardiography;Tools;Physiology;Electromyography;Signal processing algorithms;Psychology","blood pressure measurement;diseases;electrocardiography;electromyography;feature extraction;filtering theory;graphical user interfaces;health care;inference mechanisms;learning (artificial intelligence);medical signal detection;medical signal processing;psychology;public domain software;signal classification;signal denoising;sleep;telemedicine","open-source Bio-SP tool;affective computing;digital health;mobile health;signal-specific quality checking;user-friendly graphical user interface;GUI;crowd-sourced modification;MathWorks Classification Learner app;inference model;MATLAB;category discrimination;artifact-noise filtering;telemedicine;disease early-detection-diagnosis;peripheral physiological signals;electromyogram;electrocardiogram;physiological patterns;sleep quality assessment;field experts;feedback;scientific literature;biosignal-specific processing toolbox;affective phenomena;human emotional phenomena;health tracking;healthcare applications;psychological studies;impedance cardiography;continuous blood pressure;electrodermal activity;peripheral physiological data;open-source feature extraction tool","","2","","61","","26 Oct 2018","","","IEEE","IEEE Journals"
"Multiple Linear Discriminant Models for Extracting Salient Characteristic Patterns in Capsule Endoscopy Images for Multi-Disease Detection","A. K. Kundu; S. A. Fattah; K. A. Wahid","Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Electrical and Computer Engineering, University of Saskatchewan, Saskatoon, Canada","IEEE Journal of Translational Engineering in Health and Medicine","6 Mar 2020","2020","8","","1","11","Background: Computer-aided disease detection schemes from wireless capsule endoscopy (WCE) videos have received great attention by the researchers for reducing physicians' burden due to the time-consuming and risky manual review process. While single disease classification schemes are greatly dealt by the researchers in the past, developing a unified scheme which is capable of detecting multiple gastrointestinal (GI) diseases is very challenging due to the highly irregular behavior of diseased images in terms of color patterns. Method: In this paper, a computer-aided method is developed to detect multiple GI diseases from WCE videos utilizing linear discriminant analysis (LDA) based region of interest (ROI) separation scheme followed by a probabilistic model fitting approach. Commonly in training phase, as pixel-labeled images are available in small number, only the image-level annotations are used for detecting diseases in WCE images, whereas pixel-level knowledge, although a major source for learning the disease characteristics, is left unused. In view of learning the characteristic disease patterns from pixel-labeled images, a set of LDA models are trained which are later used to extract the salient ROI from WCE images both in training and testing stages. The intensity patterns of ROI are then modeled by a suitable probability distribution and the fitted parameters of the distribution are utilized as features in a supervised cascaded classification scheme. Results: For the purpose of validation of the proposed multi-disease detection scheme, a set of pixel-labeled images of bleeding, ulcer and tumor are used to extract the LDA models and then, a large WCE dataset is used for training and testing. A high level of accuracy is achieved even with a small number of pixel-labeled images. Conclusion: Therefore, the proposed scheme is expected to help physicians in reviewing a large number of WCE images to diagnose different GI diseases.","2168-2372","","10.1109/JTEHM.2020.2964666","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8962125","Capsule endoscopy;linear discriminant analysis;gastrointestinal disease detection;probability density function model;support vector machine","Diseases;Feature extraction;Training;Hemorrhaging;Tumors;Image segmentation","biomedical optical imaging;diseases;endoscopes;feature extraction;image classification;learning (artificial intelligence);linear discriminant analysis;medical image processing;tumours","linear discriminant analysis based region of interest separation scheme;computer-aided disease detection schemes;multidisease detection scheme;supervised cascaded classification scheme;LDA models;disease patterns;pixel-level knowledge;WCE images;image-level annotations;pixel-labeled images;probabilistic model fitting approach;WCE videos;computer-aided method;color patterns;multiple gastrointestinal diseases;single disease classification schemes;wireless capsule endoscopy videos;capsule endoscopy images;salient characteristic patterns;multiple linear discriminant models","","","","33","CCBY","17 Jan 2020","","","IEEE","IEEE Journals"
"The Identification of Alzheimer’s Disease Using Functional Connectivity Between Activity Voxels in Resting-State fMRI Data","Y. Shi; W. Zeng; J. Deng; W. Nie; Y. Zhang","Information Engineering College, Shanghai Maritime University, Shanghai, China; Information Engineering College, Shanghai Maritime University, Shanghai, China; Information Engineering College, Shanghai Maritime University, Shanghai, China; Information Engineering College, Shanghai Maritime University, Shanghai, China; Information Engineering College, Shanghai Maritime University, Shanghai, China","IEEE Journal of Translational Engineering in Health and Medicine","27 Apr 2020","2020","8","","1","11","Background: Alzheimer's disease (AD) is a common neurodegenerative disease occurring in the elderly population. The effective and accurate classification of AD symptoms by using functional magnetic resonance imaging (fMRI) has a great significance for the clinical diagnosis and prediction of AD patients. Methods: Therefore, this paper proposes a new method for identifying AD patients from healthy subjects by using functional connectivities (FCs) between the activity voxels in the brain based on fMRI data analysis. Firstly, independent component analysis is used to detect the activity voxels in the fMRI signals of AD patients and healthy subjects; Secondly, the FCs between the common activity voxels of the two groups are calculated, and then the FCs with significant differences are further identified by statistical analysis between them; Finally, the classification of AD patients from healthy subjects is realized by using FCs with significant differences as the feature samples in support vector machine. Results: The results show that the proposed identification method can obtain higher classification accuracy, and the FCs between activity voxels within prefrontal lobe as well as those between prefrontal and parietal lobes play an important role in the prediction of AD patients. Furthermore, we also find that more brain regions and much more voxels in some regions are activity in AD group compared with health control group. Conclusion: It has a great potential value for the AD pathogenesis mechanism study.","2168-2372","","10.1109/JTEHM.2020.2985022","National Natural Science Foundation of China; Shanghai Sailing Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9056851","Activity voxels;fMRI;functional connectivity;independent component analysis;support vector machine","Functional magnetic resonance imaging;Support vector machines;Integrated circuits;Dementia;Feature extraction;Independent component analysis","biomedical MRI;brain;data analysis;diseases;independent component analysis;medical image processing;neurophysiology;statistical analysis;support vector machines","identification method;common activity voxels;fMRI signals;independent component analysis;fMRI data analysis;healthy subjects;functional magnetic resonance imaging;effective classification;common neurodegenerative disease;resting-state fMRI data;functional connectivity;Alzheimer's disease;AD patients;FCs","","","","46","CCBY","3 Apr 2020","","","IEEE","IEEE Journals"
"Research on Ripple Algorithm in Automatic Target Recognition","J. Lan; T. Huang; L. Wan","Dept. of Meas. & Control Technol., Univ. of Sci. & Technol. Beijing, Beijing; Dept. of Meas. & Control Technol., Univ. of Sci. & Technol. Beijing, Beijing; Dept. of Meas. & Control Technol., Univ. of Sci. & Technol. Beijing, Beijing","2008 First International Conference on Intelligent Networks and Intelligent Systems","21 Nov 2008","2008","","","487","490","Automatic target recognition (ATR) is increasingly important in a wide range of video and image applications, such as machine vision and security monitoring. In order to reduce the complexity and time lag in unmanned aircraft vehicle system (UAVS) for signals surveillance, force protection and strike, this paper proposes a novel ripple algorithm for ATR. This algorithm uses concentric circles to extract features of object and the features are quasi invariant to target translations, rotations and scaling. Through outdoor experimentation, it is proven that the ripple algorithm is correct and effective with high-speed in program running time. The recognition rate is more than 90% and the code runs on the order much faster than the implementation based on the traditional algorithms.","","978-0-7695-3391-9","10.1109/ICINIS.2008.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4683270","","Target recognition;Unmanned aerial vehicles;Feature extraction;Object recognition;Gray-scale;Intelligent networks;Protection;Color;Image converters;Intelligent systems","aircraft;computer vision;feature extraction;image recognition;remotely operated vehicles","automatic target recognition;ripple algorithm;machine vision;security monitoring;unmanned aircraft vehicle system;signals surveillance;force protection","","2","","3","","21 Nov 2008","","","IEEE","IEEE Conferences"
"Automatic Iris Segmentation Based on Local Areas","GuangZhu Xu; ZaiFeng Zhang; YiDe Ma","Lanzhou Univeristy, Lanzhou, Gansu, 730000,P.R. China; Sch. of Inf. Sci. & Eng., Lanzhou Univ.; Sch. of Inf. Sci. & Eng., Lanzhou Univ.","18th International Conference on Pattern Recognition (ICPR'06)","18 Sep 2006","2006","4","","505","508","In this paper a novel and robust method for automatic iris segmentation based on local areas is described. Such method is composed of three main parts, (a) Find the local rectangle region which has the minimum intensity mean and extend it to locate pupil, (b) Select two small local sector areas including the outer boundaries of iris to locate outer iris, (c) Translate the iris from polar coordinates into Cartesian coordinates and normalize it to fixed size to compensate the stretching of the iris texture as the pupil changes in size and remove the non-concentricity of the iris and the pupil. The method was implemented using CASIA iris image databases. The experimental results show that the proposed method has an encouraging result with an overall accuracy of 98.42%","1051-4651","0-7695-2521-0","10.1109/ICPR.2006.300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1699889","","Biometrics;Iris recognition;Eyelashes;Image segmentation;Robustness;Image databases;Fingerprint recognition;Feature extraction;Authentication;Eyelids","eye;geometry;image segmentation;image texture","automatic iris segmentation;local rectangle region;polar coordinates;Cartesian coordinates;iris texture;CASIA iris image database","","5","","7","","18 Sep 2006","","","IEEE","IEEE Conferences"
"Computer Vision Based Vacuum Circuit-Breaker's Stroke Parameter Automatic Measurement","S. Zhao; J. Chi; B. Li","North China Electr. Power Univ., Baoding, China; North China Electr. Power Univ., Baoding, China; North China Electr. Power Univ., Baoding, China","2009 WRI Global Congress on Intelligent Systems","21 Aug 2009","2009","2","","223","227","In order to solve the existing deficiencies of measuring the vacuum circuit-breakerpsilas stroke parameter, this paper proposes a new non-contact measurement method based on modern computer vision technology. Firstly, during the movement of switching operation of the vacuum circuit-breaker, CCD camera is utilized as the detect sensor to capture special images with specific screw fixed in the moving conduction-bar; secondly, the quality of the images is enhanced by image preprocessing, such as image filter, enhancement and distorted calibration etc.; and then recognizing the position of the specific screwpsilas axes by using template matching technique; at last calculating stroke parameter value according to the axespsila displacement and translate to the real-world results. The experimental results shows that the proposed method not only can realize a non-contact measurement of the vacuum circuit-breakerpsilas stroke parameter, but also can meet the needs of quickly and precise measuring in power test field experiment.","2155-6091","978-0-7695-3571-5","10.1109/GCIS.2009.331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5209435","vacuum circuit-breaker;stroke parameter;template matching;computer vision","Computer vision;Vacuum technology;Charge-coupled image sensors;Power measurement;Circuit testing;Switching circuits;Charge coupled devices;Fasteners;Matched filters;Calibration","CCD image sensors;computer vision;image enhancement;image matching;vacuum circuit breakers","vacuum circuit-breakers stroke parameter automatic measurement;computer vision technology;noncontact measurement method;switching operation;CCD camera;image sensor;image enhancement;image preprocessing;image recognition;template matching technique","","","","8","","21 Aug 2009","","","IEEE","IEEE Conferences"
"Automatic Target Recognition in SAR Images Based on a Combination of CNN and SVM","T. -D. Wu; Y. Yen; J. H. Wang; R. J. Huang; H. -W. Lee; H. -F. Wang","National Taiwan Ocean University,Department of Electrical Engineering,Taiwan; National Taiwan Ocean University,Department of Electrical Engineering,Taiwan; National Taiwan Ocean University,Department of Electrical Engineering,Taiwan; National Taiwan Ocean University,Department of Electrical Engineering,Taiwan; National Chung-Shan Institute of Science & Technology,Chemical Systems Research Division,Taiwan; Chung Chou University of Science and Technology,Department of Intelligent Automation Engineering,Taiwan","2020 International Workshop on Electromagnetics: Applications and Student Innovation Competition (iWEM)","6 Nov 2020","2020","","","1","2","In recent years, convolutional neural network (CNN) has been increasingly considered as a promising technology for military and homeland security applications. The fusion of CNN and Support vector machine (SVM), a popular traditional machine learning approach, has received intensive attention in the field of synthetic aperture radar (SAR) automatic target recognition (ATR). This paper, firstly, discusses the effects of some preprocessing and image enhancement methods on the performance of SAR ATR, starting with the pre-trained AlexNet in a transfer-learning based approach. Secondly, the architecture of AlexNet is modified to form a new model suitable for SAR ATR. Finally, we propose a hybrid model associated with the success of the learning feature of our CNN model and the ability of SVM to process high-dimensional dataset effectively. To evaluate the proposed method, experiments are performed on the Moving and Stationary Target Acquisition and Recognition (MSTAR) public database. The comparative results demonstrate that these preprocessing and enhancement methods prior to the deep-learning process are not necessary since the feature representation ability of AlexNet is already powerful. Furthermore, experimental results on the benchmark MSTAR dataset illustrate the effectiveness of the proposed new model. On classification of ten-class targets, the commonly used translation augmentation for training data has been performed. By combining the CNN and SVM, the classification accuracy percentages can be slightly improved for our proposed new model.","","978-1-7281-9989-4","10.1109/iWEM49354.2020.9237422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237422","SAR ATR;CNN;SVM","Support vector machines;Technological innovation;Target recognition;Training data;Radar polarimetry;Synthetic aperture radar;Image enhancement","convolutional neural nets;feature extraction;image classification;image enhancement;learning (artificial intelligence);radar imaging;radar target recognition;support vector machines;synthetic aperture radar","ten-class targets;CNN SVM;SAR images;convolutional neural network;military homeland security applications;support vector machine;machine learning approach;synthetic aperture radar automatic target recognition;image enhancement methods;SAR ATR;pre-trained AlexNet;transfer-learning based approach;hybrid model;learning feature;CNN model;high-dimensional dataset;Recognition public database;preprocessing methods;deep-learning process;feature representation ability;benchmark MSTAR dataset","","","","4","","6 Nov 2020","","","IEEE","IEEE Conferences"
"An automatic method for determining the anatomical relevant space for fast volumetric cardiac imaging","A. Ortega; J. Pedrosa; B. Heyde; L. Tong; J. D'hooge","Department of Cardiovascular Sciences, KU Leuven, Belgium; Department of Cardiovascular Sciences, KU Leuven, Belgium; Department of Cardiovascular Sciences, KU Leuven, Belgium; Department of Cardiovascular Sciences, KU Leuven, Belgium; Department of Cardiovascular Sciences, KU Leuven, Belgium","2015 IEEE International Ultrasonics Symposium (IUS)","16 Nov 2015","2015","","","1","4","Fast volumetric cardiac imaging requires to reduce the number of transmit events within a single volume. One way of achieving this is by limiting the field-of-view (FOV) of the recording to the anatomically relevant domain only (e.g. the myocardium when investigating cardiac mechanics). Although fully automatic solutions towards myocardial segmentation exist, translating that information in a fast ultrasound scan sequence is not trivial. The aim of this study was therefore to develop a methodology to automatically define the FOV from a volumetric dataset in the context of anatomical scanning. Hereto, a method is proposed where the anatomical relevant space is automatically identified as follows. First, the left ventricular myocardium is localized in the volumetric ultrasound recording using a fully automatic real-time segmentation framework (i.e. BEAS). Then, the extracted meshes are employed to define a binary mask identifying myocardial voxels only. Later, using these binary images, the percentage of pixels along a given image line that belong to the myocardium is calculated. Finally, a spatially continuous FOV that covers `T' percentage of the myocardium is found by means of a ring-shaped template matching, giving as a result the opening angle and `thickness' for a conical scan. This approach was tested on 27 volumetric ultrasound datasets, a T = 85% was used. The mean initial opening angle for a conical scan was of 19.67±8.53° while the mean `thickness' of the cone was 19.01±3.35°. Therefore, a reduction of 48.99% in the number of transmit events was achieved, resulting in a frame rate gain factor of 1.96. As a conclusion, anatomical scanning in combination with new scanning sequences techniques can increase frame rate significantly while keeping information of the relevant structures for functional imaging.","","978-1-4799-8182-3","10.1109/ULTSYM.2015.0124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7329603","Fast cardiac imaging;Beamforming;Anatomical scan","Myocardium;Imaging;Ultrasonic imaging;Image segmentation;Array signal processing;Acoustic beams;Real-time systems","biomedical ultrasonics;cardiology;image segmentation;image sequences;medical image processing;ultrasonic imaging","anatomical relevant space;fast volumetric cardiac imaging;field-of-view;anatomically relevant domain;myocardial segmentation;fast ultrasound scan sequence;anatomical scanning;left ventricular myocardium;volumetric ultrasound recording;automatic real-time segmentation framework;binary mask identifying myocardial voxels;spatially continuous FOV;ring-shaped template matching;conical scan;volumetric ultrasound datasets;mean initial opening angle;frame rate gain factor;scanning sequence techniques","","","","12","","16 Nov 2015","","","IEEE","IEEE Conferences"
"Automatic retinal image registration using fully connected vascular tree","J. Parekar; P. Porwal; M. Kokare","Center of Excellence in Signal and Image Processing Shri Guru Gobind Singhji Institute of Engineering and Technology Vishnupuri, Nanded (M.S.), India; Center of Excellence in Signal and Image Processing Shri Guru Gobind Singhji Institute of Engineering and Technology Vishnupuri, Nanded (M.S.), India; Center of Excellence in Signal and Image Processing Shri Guru Gobind Singhji Institute of Engineering and Technology Vishnupuri, Nanded (M.S.), India","2016 International Conference on Signal and Information Processing (IConSIP)","16 Feb 2017","2016","","","1","5","Retinal image registration plays very important role in medical imaging, as it combines the information of different field of view images and temporal images. This paper presents the novel method for retinal image registration based on bifurcation structure matching. In proposed method, segmentation is based on the multi-scale ridge detection, and the preprocessing steps used to avoid false detection of vessels. Also, Dijkstra's algorithm is used to get a fully connected vascular tree. A novel technique is proposed for removing the false correspondence. This method is invariant to translation, rotation and scaling since feature vector contains normalized lengths and angles. The registered image is visually good as well as experimental performance shows very good results.","","978-1-5090-1522-1","10.1109/ICONSIP.2016.7857498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7857498","Retinal image registration;Vessel segmentation;Bifurcation structure;Transformation model","Retina;Bifurcation;Image segmentation;Image registration;Biomedical imaging;Detectors;Image edge detection","biomedical optical imaging;blood vessels;edge detection;eye;feature extraction;image matching;image registration;image segmentation;medical image processing","automatic retinal image registration;fully connected vascular tree;temporal images;bifurcation structure matching;image segmentation;multiscale ridge detection;false vessel detection;Dijkstra algorithm;feature vector","","3","","9","","16 Feb 2017","","","IEEE","IEEE Conferences"
"Thin-plate spline based automatic alignment of dynamic MR breast images","H. Wang; B. Zheng; W. Good; Tian-Ge Zhuang","Dept. of Radiol., Pittsburgh Univ. Sch. of Med., PA, USA; NA; NA; NA","Proceedings of the 22nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (Cat. No.00CH37143)","6 Aug 2002","2000","4","","2850","2853 vol.4","We propose a thin-plate spline based dynamic MR breast image registration approach under the assumption that only small alignment differences exist after global registration. Control point pairs between reference and sensed images are automatically and adaptively selected according to image gradient and local correlation of the image under translation and rotation. These landmarks are not homogeneously distributed, but are only located on skin lines, at edge corners and on contour ridges which visually demonstrate the structural deformation due to patient motion, normal breathing and cardiac motion. For MR breast sequences which have intensity changes after contrast injection, this method shows robust results and has less computational cost because of its dependence on fewer, but more effective, control points as compared to traditional iterative registration algorithms.","1094-687X","0-7803-6465-1","10.1109/IEMBS.2000.901462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=901462","","Spline;Automatic control;Image registration;Equations;Skin;Breast cancer;Image edge detection;Image motion analysis;Motion analysis;Helium","biomedical MRI;image registration;mammography;medical image processing;image sequences;splines (mathematics)","thin-plate spline based automatic alignment;dynamic MRI breast images;image registration approach;global registration;control point pairs;sensed images;reference images;image gradient;local correlation;skin lines;edge corners;contour ridges;intensity changes;contrast injection","","5","1","8","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic target recognition of ISAR object images based on neural network","Wu Ning; Wugun Chen; Xinggan Zhang","Dept. of Electron. Eng., Nanjing Univ. of Aeronaut. & Astronaut., China; Dept. of Electron. Eng., Nanjing Univ. of Aeronaut. & Astronaut., China; NA","International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003","5 Apr 2004","2003","1","","373","376 Vol.1","In this paper an integrate approach of automatic target recognition is proposed with a three-feedforward neural network. It includes images pre-processing, feature extraction and automatic target recognition and classification of ISAR object images. Using the log-spiral transform after moving the centroid of ISAR images to the original point makes the target recognition invariant on translation, rotation and scale. The approach improves the ratio of accurate recognition and reduces the amount of calculation. The results of experiments with field data show that the approach is effective.","","0-7803-7702-8","10.1109/ICNNSP.2003.1279287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1279287","","Target recognition;Neural networks;Pixel;Radar imaging;Image recognition;Feature extraction;Character recognition;Radar scattering;Optical scattering;Reconnaissance","feature extraction;radar imaging;synthetic aperture radar;radar target recognition;military radar;military computing;radar computing;feedforward neural nets;image classification","automatic target recognition;ISAR object images;neural network;feedforward neural network;images preprocessing;feature extraction;log-spiral transform","","3","","5","","5 Apr 2004","","","IEEE","IEEE Conferences"
"Automatic labeling of continuous wave Doppler images based on combined image and sentence networks","M. Moradi; Y. Guo; Y. Gur; T. Syeda-Mahmood","IBM Research - Almaden Research Center, San Jose, CA, United States of America; IBM Research - Almaden Research Center, San Jose, CA, United States of America; IBM Research - Almaden Research Center, San Jose, CA, United States of America; IBM Research - Almaden Research Center, San Jose, CA, United States of America","2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)","19 Jun 2017","2017","","","667","670","As medical imaging datasets grow, we are approaching the era of big data for radiologic decision support systems. This requires renewed efforts in dataset curation and labeling. We propose a methodology for weak labeling of medical images for attributes such as anatomy and disease that relies on image to sentence transformation. The methodology consists of three models, a convolutional neural network that is trained on a coarse classification task and acts as an image feature generator, a language model to map sentences to a fixed length space, and a multi-layer perceptron that acts as a function approximator to map images to the sentence space. The transform model is trained on matched image-sentence pairs on a dataset of echocardiography studies. For a given image, labels are extracted from the closest sentences to the output of the image-sentence transform. We show that the resulting solution has an 78.2% accuracy in labeling Doppler images with aortic stenosis. We also show that the retrieved sentences are consistent with the true sentences in terms of meaning with an average BLEU score of 0.34, matching the current highly performing machine translation solutions.","1945-8452","978-1-5090-1172-8","10.1109/ISBI.2017.7950608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950608","Image labeling;multimodal classification","Valves;Training;Labeling;Doppler effect;Diseases;Transforms;Feature extraction","Doppler measurement;echocardiography;medical image processing;multilayer perceptrons","automatic labeling;continuous wave Doppler images;medical imaging;convolutional neural network;multilayer perceptron;matched image-sentence pairs;echocardiography;image-sentence transform","","","","12","","19 Jun 2017","","","IEEE","IEEE Conferences"
"Variable shape models for LS-based automatic building extraction from VHR satellite imagery","Weian Wang; Yi Liu; Jiao lu; Bo zheng","Dept of SURVEYING AND GEOINFORMATICS, Tongji university Shanghai, China; Dept of SURVEYING AND GEOINFORMATICS, Tongji university Shanghai, China; Dept of SURVEYING AND GEOINFORMATICS, Tongji university Shanghai, China; Dept of SURVEYING AND GEOINFORMATICS, Tongji university Shanghai, China","2009 Joint Urban Remote Sensing Event","26 Jun 2009","2009","","","1","4","In this paper, we propose a level set based automatic building extraction method using prior shapes. We introduce a variable shape model which together with the level set function for segmentation dynamically indicates the region with which the prior shape should be compared. Our model is capable of segmenting an object from an image based on the image intensity as well as the prior shape. In addition, the proposed model permits translation, scaling and rotation of the prior shape. Moreover, a fast way is also established for the minimization of our functional. The experiments validate our model.","2334-0932","978-1-4244-3460-2","10.1109/URS.2009.5137647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5137647","","Shape;Satellites;Image segmentation;Level set;Active contours;Labeling;Feature extraction;Humans;Data mining;Remote sensing","feature extraction;geophysical signal processing;image segmentation;remote sensing","level set-based automatic building extraction;VHR satellite imagery;variable shape model;image segmentation;image intensity","","1","","3","","26 Jun 2009","","","IEEE","IEEE Conferences"
"Automatic facial feature detection and location","R. Pinto-Elias; J. H. Sossa-Azuela","Centro Nacional de Investigacion, Morelos, Mexico; NA","Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)","6 Aug 2002","1998","2","","1360","1364 vol.2","A method to automatically detect and locate human face features (eyes and mouth) in a 2D gray level image is presented. The method uses a genetic algorithm (GA) and an invariant description of the facial features to accomplish the task. The descriptors used are the well known first four translation, rotation, and scale moment invariants proposed by Hu (1962). In a first step, an image possibly containing a face or a set of faces is first divided into small cells of fixed size. For each cell, the ordinary moments are next computed. From these quantities, the corresponding Hu's invariants are then derived. Human face feature detection and location is thus accomplished by grouping individual cells using a genetic algorithm by fitting a specific cost function. The cost function corresponds to the invariant description of a specified face feature (eye or mouth) given in terms of the corresponding gray level values.","1051-4651","0-8186-8512-3","10.1109/ICPR.1998.711954","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=711954","","Facial features;Face detection","feature extraction;face recognition;genetic algorithms;image segmentation","facial feature detection;facial feature location;eyes;mouth;2D gray level image;invariant description;ordinary moments;Hu's invariants;cost function;gray level values","","7","5","18","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Rotation-invariant MLP classifiers for automatic aerial image recognition","S. Greenberg; H. Guterman; S. R. Rotman","Dept. of Electr. & Comput. Eng., Ben-Gurion Univ. of the Negev, Beer-Sheva, Israel; Dept. of Electr. & Comput. Eng., Ben-Gurion Univ. of the Negev, Beer-Sheva, Israel; Dept. of Electr. & Comput. Eng., Ben-Gurion Univ. of the Negev, Beer-Sheva, Israel","Eighteenth Convention of Electrical and Electronics Engineers in Israel","6 Aug 2002","1995","","","2.2.4/1","2.2.4/5","This paper describes the application of Multi Layer Perceptron (MLP) neural networks to the problem of Automatic Aerial Image Recognition (AAIR). The classification of aerial images independent of their orientation is required for automatic tracking and target recognition. Rotation-invariance is achieved by using rotation invariant feature space in conjunction with feed forward neural networks. The performance of the neural network based classifiers in conjunction with 3 types of rotation-invariant AAIR global features: the Zernike moments, central moments, and polar transform are examined. The performance of the Zernike based classifier is compared with that of the classical central moments, and polar transform. The real part of the phase spectrum of the Fourier plane is employed in combination with the MLP for rotation and translation invariance. The advantages of these approaches are discussed. Although a large image data base would be necessary before this approach could be fully validated, the initial results are very promising.","","0-7803-2498-6","10.1109/EEIS.1995.513798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=513798","","Image recognition;Neural networks;Covariance matrix;Neurons;Target recognition;Feeds;Feedforward neural networks;Shape;Layout;Application software","feedforward neural nets;multilayer perceptrons;image classification","rotation-invariant MLP classifiers;automatic aerial image recognition;multilayer perceptron;MLP neural networks;image orientation independence;automatic tracking;target recognition;rotation invariant feature space;feedforward neural networks;Zernike moments;central moments;polar transform;phase spectrum;Fourier plane","","1","","11","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic Lipreading with Limited Training Data","S. H. Leung; A. W. C. Liew; W. H. Lau; S. L. Wang; S. H. Leung; A. W. C. Liew; W. H. Lau; S. L. Wang","NA; NA; NA; Sch. of Info. Security Eng., Shanghai Jiaotong Univ.; NA; NA; NA; Sch. of Info. Security Eng., Shanghai Jiaotong Univ.","18th International Conference on Pattern Recognition (ICPR'06)","18 Sep 2006","2006","3","","881","884","Speech recognition solely based on visual information such as the lip shape and its movement is referred to as lipreading. This paper presents an automatic lipreading technique for speaker dependent (SD) and speaker independent (SI) speech recognition tasks. Since the visual features are derived according to the frame rate of the video sequence, spline representation is then employed to translate the discrete-time sampled visual features into continuous domain. The spline coefficients in the same word class are constrained to have similar expression and can be estimated from the training data by the EM algorithm. In addition, an adaptive multi-model approach is proposed to overcome the variation caused by different speaking style in speaker-independent recognition task. The experiments are carried out to recognize the ten English digits and an accuracy of 96% for speaker dependent recognition and 88% for speaker independent recognition have been achieved, which shows the superiority of our approach compared with other classifiers investigated","1051-4651","0-7695-2521-0","10.1109/ICPR.2006.301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1699666","","Training data;Hidden Markov models;Speech recognition;Spline;Data mining;Automatic speech recognition;Data security;Information security;Shape;Video sequences","expectation-maximisation algorithm;speech recognition;splines (mathematics);video signal processing","automatic lipreading;speech recognition;visual information;speaker dependent;speaker independent;visual features;video sequence;spline representation;EM algorithm","","9","","7","","18 Sep 2006","","","IEEE","IEEE Conferences"
"A Framework for Automatic Segmentation of Lung Nodules from Low Dose Chest CT Scans","A. El-Baz; A. Farag; G. Gimel'farb; R. Falk; M. A. El-Ghar; T. Eldiasty","University of Louisville, Louisville, Kentucky, USA.; CVIP Lab., Louisville Univ., KY; NA; NA; NA; NA","18th International Conference on Pattern Recognition (ICPR'06)","18 Sep 2006","2006","3","","611","614","To accurately separate each pulmonary nodule from its background in a low dose computer tomography (LDCT) chest image, two new adaptive probability models of the visual appearance of small 2D and large 3D pulmonary nodules are jointly used to control the evolution of the de-formable model. The appearance prior is modeled with a translation and rotation invariant Markov-Gibbs random field of voxel intensities with pairwise interaction. The model is analytically identified from a set of training nodule images with normalized intensity ranges. Both the nodules and their background in each current multi-modal chest image are also modeled with a linear combination of discrete Gaussians that closely approximate the empirical marginal probability distribution of voxel intensities. Experiments with real LDCT chest images confirm the high accuracy of the proposed approach","1051-4651","0-7695-2521-0","10.1109/ICPR.2006.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1699600","","Lungs;Computed tomography;Image segmentation;Programmable control;Adaptive control;Automatic control;Image analysis;Gaussian approximation;Gaussian distribution;Probability distribution","adaptive systems;computerised tomography;image segmentation;Markov processes;medical image processing;probability","lung nodule segmentation;low dose computer tomography chest imaging;adaptive probability model;pulmonary nodule;Markov-Gibbs random field;marginal probability distribution;voxel intensity","","14","","11","","18 Sep 2006","","","IEEE","IEEE Conferences"
"Automatic classification of cotton boll using signature curve and boundary descriptors","S. Kumar; M. Kashyap; Y. Choudhary; S. S. Pal; M. Bhattacharya","ABV Indian Institute of Information Technology and Management, Gwalior-474010, M.P., India; ABV Indian Institute of Information Technology and Management, Gwalior-474010, M.P., India; ABV Indian Institute of Information Technology and Management, Gwalior-474010, M.P., India; ABV Indian Institute of Information Technology and Management, Gwalior-474010, M.P., India; ABV Indian Institute of Information Technology and Management, Gwalior-474010, M.P., India","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","28 Sep 2015","2015","","","1599","1605","The correlation between the environmental features and image features of cotton bolls is the necessary step for the pattern recognition and translate those features for machine understanding is the main challenge to distinguish mature cotton boll from immature one. Present work is tried to solve this problem using shape based features. The fuzzy based classifier is introduced for the decision making. Any improper acquisition of images of cotton bolls, like intense illumination or deep shadows (which is of course absent in natural settings) will produce improper results.","","978-1-4799-8792-4","10.1109/ICACCI.2015.7275842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275842","Feature extraction;Fuzzy based classifier;Cotton boll","Image edge detection;Image segmentation;Lead","agriculture;cotton;decision making;feature extraction;fuzzy set theory;image classification","cotton ball automatic classification;signature curve;boundary descriptors;cotton ball environmental features;cotton ball image features;pattern recognition;shape based features;fuzzy based classifier;decision making","","","","17","","28 Sep 2015","","","IEEE","IEEE Conferences"
"Automatic Allophone Deriving for Korean Speech Recognition","J. Xu; Y. Si; J. Pan; Y. Yan","Key Lab. of Speech Acoust. & Content Understanding, Beijing, China; Key Lab. of Speech Acoust. & Content Understanding, Beijing, China; Key Lab. of Speech Acoust. & Content Understanding, Beijing, China; Key Lab. of Speech Acoust. & Content Understanding, Beijing, China","2013 Ninth International Conference on Computational Intelligence and Security","24 Feb 2014","2013","","","776","779","In Korean, the pronunciations of phonemes are severely affected by their contexts. Thus, using phonemes directly translated from their written forms as basic units for acoustic modeling is problematic, as these units lack the ability to capture the complex pronunciation variations occurred in continuous speech. Allophone, a sub-phone unit in phonetics but served as independent phoneme in speech recognition, is considered to have the ability to describe complex pronunciation variations. This paper presents a novel approach called Automatic Allophone Deriving (AAD). In this approach, statistics from Gaussian Mixture Models are used to create measurements for allophone candidates, and decision trees are used to derive allophones. Question set used by the decision tree is also generated automatically, since we assumed no linguistic knowledge would be used in this approach. This paper also adopts long-time features over conventional cepstral features to capture acoustic information over several hundred milliseconds for AAD, as co-articulation effects are unlikely to be limited to a single phoneme. Experiment shows that AAD outperforms previous approaches which derive allophones from linguistic knowledge. Additional experiments use long-time features directly in acoustic modeling. The results show that performance improvement achieved by using the same allophones can be significantly improved by using long-time features, compared with corresponding baselines.","","978-1-4799-2549-0","10.1109/CIS.2013.169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746537","allophone;Korean speech recognition;long-time features","Acoustics;Speech recognition;Context;Pragmatics;Decision trees;Clustering algorithms;Hidden Markov models","acoustic signal processing;decision trees;Gaussian processes;natural language processing;speech recognition","automatic allophone deriving approach;Korean speech recognition;phoneme pronunciation;acoustic modeling;pronunciation variations;continuous speech;AAD approach;statistics;Gaussian mixture models;decision trees;linguistic knowledge;cepstral features;acoustic information capture","","1","","10","","24 Feb 2014","","","IEEE","IEEE Conferences"
"Automatic map generation with mobile robot","T. Matsuo; K. Tanaka; N. Abe","Fac. of Comput. Sci. & Syst. Eng., Kyushu Inst. of Technol., Iizuka, Japan; NA; NA","IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)","6 Aug 2002","1999","4","","680","685 vol.4","A method is described by which a mobile robot acquires an environment map by walking around the given environment for itself. The mobile robot in the paper is able to translate and rotate and has 3 DOF with respect to the x and y coordinates and the rotational angle around the z axis. It is equipped with a laser range sensor, ultrasonic wave sensors, infrared radiation sensors and contact sensors. Every time it moves the displacement error accumulates. So a map generated becomes incorrect due to the influence of the error. A method is proposed in which the displacement error is revised and a correct map is acquired by exploiting a laser range sensor. The method for the generation of locus and the running control of the mobile robot referring to the map acquired are also proposed.","1062-922X","0-7803-5731-0","10.1109/ICSMC.1999.812486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812486","","Mobile robots;Infrared sensors;Sensor systems;Robot sensing systems;Robot kinematics;Ultrasonic variables measurement;Computer science;Systems engineering and theory;Legged locomotion;Error correction","mobile robots;laser ranging;infrared detectors;ultrasonic transducers;image segmentation","automatic map generation;environment map;3 DOF robot;rotational angle;laser range sensor;ultrasonic wave sensors;infrared radiation sensors;contact sensors;displacement error","","3","","6","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic lattice detection in near-regular histology array images","B. A. Canada; G. K. Thomas; K. C. Cheng; J. Z. Wang; Yanxi Liu","The Pennsylvania State University, University Park, USA; The Pennsylvania State University, University Park, USA; The Pennsylvania State University, University Park, USA; The Pennsylvania State University, University Park, USA; The Pennsylvania State University, University Park, USA","2008 15th IEEE International Conference on Image Processing","12 Dec 2008","2008","","","1452","1455","Near-regular texture (NRT), denoting deviations from otherwise symmetric wallpaper patterns, is commonly observable in the real world. Existing lattice detection algorithms capture the underlying lattice of an NRT pattern and all of its individual texels, facilitating an automated analysis of NRT. Many real world images, as in those of zebrafish larval histology arrays, depart significantly from regularity and challenge the current state of the art wallpaper group theory-based lattice detection methods. We propose an alternative 2D lattice detection algorithm that exploits translation and reflection symmetries and specific imaging cues. By outperforming existing methods on histology array images, our algorithm leads us towards complete automation of high-throughput histological image processing while broadening the spectrum of NRT computation.","2381-8549","978-1-4244-1765-0","10.1109/ICIP.2008.4712039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4712039","lattice estimation;biological tissues;biomedical image processing","Lattices;Detection algorithms;Biomedical imaging;Microscopy;Pattern analysis;Algorithm design and analysis;Reflection;Automation;Image processing;Biology computing","biological tissues;image texture;medical image processing","automatic lattice detection;near-regular histology array images;near-regular texture;zebrafish larval histology arrays;symmetric wallpaper patterns;lattice detection algorithms;NRT pattern;real world images;group theory-based lattice detection methods;reflection symmetries;lattice estimation;biological tissues;biomedical image processing","","3","","8","","12 Dec 2008","","","IEEE","IEEE Conferences"
"Automatic VHDL generation for solving rotation and scale-invariant template matching in FPGA","H. P. A. Nobre; H. Y. Kim","Escola Politécnica, Universidade de Sao Paulo, Brazil; Escola Politécnica, Universidade de Sao Paulo, Brazil","2009 5th Southern Conference on Programmable Logic (SPL)","12 May 2009","2009","","","21","26","Template matching is a classical problem in computer vision. It consists in detecting the presence of a given template in a digital image. This task becomes considerably more complex with the invariance to rotation, scale, translation, brightness and contrast (RSTBC). A novel RSTBC-invariant robust template matching algorithm named Ciratefi was recently proposed. However, its execution in a conventional computer takes several seconds. Moreover, the implementation of its general version in hardware is difficult, because there are many adjustable parameters. This paper proposes a software that automatically generates compilable Hardware Description Logic (VHDL) modules that implements Ciratefi in Field Programmable Gate Array (FPGA) devices. The proposed solution accelerates the time to process a frame from 7s (in a 3 GHz PC) to 1.06 ms. This excellent performance (more than the required for a real-time system) may lead to cost-effective high-performance co-processing computer vision systems.","","978-1-4244-3847-1","10.1109/SPL.2009.4914890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4914890","","Field programmable gate arrays;Computer vision;Hardware;Programmable logic arrays;Digital images;Brightness;Robustness;Automatic logic units;Logic devices;Acceleration","computer vision;field programmable gate arrays;hardware description languages","automatic VHDL generation;rotation template matching;scale-invariant template matching;FPGA;computer vision;digital image;Ciratefi;hardware description logic","","5","","13","","12 May 2009","","","IEEE","IEEE Conferences"
"Methodology for automatic movement cycle extraction using Switching Linear Dynamic System","R. De Souza Baptista; A. Padilha Lanari Bó; M. Hayashibe","LARA, Universidade de Brasília, Brazil; LARA, Universidade de Brasília, Brazil; INRIA DEMAR Project and LIRMM, CNRS/University of Montpellier, France","2015 7th International IEEE/EMBS Conference on Neural Engineering (NER)","2 Jul 2015","2015","","","743","746","Human motion assessment is key for motor-control rehabilitation. Using standardized definitions and spatiotemporal features - usually presented as a movement cycle diagram- specialists can associate kinematic measures to progress in rehabilitation therapy or motor impairment due to trauma or disease. Although devices for capturing human motion today are cheap and widespread, the automatic interpretation of kinematic data for rehabilitation is still poor in terms of quantitative performance evaluation. In this paper we present an automatic approach to extract spatiotemporal features from kinematic data and present it as a cycle diagram. This is done by translating standard definitions from human movement analysis into mathematical elements of a Switching Linear Dynamic System model. The result is a straight-forward procedure to learn a tracking model from a sample execution. This model is robust when used to automatically extract the movement cycle diagram of the same motion (the Sit-Stand-Sit, as an example) executed in different subject-specific manner such as his own motion speed.","1948-3554","978-1-4673-6389-1","10.1109/NER.2015.7146730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7146730","","Switches;Superluminescent diodes;Feature extraction;Mathematical model;Standards;Estimation;Kinematics","biomechanics;feature extraction;kinematics;medical signal processing;patient rehabilitation","automatic movement cycle extraction;switching linear dynamic system;human motion assessment;motor-control rehabilitation;spatiotemporal features;kinematic measures;rehabilitation therapy;motor impairment;trauma;disease;human motion;kinematic data;quantitative performance evaluation;spatiotemporal feature extraction;cycle diagram;human movement analysis;tracking model;sit-stand-sit cycle","","1","","11","","2 Jul 2015","","","IEEE","IEEE Conferences"
"Automatic Image Annotation Based on Improved Relevance Model","H. Song; X. Li; P. Wang","Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China; Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China; NA","2009 Asia-Pacific Conference on Information Processing","7 Aug 2009","2009","2","","59","62","Automatic image annotation is an important and promising solution to narrow the semantic gap between low-level visual feature and high-level semantic concept. Here we propose an improved relevance model to solve image annotation problem. Unlike the classical approaches including classification, and translation model, the improved model is capable of discovering the correlation between blobs (segmented regions) and textual keywords so as to automatically generate keywords for un-annotated image according to joint probabilities. Moreover, it has the ability to detect and remove false keyword(s) by considering the co-occurrence of keywords through machine learning. Experiments demonstrate that the proposed approach outperforms the previous algorithms for image annotation.","","978-0-7695-3699-6","10.1109/APCIP.2009.151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5197136","image annotation;relevance model;image retrieval;joint probability;co-occurrence","Image retrieval;Object recognition;Search engines;Educational institutions;Computer science;Image segmentation;Machine learning;Shape;Image storage;Information retrieval","image classification;image retrieval;learning (artificial intelligence);probability","automatic image annotation;relevance model;low-level visual feature;high-level semantic concept;machine learning;joint probability;image retrieval;image classification","","1","","9","","7 Aug 2009","","","IEEE","IEEE Conferences"
"A Research on Paper-Mediated Braille Automatic Extraction Method","H. Zhang; J. Li; J. Yin","Coll. of Electron. & Inf. Eng., Changchun Univ., Changchun, China; Coll. of Electron. & Inf. Eng., Changchun Univ., Changchun, China; Coll. of Electron. & Inf. Eng., Changchun Univ., Changchun, China","2010 International Conference on Intelligent Computation Technology and Automation","26 Jul 2010","2010","1","","328","331","The paper-mediated Braille automatic and translation method referred in this paper, capture Braille images by the digital cameras, and pre-process the Braille images, segment the Braille images by the image processing technology and then extract the Braille features by the fixed nature of Braille. This method is simple, convenient, and easy to operate, also able to extract dots online in real time. The paper gives the general method of Braille extraction technology, and introduces the specific image processing approach in detail. The experiments show that the method is effective and accurate for Braille extraction.","","978-1-4244-7280-2","10.1109/ICICTA.2010.145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5523323","Braille image acquisition;Braille image Processing;Braille image segmentation;Braille Feature Extraction","Automation","cameras;feature extraction;image segmentation","paper-mediated Braille automatic extraction method;digital cameras;Braille image segmentation;image processing technology;Braille features extraction;online dots extraction","","4","","7","","26 Jul 2010","","","IEEE","IEEE Conferences"
"Automatic stress-relieving music recommendation system based on photoplethysmography-derived heart rate variability analysis","I. Shin; J. Cha; G. W. Cheon; C. Lee; S. Y. Lee; H. Yoon; H. C. Kim","Interdisciplinary Program, Biomedical Engineering Major, Graduate School, Seoul National University, Seoul, Republic of Korea; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD 21218 USA; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD 21218 USA; Interdisciplinary Program, Bioengineering Major, Graduate School, Seoul National University, Seoul, Republic of Korea; Department of Biomedical Engineering, University of Michigan, Ann Arbor, MI 48109 USA; Department of Biomedical Engineering, College of Medicine, Seoul National University, 101 Daehak-ro, Jongno-gu, Seoul 110-799, Republic of Korea; Department of Biomedical Engineering, College of Medicine, Seoul National University, 101 Daehak-ro, Jongno-gu, Seoul 110-799, Republic of Korea","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","6 Nov 2014","2014","","","6402","6405","This paper presents an automatic stress-relieving music recommendation system (ASMRS) for individual music listeners. The ASMRS uses a portable, wireless photoplethysmography module with a finger-type sensor, and a program that translates heartbeat signals from the sensor to the stress index. The sympathovagal balance index (SVI) was calculated from heart rate variability to assess the user's stress levels while listening to music. Twenty-two healthy volunteers participated in the experiment. The results have shown that the participants' SVI values are highly correlated with their prespecified music preferences. The sensitivity and specificity of the favorable music classification also improved as the number of music repetitions increased to 20 times. Based on the SVI values, the system automatically recommends favorable music lists to relieve stress for individuals.","1558-4615","978-1-4244-7929-0","10.1109/EMBC.2014.6945093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945093","","Heart rate variability;Stress;Recommender systems;Indexes;Multiple signal classification;Educational institutions;Music","cardiology;medical signal processing;music;photoplethysmography;signal classification","automatic stress-relieving music recommendation system;ASMRS;heart rate variability analysis;music listeners;wireless photoplethysmography module;finger-type sensor;heartbeat signals;stress index;sympathovagal balance index;SVI;music classification","Adolescent;Automation;Female;Heart Rate;Humans;Male;Music;Music Therapy;Photoplethysmography;ROC Curve;Stress, Psychological;User-Computer Interface;Vagus Nerve","2","","13","","6 Nov 2014","","","IEEE","IEEE Conferences"
"Automatic detection and recognition of road sign for driver assistance system","A. Hechri; A. Mtibaa","Laboratory of Electronics and Micro-Electronics, FSM, University of Monastir; Laboratory of Electronics and Micro-Electronics, FSM, University of Monastir","2012 16th IEEE Mediterranean Electrotechnical Conference","10 May 2012","2012","","","888","891","Automatic road-signs recognition is becoming a part of Driver Assisting Systems which role is to increase safety and driving comfort. This paper presents an efficient approach for detecting and recognizing road sign in traffic scene images acquired from a moving vehicle. The developed road sign recognition system is divided into two stages: detection stage to localize signs from a whole image, and classification stage that classifies the detected sign into one of the reference signs. The detection module segments the input image in the YCBCR colour space, and then detects road signs using a shape filtering method. The classification module determines the type of detected road signs using a Multi-layer Perceptron neural networks. An extensive experimentation has shown that the proposed approach is robust enough to detect and recognize road signs under varying translation, rotation and lighting conditions.","2158-8481","978-1-4673-0784-0","10.1109/MELCON.2012.6196571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6196571","Road sign;segmentation;recognition;vehicle","Roads;Image color analysis;Shape;Vehicles;Image segmentation;Neural networks;Lighting","driver information systems;filtering theory;image classification;image colour analysis;image recognition;multilayer perceptrons;object detection;road safety;road traffic","road sign automatic detection;driver assistance system;automatic road-sigs recognition system;traffic scene images;image classification;YCBCR colour space;shape filtering method;multilayer perceptron neural networks;lighting conditions","","9","","13","","10 May 2012","","","IEEE","IEEE Conferences"
"Real time EEG based automatic brainwave regulation by music","A. Hossan; A. M. M. Chowdhury","Department of Applied Physics, Electronics and Communication Engineering, University of Chittagong, Chittagong, Bangladesh; Department of Applied Physics, Electronics and Communication Engineering, University of Chittagong, Chittagong, Bangladesh","2016 5th International Conference on Informatics, Electronics and Vision (ICIEV)","1 Dec 2016","2016","","","780","784","In this paper, we proposed an approach to automatically regulate the mood or brainwave of the paralyzed or sensory impaired or psychologically sick people. A MATLAB based algorithm has been developed to analyze the brain wave signals obtained from a real time EEG data acquisition system corresponding to different moods of the subject people. The program translated the brain wave signals into command to select and play suitable music tracks according to the state of the brain. These music or sound tracks, selected based on the choice of the target people, acted in turn as a feedback to continuously adjust the subject's mood as desired. Relaxation music, subcontinental soft patriotic, Rabindra Sangeet & motivational songs were used as example for relaxation, entertainment and warm-up respectively. Sample stored EEG data of different patients having different moods were used to verify the feasibility of the approach and the obtained results ensured its effectiveness.","","978-1-5090-1269-5","10.1109/ICIEV.2016.7760107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760107","Brainwave regulation;BCI;Real time EEG data acquisition;Music Therapy;Neurofeedback","Real-time systems;Electroencephalography;Mood;MATLAB;Algorithm design and analysis;Data acquisition;Music","bioelectric potentials;data acquisition;electroencephalography;medical signal processing;music;neurophysiology;psychology","real time EEG based automatic brainwave regulation;psychologically sick people;sensory impaired people;paralyzed people;MATLAB based algorithm;brain wave signals;real time EEG data acquisition system;music tracks;relaxation music;subcontinental soft patriotic songs;Rabindra Sangeet & motivational songs","","4","","22","","1 Dec 2016","","","IEEE","IEEE Conferences"
"Automatic Classification of Partial Shoeprints Using Advanced Correlation Filters for use in Forensic Science","M. Gueham; A. Bouridane; D. Crookes","School of Electronics, Electrical Engineering and Computer Science, Queen's University Belfast, United Kingdom; School of Electronics, Electrical Engineering and Computer Science, Queen's University Belfast, United Kingdom; School of Electronics, Electrical Engineering and Computer Science, Queen's University Belfast, United Kingdom","2008 19th International Conference on Pattern Recognition","23 Jan 2009","2008","","","1","4","One of the most difficult problems in automatic shoeprint classification is the matching of partial shoeprint images. This task becomes more challenging in the presence of geometric distortions (e.g. translated and/or rotated partial prints). In this paper, we evaluate the performance of advanced correlation filters (ACFs) for the automatic classification of partial shoeprints. The optimal trade-off synthetic discriminant function (OTSDF) filter and the unconstrained OTSDF (UOTSDF) filter, in particular, were used to match partial shoeprint images with different qualities. Experimental assessment using a shoeprint image database has demonstrated the efficient classification performance of ACFs compared to other state-of-the-art methods.","1051-4651","978-1-4244-2174-9","10.1109/ICPR.2008.4761058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761058","","Forensics;Matched filters;Fourier transforms;Additive noise;Layout;Databases;Footwear;Noise reduction;Computer science;Electronic mail","filtering theory;forensic science;image classification;image matching","automatic partial shoeprints classification;advanced correlation filters;forensic science;partial shoeprint image matching;geometric distortions;optimal trade-off synthetic discriminant function filter;unconstrained OTSDF filter","","7","","11","","23 Jan 2009","","","IEEE","IEEE Conferences"
"INVAID-integration of computer vision techniques for automatic incident detection","K. Keen; N. Hoose","Wootton Jeffreys Consultants Ltd., Woking, UK; NA","IEE Colloquium on Car and its Environment - What DRIVE and PROMETHEUS Have to Offer","6 Aug 2002","1990","","","9/1","9/3","The partners in INVAID represent research, consultancy and manufacturing interests in UK, France, Spain and the Belgium. The INVAID aim is to develop the use of computer vision tools to translate observations in the form of video pictures into traffic condition reports. The particular conditions of interest are incidents and traffic congestion. The overall project and the computer vision tools TITAN and IMPACTS are described by the authors.<>","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=189736","","Road vehicle electronics;Machine vision;Traffic control (transportation)","automotive electronics;computer vision;research initiatives;road traffic;traffic computer control","road traffic control;Europe;automotive electronics;INVAID;computer vision;automatic incident detection;represent;video pictures;traffic condition reports;incidents;traffic congestion;TITAN;IMPACTS","","","","","","6 Aug 2002","","","IET","IET Conferences"
"An EMG-based Human-Machine Interface to control multimedia player","M. T. Hammi; O. Salem; A. Mehaoua","LIPADE Laboratory, Paris Descartes University, France; LIPADE Laboratory, Paris Descartes University, France; LIPADE Laboratory, Paris Descartes University, France","2015 17th International Conference on E-health Networking, Application & Services (HealthCom)","19 Apr 2016","2015","","","274","279","The electromyogram signals generated by muscles are used in numerous fields such as augmented reality, biomedical, kinematics, gaming, 3D animations and Human-Machine Interfaces. The latter is specifically used to help persons with reduced mobility or amputee with specific constraints to remote control machines. In this paper, we present a novel EMG-based system that aims to control multimedia player in simple, efficient and flexible manner. The implementation of our proposed approach was realized in order to achieve experiments and to conduct performance analysis. Our approach uses pattern recognition and contraction duration to derive four predefined actions. Our experimental results show the capacity of our system to achieve good detection accuracy of user EMG-based commands and to translate these commands into actions in media player system.","","978-1-4673-8325-7","10.1109/HealthCom.2015.7454511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7454511","ElectroMyoGram (EMG);muscles contractions;signal processing;media player;Human-Machine Interface (HMI)","Electromyography;Muscles;Electrodes;Media;Feature extraction;Sensors;Pregnancy","electromyography;human computer interaction;medical signal processing;pattern recognition","contraction duration;pattern recognition;multimedia player control;muscles;electromyogram signals;EMG-based human-machine interface","","3","","12","","19 Apr 2016","","","IEEE","IEEE Conferences"
"Application of feature tracking in a vision based human machine interface for Xbox","M. A. Oskoei; H. Hu","School of Computer Science and Electronic Engineering University of Essex, Wivenhoe Park, Colchester CO4 3SQ, U.K.; School of Computer Science and Electronic Engineering University of Essex, Wivenhoe Park, Colchester CO4 3SQ, U.K.","2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)","25 Feb 2010","2009","","","1738","1743","This paper presents a vision based human machine interface (HMI) for the Xbox. It applies feature tracking algorithms to recognize user's head gestures and translates them into commands for the game. The pyramidal implementation of Lucas Kanade feature tracking is used to trace the optical flows in a sequence of frames. The experimental results show the feasibility of the proposed vision based interface, which has reasonable performance comparing with the standard game pad.","","978-1-4244-4774-9","10.1109/ROBIO.2009.5420435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5420435","Computer Vision;Optical Flow;Motion Tracking;Human Machine Interface;Xbox","Humans;Magnetic heads;Face detection;Hidden Markov models;Computer vision;Tracking;Games;Image motion analysis;Eyes;Application software","computer games;computer vision;gesture recognition;human computer interaction;image sequences;tracking","vision based human machine interface;Xbox;feature tracking algorithms;user head gesture recognition;Lucas Kanade feature tracking;optical flows;standard game pad;video games","","4","1","9","","25 Feb 2010","","","IEEE","IEEE Conferences"
"The Role of the Control Framework for Continuous Teleoperation of a Brain–Machine Interface-Driven Mobile Robot","L. Tonin; F. C. Bauer; J. del R. Millán","Intelligent Autonomous System Lab, Department of Information Engineering, University of Padova, Padua, Italy; aiCTX AG, Zurich, Switzerland; Department of Electrical and Computer Engineering & the Department of Neurology, University of Texas at Austin, Austin, USA","IEEE Transactions on Robotics","6 Feb 2020","2020","36","1","78","91","Despite the growing interest in brain-machine interface (BMI)-driven neuroprostheses, the translation of the BMI output into a suitable control signal for the robotic device is often neglected. In this article, we propose a novel control approach based on dynamical systems that was explicitly designed to take into account the nature of the BMI output that actively supports the user in delivering real-valued commands to the device and, at the same time, reduces the false positive rate. We hypothesize that such a control framework would allow users to continuously drive a mobile robot and it would enhance the navigation performance. 13 healthy users evaluated the system during three experimental sessions. Users exploit a 2-class motor imagery BMI to drive the robot to five targets in two experimental conditions: with a discrete control strategy, traditionally exploited in the BMI field, and with the novel continuous control framework developed herein. Experimental results show that the new approach: 1) allows users to continuously drive the mobile robot via BMI; 2) leads to significant improvements in the navigation performance; and 3) promotes a better coupling between user and robot. These results highlight the importance of designing a suitable control framework to improve the performance and the reliability of BMI-driven neurorobotic devices.","1941-0468","","10.1109/TRO.2019.2943072","Hasler Foundation, Bern, Switzerland; National Centre of Competence in Research Robotics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8879618","Brain–machine interface (BMI);control framework;motor imagery (MI);neurorobotics","Decoding;Electroencephalography;Task analysis;Mobile robots;Navigation;Performance evaluation","brain-computer interfaces;electroencephalography;medical robotics;medical signal processing;mobile robots;neurophysiology;telemedicine;telerobotics","brain-machine interface-driven mobile robot;brain-machine interface-driven neuroprostheses;motor imagery BMI;BMI-driven neurorobotic devices","","1","","48","IEEE","22 Oct 2019","","","IEEE","IEEE Journals"
"Hemicraniectomy in Traumatic Brain Injury: A Noninvasive Platform to Investigate High Gamma Activity for Brain Machine Interfaces","M. Vaidya; R. D. Flint; P. T. Wang; A. Barry; Y. Li; M. Ghassemi; G. Tomic; J. Yao; C. Carmona; E. M. Mugler; S. Gallick; S. Driver; N. Brkic; D. Ripley; C. Liu; D. Kamper; A. H. Do; M. W. Slutzky","Department of Neurology, Northwestern University, Chicago, IL, USA; Department of Neurology, Northwestern University, Chicago, IL, USA; Department of Biomedical Engineering, University of California, Irvine, CA, USA; Shirley Ryan Ability Lab, Chicago, IL, USA; Department of Neurology, University of California, Irvine, CA, USA; Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA; Department of Neurology, Northwestern University, Chicago, IL, USA; Physical Therapy & Human Movement Science, Northwestern University, Chicago, IL, USA; Physical Therapy & Human Movement Science, Northwestern University, Chicago, IL, USA; Department of Neurology, Northwestern University, Chicago, IL, USA; Shirley Ryan Ability Lab, Chicago, IL, USA; Shirley Ryan Ability Lab, Chicago, IL, USA; Shirley Ryan Ability Lab, Chicago, IL, USA; Shirley Ryan Ability Lab, Chicago, IL, USA; Rancho Los Amigos National Center for Rehabilitation, Downey, CA, USA; Department of Biomedical Engineering, North Carolina State University, Raleigh, NC, USA; Department of Neurology, University of California, Irvine, CA, USA; Shirley Ryan Ability Lab, Chicago, IL, USA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","5 Jul 2019","2019","27","7","1467","1472","Brain-machine interfaces (BMIs) translate brain signals into control signals for an external device, such as a computer cursor or robotic limb. These signals can be obtained either noninvasively or invasively. Invasive recordings, using electrocorticography (ECoG) or intracortical microelectrodes, provide higher bandwidth and more informative signals. Rehabilitative BMIs, which aim to drive plasticity in the brain to enhance recovery after brain injury, have almost exclusively used non-invasive recordings, such electroencephalography (EEG) or magnetoencephalography (MEG), which have limited bandwidth and information content. Invasive recordings provide more information and spatiotemporal resolution, but do incur risk, and thus are not usually investigated in people with stroke or traumatic brain injury (TBI). Here, in this paper, we describe a new BMI paradigm to investigate the use of higher frequency signals in brain-injured subjects without incurring significant risk. We recorded EEG in TBI subjects who required hemicraniectomies (removal of a part of the skull). EEG over the hemicraniectomy (hEEG) contained substantial information in the high gamma frequency range (65-115 Hz). Using this information, we decoded continuous finger flexion force with moderate to high accuracy (variance accounted for 0.06 to 0.52), which at best approaches that using epidural signals. These results indicate that people with hemicraniectomies can provide a useful resource for developing BMI therapies for the treatment of brain injury.","1558-0210","","10.1109/TNSRE.2019.2912298","National Institutes of Health; Doris Duke Charitable Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8697146","Brain–machine interface;brain–computer interface;EEG;traumatic brain injury;high gamma","Electroencephalography;Force;Electrodes;Brain injuries;Neurology;Brain-computer interfaces;Bandwidth","biomechanics;biomedical electrodes;brain;brain-computer interfaces;electroencephalography;injuries;magnetoencephalography;medical signal processing;microelectrodes;neurophysiology;patient rehabilitation","intracortical microelectrodes;control signals;brain signals;brain-machine interfaces;brain machine interfaces;investigate high gamma activity;noninvasive platform;epidural signals;high gamma frequency range;substantial information;hemicraniectomy;brain-injured subjects;higher frequency signals;traumatic brain injury;invasive recordings;noninvasive recordings;rehabilitative BMIs;informative signals;frequency 65.0 Hz to 115.0 Hz","Adult;Artifacts;Brain Injuries, Traumatic;Brain-Computer Interfaces;Decompressive Craniectomy;Electroencephalography;Female;Fingers;Gamma Rhythm;Humans;Magnetoencephalography;Male;Muscle Contraction;Prosthesis Design;Psychomotor Performance","2","","36","","23 Apr 2019","","","IEEE","IEEE Journals"
"State observer for a class of nonlinear systems and its application to machine vision","Xinkai Chen; H. Kano","Dept. of Electron. & Inf. Syst., Shibaura Inst. of Technol., Saitama, Japan; NA","IEEE Transactions on Automatic Control","15 Nov 2004","2004","49","11","2085","2091","In this note, we consider the state observer problem for a class of nonlinear systems which are usually encountered in the machine vision study. The formulation of the state observer is motivated by the sliding mode methods and adaptive control techniques. The proposed observer is applied to the identification problems of the motion parameters and space position of a moving object by using the perspective observation of a single point. It is clarified that the rotation parameters can be observed by using the observation of one camera, and the position and translation parameters cannot be observed by using one camera and must appeal to stereo vision. Simulation results show that the proposed algorithm is effective.","1558-2523","","10.1109/TAC.2004.837529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1356135","Machine vision;nonlinear system;perspective observation;state observer","Nonlinear systems;Machine vision;Observability;Cameras;Adaptive control;Stereo vision;Space charge;Charge coupled devices;Linear systems;Sufficient conditions","observers;computer vision;nonlinear control systems;variable structure systems;adaptive control","state observer;nonlinear systems;machine vision;sliding mode methods;adaptive control;perspective observation","","62","","20","","15 Nov 2004","","","IEEE","IEEE Journals"
"Tuberculosis bacilli detection in Ziehl-Neelsen-stained tissue using affine moment invariants and Extreme Learning Machine","M. K. Osman; M. Y. Mashor; H. Jaafar","Faculty of Electrical Engineering, Universiti Teknologi MARA (UiTM) Malaysia; Electronic & Biomedical Intelligent Systems (EBItS), Research Group, School of Mechatronic Engineering, Universiti Malaysia Perlis, Ulu Pauh Main Campus, 02600 Arau, Malaysia; Department of Pathology, School of Medical Science, Universiti Sains Malaysia, 16150 Kubang Kerian, Kelantan, Malaysia","2011 IEEE 7th International Colloquium on Signal Processing and its Applications","2 May 2011","2011","","","232","236","This paper describes an approach to automate the detection and classification of tuberculosis (TB) bacilli in tissue section using image processing technique and feedforward neural network trained by Extreme Learning Machine. It aims to assist pathologists in TB diagnosis and give an alternative to the conventional manual screening process, which is time-consuming and labour-intensive. Images are captured from Ziehl-Neelsen (ZN) stained tissue slides using light microscope as it is commonly used approach for diagnosis of TB. Then colour image segmentation is used to locate the regions correspond to the bacilli. After that, affine moment invariants are extracted to represent the segmented regions. These features are invariant under rotation, scale and translation, thus useful to represent the bacilli. Finally, a single layer feedforward neural network (SLFNN) trained by Extreme Learning Machine (ELM) is used to detect and classify the features into three classes: `TB', `overlapped TB' and `non-TB'. The results indicate that the ELM gives acceptable classification performance with shorter training period compared to the standard backpropagation training algorithms.","","978-1-61284-413-8","10.1109/CSPA.2011.5759878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5759878","Tuberculosis bacilli detection;tissue sections;affine moment invariants;Extreme Learning Machine;neural network","Training;Signal processing algorithms;Image segmentation;Classification algorithms;Microscopy;Image color analysis;Machine learning","biomedical optical imaging;diseases;image segmentation;medical image processing;neural nets;optical microscopy","tuberculosis bacilli detection;Ziehl-Neelsen-stained tissue;affine moment invariants;extreme learning machine;image processing;single layer feedforward neural network;TB diagnosis;Ziehl-Neelsen stained tissue slide;light microscopy;colour image segmentation","","10","","13","","2 May 2011","","","IEEE","IEEE Conferences"
"Machine Learning techniques to identify potential drug targets for Anti-epileptic drugs","A. Kumar; C. Janaki; M. V. Hosur; S. N. Pal","Center for Development of Advanced Computing (C-DAC), Knowledge Park,Bengaluru; Center for Development of Advanced Computing (C-DAC), Knowledge Park,Bengaluru; National Institute of Advanced Studies (NIAS),Bengaluru; Center for Development of Advanced Computing (C-DAC), Knowledge Park,Bengaluru","2020 IEEE International Conference on Machine Learning and Applied Network Technologies (ICMLANT)","23 Feb 2021","2020","","","1","6","Epilepsy is a neurological disorder affecting millions worldwide. Though many Anti-epileptic drugs (AEDs) are available for treatment, almost 30% of patients with Epilepsy (PWE) are resistant to these AEDs. In this paper, we applied Machine learning (ML) techniques for predicting potential drug targets for development of new AEDs. For this particular problem statement, some of the widely used classification algorithms have been applied. Features related to physico-chemical, structural properties, and post-translational modifications are considered for training the ML models. These models have been trained using three different datasets i.e. epilepsy-associated proteins known from literature, all the reviewed human proteins, and known AED targets. Applying different approaches, an accuracy of more than 80% is achieved, and even the F1 score is found to be significant. We have identified few novel druggable proteins that could act as potential targets for patients with refractory seizures.","","978-1-7281-8885-0","10.1109/ICMLANT50963.2020.9355971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355971","Epilepsy;Machine learning;Drug targets;Anti-epileptic drugs;Stacking approaches","Drugs;Proteins;Training;Neurological diseases;Epilepsy;Prediction algorithms;Testing","biochemistry;diseases;drugs;electroencephalography;learning (artificial intelligence);medical disorders;medical signal processing;molecular biophysics;neurophysiology;patient treatment;proteins","machine learning techniques;potential drug targets;classification algorithms;ML models;epilepsy-associated proteins;AED targets;potential targets;antiepileptic drugs;neurological disorder;epilepsy;physicochemical properties;structural properties","","","","24","","23 Feb 2021","","","IEEE","IEEE Conferences"
"Motion estimation for human-machine interaction","G. M. Phade; P. D. Uddharwar; P. A. Dhulekar; S. T. Gandhe","Electronics and Telecommunication Department, Savitribai Phule University of Pune, India; Electronics and Telecommunication Department, Savitribai Phule University of Pune, India; Electronics and Telecommunication Department, Savitribai Phule University of Pune, India; Electronics and Telecommunication Department, Savitribai Phule University of Pune, India","2014 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)","26 Oct 2015","2014","","","000149","000154","Human machine interaction is an evolving research area which deals with interactions with machines that will be as natural as an interaction between humans. Vision based motion estimation can also be said as the interpretation of motions via mathematical algorithms. This system have applications in domains such as sign language translation, virtual environments, smart surveillance, controlling robots, control of machines, medical systems etc. The task is challenging due to the accuracy, further the items in the background or distinct features of the users may make recognition more difficult. In this survey, we address these challenges. We provide an overview of current advances in the field. Further, we discuss limitations and outline promising directions of research. A brief overview of methodologies implemented by earlier researcher on vision based gesture recognition for human machine interaction is discussed here.","2162-7843","978-1-4799-1812-6","10.1109/ISSPIT.2014.7300579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300579","Human machine interaction (HMI);Motion estimation;Gesture recognition","Estimation;Virtual environments;Monitoring;Biomedical monitoring;Graphical user interfaces;Personal digital assistants;Radio frequency","computer vision;gesture recognition;human computer interaction;motion estimation","human-machine interaction;vision-based motion estimation;motion interpretation;mathematical algorithms;background items;user features;vision-based gesture recognition;HMI","","8","","16","","26 Oct 2015","","","IEEE","IEEE Conferences"
"From Group-Level Statistics to Single-Subject Prediction: Machine Learning Detection of Concussion in Retired Athletes","R. Boshra; K. Dhindsa; O. Boursalie; K. I. Ruiter; R. Sonnadara; R. Samavi; T. E. Doyle; J. P. Reilly; J. F. Connolly","School of Biomedical Engineering, McMaster University, Hamilton, Canada; ARiEAL Research Centre, McMaster University, Hamilton, Canada; School of Biomedical Engineering, McMaster University, Hamilton, Canada; ARiEAL Research Centre, McMaster University, Hamilton, Canada; School of Biomedical Engineering, McMaster University, Hamilton, Canada; Vector Institute, Toronto, Canada; School of Biomedical Engineering, McMaster University, Hamilton, Canada; School of Biomedical Engineering, McMaster University, Hamilton, Canada; School of Biomedical Engineering, McMaster University, Hamilton, Canada","IEEE Transactions on Neural Systems and Rehabilitation Engineering","5 Jul 2019","2019","27","7","1492","1501","There has been increased effort to understand the neurophysiological effects of concussion aimed to move diagnosis and identification beyond current subjective behavioral assessments that suffer from poor sensitivity. Recent evidence suggests that event-related potentials (ERPs) measured with electroencephalography (EEG) are persistent neurophysiological markers of past concussions. However, as such evidence is limited to group-level analyzes, the extent to which they enable concussion detection at the individual-level is unclear. One promising avenue of research is the use of machine learning to create quantitative predictive models that can detect prior concussions in individuals. In this paper, we translate the recent group-level findings from ERP studies of concussed individuals into a machine learning framework for performing single-subject prediction of past concussion. We found that a combination of statistics of single-subject ERPs and wavelet features yielded a classification accuracy of 81% with a sensitivity of 82% and a specificity of 80%, improving on current practice. Notably, the model was able to detect concussion effects in individuals who sustained their last injury as much as 30 years earlier. However, failure to detect past concussions in a subset of individuals suggests that the clear effects found in group-level analyses may not provide us with a full picture of the neurophysiological effects of concussion.","1558-0210","","10.1109/TNSRE.2019.2922553","Canada Foundation for Innovation; Senator WilliamMcMaster Chair in Cognitive Neuroscience of Language (JFC); Natural Sciences and Engineering Research Council of Canada; Southern Ontario Smart Computing Innovation Platform (SOSCIP; OB, RS, and TD); Ontario Ministry of Research and Innovation (RB); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735773","Event-related potentials;brain injury;electroencephalography;EEG;concussions;machine learning;explainable models","Electroencephalography;Injuries;Machine learning;Standards;Electrodes;Brain;Sensitivity","brain;electroencephalography;injuries;learning (artificial intelligence);medical signal processing;neurophysiology","concussion detection;group-level analyzes;persistent neurophysiological markers;event-related potentials;current subjective behavioral assessments;machine learning detection;group-level statistics;neurophysiological effects;group-level analyses;past concussions;concussion effects;single-subject ERPs;single-subject prediction;machine learning framework;concussed individuals;ERP studies;recent group-level findings;quantitative predictive models;time 30.0 year","Athletes;Brain Concussion;Electroencephalography;Evoked Potentials;Humans;Machine Learning;Male;Middle Aged;Models, Neurological;Neuropsychological Tests;Reproducibility of Results;Wavelet Analysis","","","49","","12 Jun 2019","","","IEEE","IEEE Journals"
"Hand Gesture Detection Based Real-Time American Sign Language Letters Recognition using Support Vector Machine","X. Jiang; W. Ahmad","Glasgow College, University of Electronic Science and Technology of China; University of Glasgow","2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)","4 Nov 2019","2019","","","380","385","Sign language is an indispensable communication means for deaf-mute people because of their hearing impairment. At present, sign language is not a popular communication method among hearing people, so that the majority of the hearing is not willing to have a talk with the deaf-mute, or they have to spend much time and energy trying to figure out what the correct meaning is. Sign Language Recognition (SLR), which aims to translate sign language to people who know few about it in the form of text or speech, can be said to be a great help to deaf-mute and hearing people communicate. In this study, a real-time vision-based static hand gesture recognition system for sign language was developed. All data is collected from a USB camera connected to a computer, and no auxiliary items (such as gloves) were required. The proposed system is based on a skin color algorithm in HSV color space to find the Region of Interest (ROI), where hand gesture is. After completing all pre-processing work, 8 features were extracted from each sample using Principal Component Analysis (PCA). The recognition machine learning approach used was based on Support Vector Machine (SVM). The experimental results show that this system can distinguish B, D, F, L and U, these five American sign language hand gestures, with the success rate of about 99.4%.","","978-1-7281-3024-8","10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8890379","Sign Language Recognition, skin color algorithm, Principal Component Analysis, Support Vector Machine","Image color analysis;Gesture recognition;Skin;Assistive technology;Feature extraction;Lighting;Support vector machines","cameras;computer vision;feature extraction;handicapped aids;image colour analysis;learning (artificial intelligence);principal component analysis;sign language recognition;support vector machines","hand gesture detection;real-time american sign language letters recognition;support vector machine;deaf-mute people;hearing impairment;communication method;hearing people;real-time vision-based static hand gesture recognition system;machine learning approach;American sign language hand gestures;sign language recognition;SVM;principal component analysis;PCA;feature extraction;skin color algorithm;HSV color space;Region of Interest;ROI;USB camera","","2","","8","","4 Nov 2019","","","IEEE","IEEE Conferences"
"Early Detection and Continuous Quantization of Plant Disease Using Template Matching and Support Vector Machine Algorithms","R. Zhou; S. Kaneko; F. Tanaka; M. Kayamori; M. Shimizu","Graduation Sch. of Inf. Sci. & Technol., Hokkaido Univ., Sapporo, Japan; Graduation Sch. of Inf. Sci. & Technol., Hokkaido Univ., Sapporo, Japan; Central Agric. Exp. Station, Hokkaido Res. Organ., Naganuma, Japan; Central Agric. Exp. Station, Hokkaido Res. Organ., Naganuma, Japan; Central Agric. Exp. Station, Hokkaido Res. Organ., Naganuma, Japan","2013 First International Symposium on Computing and Networking","30 Jan 2014","2013","","","300","304","This paper present a novel method for robust and early Cercospora leaf spot detection in sugar beet using hybrid algorithms of template matching and support vector machine. We adopt three-stage framework to achieve our research target: first, a plant segmentation index of G-R is introduced to distinguish leaf parts from soil-contained background for automatic selection of initial sub templates, second, we adopt a robust template matching method called orientation code matching (OCM), which could not only realize the continuous and site-specific observation of disease development, but also shows its excellent robustness for nonrigid plant object searching in scene illumination, foliar translation and small rotation, afterward, we employ a machine learning method of support vector machine (SVM) for robust and early disease classification by a color-based feature named two dimensional (2D) xy-color histogram, which has stable ability to classify disease against various illumination changes. The indoor experiment results demonstrate the robust performance of our proposed method for early disease detection against complex changes of external environment.","2379-1896","978-1-4799-2796-8","10.1109/CANDAR.2013.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726914","disease detection;image processing;orientation code matching (OCM);support vector machine (SVM);sugar beet;cercospora leaf spot (CLS)","Diseases;Support vector machines;Image color analysis;Sugar industry;Robustness;Quantization (signal);Histograms","botany;image classification;image matching;learning (artificial intelligence);quantisation (signal);support vector machines","plant disease early detection;plant disease continuous quantization;template matching;support vector machine algorithms;machine learning method;SVM;early disease classification;color-based feature;2D xy-color histogram","","13","1","13","","30 Jan 2014","","","IEEE","IEEE Conferences"
"Brain Machine Interface for physically retarded people using colour visual tasks","M. P. Paulraj; A. H. Adom; C. R. Hema; D. Purushothaman","School of Mechatronic Engineering, University of Malaysia Perlis, Ulu Pauh Permanent Campus, 02600 Arau, Perlis, Malaysia; School of Mechatronic Engineering, University of Malaysia Perlis, Ulu Pauh Permanent Campus, 02600 Arau, Perlis, Malaysia; School of Mechatronic Engineering, University of Malaysia Perlis, Ulu Pauh Permanent Campus, 02600 Arau, Perlis, Malaysia; School of Mechatronic Engineering, University of Malaysia Perlis, Ulu Pauh Permanent Campus, 02600 Arau, Perlis, Malaysia","2010 6th International Colloquium on Signal Processing & its Applications","9 Aug 2010","2010","","","1","4","A Brain Machine Interface is a communication system which connects the human brain activity to an external device bypassing the peripheral nervous system and muscular system. It provides a communication channel for the people who are suffering with neuromuscular disorders such as amyotrophic lateral sclerosis, brain stem stroke, quadriplegics and spinal cord injury. In this paper, a simple BMI system based on EEG signal emanated while visualizing of different colours has been proposed. The proposed BMI uses the color visual tasks and aims to provide a communication through brain activated control signal for a system from which the required task operation can be performed to accomplish the needs of the physically retarded community. The ability of an individual to control his EEG through the colour visualization enables him to control devices. The EEG signal is recorded from 10 voluntary healthy subjects using the noninvasive scalp electrodes placed over the frontal, parietal, motor cortex, temporal and occipital areas. The obtained EEG signals were segmented and then processed using an elliptic filter. Using spectral analysis, the alpha, beta and gamma band frequency spectrum features are obtained for each EEG signals. The extracted features are then associated to different control signals and a neural network model using back propagation algorithm has been developed. The proposed method can be used to translate the colour visualization signals into control signals and used to control the movement of a mobile robot. The performance of the proposed algorithm has an average classification accuracy of 95.2%.","","978-1-4244-7122-5","10.1109/CSPA.2010.5545339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5545339","Brain Machine Interface;Colour visual tasks;Neural Network","Electroencephalography;Visualization;Communication system control;Brain;Humans;Nervous system;Communication channels;Neuromuscular;Spinal cord injury;Control systems","backpropagation;brain-computer interfaces;colour vision;electroencephalography;feature extraction;handicapped aids;medical robotics;medical signal processing;mobile robots;neurophysiology;signal classification;spectral analysis","brain-machine interface;physically retarded people;colour visual tasks;human brain activity;peripheral nervous system;muscular system;neuromuscular disorders;amyotrophic lateral sclerosis;brain stem stroke;quadriplegics;spinal cord injury;EEG;colour visualization;noninvasive scalp electrodes;signal segmentation;elliptic filter;spectral analysis;alpha band frequency spectrum features;beta band frequency spectrum features;gamma band frequency spectrum features;feature extraction;neural network model;backpropagation algorithm;mobile robot;signal classification","","1","","14","","9 Aug 2010","","","IEEE","IEEE Conferences"
"Electrocardiogram Data Capturing System By Using Machine Perception","U. M. Boppana; R. P.; U. C.; K. D. Priya","Dept. Of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India; Dept. of Comput. Sci. & Eng., Hindustan Inst. of Technol. & Sci., Chennai, India; Dept. Of Computer Science and Engineering, National Institute of Technology, Tiruchirappalli, Tamil Nadu; Dept. Of Computer Science and Engineering, Hindustan Institute of Technology and Science, Chennai, India","2019 1st International Conference on Innovations in Information and Communication Technology (ICIICT)","21 Jun 2019","2019","","","1","6","Obstructive Apnea is a breathing based sleeping problem, in which throat tissues drops back in the direction of air passages as well as obstructs the air flow, partly or completely during the rest. Because of lack of air movement in blood, oxygen levels will drop suddenly in boosting high blood pressure as well as strains cardiovascular system, leads to boost the danger of Cardiovascular diseases, Stroke, Excessive Weight, Diabetes Mellitus, High blood pressure, Myocardial infarction (MI) etc. where MI is a cardiovascular disease occurs when the circulation of blood, decreases or quits to a part of the heart, which it brings about heart damage. One of commonly detected method for sleeping conditions is Polysomnography (PSG), which is a lot pricier as well as eats much effort, as a result of these reasons in a lot of the cases sleep problems were undiagnosed. To get over the drawbacks of Polysomnography in an affordable way and in less initiative, existing system uses ECG signals in order to identify OSA. ECG is mostly used to diagnose cardiovascular disease, in order to find MI from ECG where as in traditional standard system assessment of ECG signals via visual analysis can be done by physicians or medical professionals is not effective and time consuming. This paper proposes a system to conquer the disadvantage of standard system, by translating the exact ECG by utilizing machine perception and to identify the problems in ECG, mainly concentrates on OSA as well as Myocardial infraction. Machine perception is the capability of a system in order to interpret the data based on exactly how human beings detects and connects the world around them. It permits the system to collect the information based upon equipment vision with better accuracy as well as to provide it in a form which is more comfortable to the user. Computer vision mostly focuses on acquiring, processing, analyzing, and recognizing images. Here the input information is absorbed kind of images as opposed to signals for accurate ECG interpretation. This can be done by using wavelet changes and Auto Regression (AR) approaches and should able to distinguish between regular ECG and also uncommon ECG and it can be done by using improvised classification Algorithm based on integration of both K-Medoid and also improvised KNN classifier is used to reduce the computation complexity and also enhance the precision by using the hyper tuning parameters.","","978-1-7281-1604-4","10.1109/ICIICT1.2019.8741363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741363","Obstructive sleep Apnea (OSA);Electrocardiogram (ECG);Myocardial infraction (MI);Polysomnography (PSG);Machine Perception;Wavelet Transforms;Auto Regression (AR);Classification based KNN Algorithm;Clustered based K-means","Electrocardiography;Sleep apnea;Myocardium;Heart beat;Wavelet transforms;Computer science","cardiovascular system;diseases;electrocardiography;medical signal processing;pneumodynamics;sleep","electrocardiogram data capturing system;machine perception;throat tissues;air passages;air flow;air movement;high blood pressure;cardiovascular disease;heart damage;ECG signals;OSA;traditional standard system assessment;obstructive apnea;cardiovascular system;myocardial infarction;polysomnography;ECG interpretation;breathing-based sleeping problem;computer vision;autoregression approache;K-Medoid method;KNN classifier","","","","12","","21 Jun 2019","","","IEEE","IEEE Conferences"
"Color plane slicing and its applications to motion characterization for machine vision","A. V. Varun; S. Arya; V. Bagaria; H. Jhunjhunwala; G. Padma Madhuri; Z. Varghese","Siemens Corporate Technology, India; Indian Institute of Technology, Kanpur, India; Siemens Corporate Technology, India; Dhirubhai Ambani Institute of Infomation and Communication Technology, India; Siemens Corporate Technology, India; Siemens Corporate Technology, India","2009 IEEE/ASME International Conference on Advanced Intelligent Mechatronics","1 Sep 2009","2009","","","590","594","In this paper we propose the color plane slicing method, a technique to achieve a higher temporal resolution of a phenomena of interest than what is typically achievable by cameras. This is done by using a color camera and strobe lights of different colors at small intervals of time apart. The method, although simple, poses several advantages and applications to machine vision sensors which currently only inspect objects but do not characterize their motion in the context of automation and inspection. Although, the color information is traded for temporal information, we show that the framework is not restrictive for objects with or without colors. Also, the system can continue to be used as a traditional camera for quality inspection with a local translation applied to the formed composite image. Further, potential applications are discussed wherein the system is appended to an industrial grade Smart camera for ease of use. The applicability of the technique is also shown for motion characterization and vibration measurement.","2159-6255","978-1-4244-2852-6","10.1109/AIM.2009.5229947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5229947","Color plane Slicing;Speed measurement;Machine Vision;motion characterization;Smart camera;Vibration Measurement","Machine vision;Smart cameras;Inspection;Intelligent sensors;Acceleration;Sensor phenomena and characterization;Automation;Vibration measurement;Velocity measurement;Assembly","cameras;computer vision;image colour analysis;image motion analysis;image sensors","color plane slicing;motion characterization;color camera;strobe light;machine vision sensor;color information;quality inspection;vibration measurement;smart camera","","1","1","7","","1 Sep 2009","","","IEEE","IEEE Conferences"
"Comparing ANN, SVM, and HMM based Machine Learning Methods for American Sign Language Recognition using Wearable Motion Sensors","R. Fatmi; S. Rashad; R. Integlia","College of Innovation and Technoloy, Florida Polytechnic University, Lakeland, FL, USA; School of Engineering and Information, Systems Morehead State University, Morehad, KY, USA; College of Engineering, Florida Polytechnic University, Lakeland, FL, USA","2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)","14 Mar 2019","2019","","","0290","0297","Millions of people with speech and hearing impairments, worldwide, communicate through sign languages every day. In the same way that voice recognition provides a simple communication platform for most users, gesture recognition is a natural means of correspondence for the hearing-impaired. In this paper, we explore the problem of translating/converting sign language to speech, and propose an improved solution using different machine learning techniques. We seek to build a system that can be employed in the daily lives of people with hearing impairments, in order to enhance communication and collaboration between the hearing-impaired community and those untrained in American Sign Language (ASL). The system architecture is based on using wearable motion sensors and machine learning techniques. In this study, we propose a solution using Artificial Neural Networks (ANN) and Support Vector Machines (SVM), and compare their accuracy with the Hidden Markov Model (HMM) results from our previous work to recognize ASL words. Experimental results show that using ANN gives an overall higher accuracy in recognizing ASL words, compared to other machine learning techniques.","","978-1-7281-0554-3","10.1109/CCWC.2019.8666491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666491","machine learning;sign language recognition;human computer interaction;artificial neural networks;wearable technology","Gesture recognition;Assistive technology;Sensors;Electromyography;Hidden Markov models;Artificial neural networks;Image recognition","handicapped aids;hidden Markov models;learning (artificial intelligence);neural nets;sign language recognition;speech recognition;support vector machines;wearable computers","ANN;SVM;wearable motion sensors;voice recognition;gesture recognition;hearing-impaired community;HMM;machine learning techniques;support vector machines;hidden Markov model;american sign language;ASL recognition;artificial neural network","","4","","27","","14 Mar 2019","","","IEEE","IEEE Conferences"
"Utalk: Sri Lankan Sign Language Converter Mobile App using Image Processing and Machine Learning","I. S. M. Dissanayake; P. J. Wickramanayake; M. A. S. Mudunkotuwa; P. W. N. Fernando","Sri Lanka Institute of Information Technology,Malabe,Sri Lanka,10115; Sri Lanka Institute of Information Technology,Malabe,Sri Lanka,10115; Sri Lanka Institute of Information Technology,Malabe,Sri Lanka,10115; Sri Lanka Institute of Information Technology,Malabe,Sri Lanka,10115","2020 2nd International Conference on Advancements in Computing (ICAC)","26 Feb 2021","2020","1","","31","36","Deaf and mute people face various difficulties in daily activities due to the communication barrier caused by the lack of Sign Language knowledge in the society. Many researches have attempted to mitigate this barrier using Computer Vision based techniques to interpret signs and express them in natural language, empowering deaf and mute people to communicate with hearing people easily. However, most of such researches focus only on interpreting static signs and understanding dynamic signs is not well explored. Understanding dynamic visual content (videos) and translating them into natural language is a challenging problem. Further, because of the differences in sign languages, a system developed for one sign language cannot be directly used to understand another sign language, e.g., a system developed for American Sign Language cannot be used to interpret Sri Lankan Sign Language. In this study, we develop a system called Utalk to interpret static as well as dynamic signs expressed in Sri Lankan Sign Language. The proposed system utilizes Computer Vision and Machine Learning techniques to interpret sings performed by deaf and mute people. Utalk is a mobile application, hence it is non-intrusive and cost-effective. We demonstrate the effectiveness of the our system using a newly collected dataset.","","978-1-7281-8412-8","10.1109/ICAC51239.2020.9357300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9357300","Sinhala Sign Language;Computer Vision;Machine Learning","Computer vision;Assistive technology;Natural languages;Gesture recognition;Machine learning;Mobile applications;Videos","computer vision;handicapped aids;learning (artificial intelligence);mobile computing;natural language processing;sign language recognition;video signal processing","Utalk;Sri Lankan Sign Language converter mobile app;mute people;Sign Language knowledge;Computer Vision;natural language;deaf people;static signs;understanding dynamic signs;understanding dynamic visual content;American Sign Language","","","","24","","26 Feb 2021","","","IEEE","IEEE Conferences"
"Implementation of a Novel LED Backlight Device Used for Glass Bottle Detection","Z. Li-Li; W. Yan-Hua; Z. Xue-Feng; L. Hong-Yu","Coll. of Inf. & Electr. Eng., Shandong Univ. of Sci. & Technol., Qingdao, China; Coll. of Inf. & Electr. Eng., Shandong Univ. of Sci. & Technol., Qingdao, China; Tiandi (Changzhou) Autom. Co., Ltd., Changzhou, China; Coll. of Mech. & Electr. Eng., Shandong Univ. of Sci. & Technol., Qingdao, China","2013 Seventh International Conference on Image and Graphics","24 Oct 2013","2013","","","766","769","A novel LED backlight device is introduced in this paper, which is mainly used for transparent or translucent glass bottle detection relevant to machine vision. A special controlling and drive circuit is adopted in this LED backlight device to realize independent control of each color and rapid and convenient change of many colors and luminance. Because the area of LED backlight can be changed at random according to the size of tested product, many application requirements will be met easily. On the other hand, safe and effective cooling equipment is used to improve stability of the whole system.","","978-0-7695-5050-3","10.1109/ICIG.2013.153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643772","LED backlight;Machine vision;Drive circuit;Cooling equipment","Light sources;LED lamps;Machine vision;Heating;Image color analysis;Educational institutions","brightness;computer vision;driver circuits;image colour analysis;light emitting diodes;object detection","novel LED backlight device;transparent glass bottle detection;translucent glass bottle detection;drive circuit;controlling circuit;color control;cooling equipment","","","","7","","24 Oct 2013","","","IEEE","IEEE Conferences"
"Walking Through Shanshui: Generating Chinese Shanshui Paintings via Real-Time Tracking of Human Position","A. L. Zhou",artMachines,"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3185","3188","Shanshui is a traditional East Asian style of ink brush painting that depicts natural landscapes in a semi-abstract fashion. To create a Shanshui painting, ancient Chinese scholar-artists rely heavily on their travel experiences as well as their movements in natural spaces. In this paper, we propose an interactive system - ""Walking Through Shanshui"" - based on AI using Generative Adversarial Networks and various computer vision techniques. The system is an interactive art installation that helps bring the original experience of creating Shanshui to participants by tracking their movement through walking in a virtual space. It uses position tracking as input to generate Shanshui from participant's movements and to paint with a custom generative Sketch-to-Shanshui translation model. The system detects the participant's position in real-time and automatically traces it to generate a Shanshui painting instantly.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022422","AI Art;Chinese Heritage;Shanshui;Generative Machine Learning;Generative Art;Interactive Installation;Custom Data Set","Painting;Artificial intelligence;Art;Real-time systems;Legged locomotion;Tracking;TV","art;computer vision;interactive systems;neural nets;object tracking;painting","Chinese Shanshui paintings;real-time tracking;traditional East Asian style;ink brush painting;ancient Chinese scholar-artists;generative adversarial networks;position tracking;Sketch-to-Shanshui translation model;computer vision techniques;interactive system","","","","7","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Shape based approach for detecting Musa Species in fruit industry","M. Senthilarasi; S. M. M. Roomi; M. R. H. Prasanna","Department of Electronics and Communication Engineering at Thiagarajar College of Engineering, Madurai, India; Department of Electronics and Communication Engineering at Thiagarajar College of Engineering, Madurai, India; Department of Electronics and Communication Engineering at Thiagarajar College of Engineering, Madurai, India","2014 Sixth International Conference on Advanced Computing (ICoAC)","31 Aug 2015","2014","","","157","160","Agro export industries generate a substantial amount of revenue to Indian economy. In the fruit industry, various fruits like banana, mango, apple and pomegranate, etc. are transported in the conveyor for a post harvest process like classification, sorting, grading and juice extraction. The manual discrimination of various fruits consumes time and, it can be automated. This research work is intended to build an image processing algorithm that ensures automatic discrimination of banana (Musa Species.) from other fruits like Citrus, Apple, and Pomegranate. The input object is segmented using Background subtraction and threshold method. Morphological operations are performed to obtain the clear contour of the segmented objects. The shape of the banana and non-banana are described by scale and translation invariant signature. Binary SVM with signature feature vectors detect the banana fruit from the non-banana fruit automatically. The accuracy rate is 95%.","2377-6927","978-1-4799-8159-5","10.1109/ICoAC.2014.7229765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7229765","Background Subtraction;Post harvest technology;Signature;Support Vector Machine","Electronic mail;Indexes;Image segmentation;Kernel;Accuracy;Polynomials;Nose","agriculture;image segmentation;object detection;shape recognition;support vector machines","shape based approach;Musa species detection;fruit industry;image processing algorithm;automatic banana discrimination;object segmentation;background subtraction;threshold method;morphological operations;scale invariant signature;translation invariant signature;binary SVM;signature feature vectors;banana fruit detection","","","","15","","31 Aug 2015","","","IEEE","IEEE Conferences"
"Placement repeatability study of two flip chip die attach machines: leadscrew vs. linear motor","Lih-Tyng Hwang; J. Pearch","Adv. Interconnect Syst. Lab., Motorola Inc., Tempe, AZ, USA; NA","1999 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (Cat. No.99TH8399)","6 Aug 2002","1999","","","478","483","In the development of fine pitch flip chip technology, the die attach placement is considered as one of the most critical steps, since it dictates the assembly yield of the fine pitch process. The purpose of the study is to evaluate the placement repeatability of two flip chip die placement machines: one using leadscrew, the other using a linear motor. The study was conducted using a clear quartz die screen-printed with solder bumps. The quartz die were flipped, and placed onto a substrate. The die was firmly secured on the substrate using a transparent double-sided tape. The bumps near the four corners were measured using a high accuracy, automatic focusing optical microscope, and misplacements were calculated. By plotting X and Y misplacements of the four designated bumps on the quartz die, the rotational and translational placement errors were decomposed, and independently obtained. There are four spindles in each machine. By comparing the errors with the placement specifications, the placement capability was obtained. It was found that Machine B (linear motor) was more capable than Machine A (leadscrew), for all four spindles in both translational and rotational movements. Machine B was capable of delivering a 6-sigma process for a pitch of 225 /spl mu/m, with /spl plusmn/45 /spl mu/m specification limits. However, it was not quite capable of delivering a 6-sigma process for the fine pitch (150 /spl mu/m, with a /spl plusmn/30 /spl mu/m specification limits). It was also found that the reflected light from the spindle heads affected significantly the placement accuracy, because the light reflection influenced the effectiveness of the machine vision. Since the error analysis was relatively easy to perform, the error handling technique was used to optimize the die placement accuracy by altering the die placement sequence.","","0-7803-5038-3","10.1109/AIM.1999.803217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=803217","","Flip chip;Microassembly;Lead;Substrates;Optical microscopy;Error analysis;Laboratories;Assembly systems;Focusing;Head","microassembling;flip-chip devices;linear motors;computer vision;process control","placement repeatability;flip chip die attach machines;leadscrew;linear motor;fine pitch flip chip technology;assembly yield;clear quartz die;solder bumps;automatic focusing optical microscope;misplacements;6-sigma process;light reflection;machine vision;error analysis;error handling technique;die placement sequence","","","","2","","6 Aug 2002","","","IEEE","IEEE Conferences"
"A framework for recognizing and segmenting sign language gestures from continuous video sequence using boosted learning algorithm","R. Elakkiya; K. Selvamani; S. Kanimozhi","DCSE, Agni College of Technology, Chennai, India; DCSE, Anna University, Chennai, India; DIST, Anna University, Chennai, India","2014 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT)","3 Apr 2014","2014","","","498","503","The problem of vision-based sign language recognition, which is used to translate signs to English sentence, is addressed in this paper. A fully automatic system to recognize signs that starts with breaking up signs into manageable subunits is proposed. A framework for segmenting and tracking skin objects from signing videos is described. A boosting algorithm to learn a subset of weak classifiers for extracted features to combine them into a strong classifier for each sign is then applied. A joint learning strategy to share subunits across sign classes is adopted, which leads to a more efficient classification of sign gestures. Experimental results shown by the system demonstrate that the proposed approach is promising to build an effective and scalable system on real-world hand gesture recognition from continuous video sequences.","","978-1-4799-2900-9","10.1109/ICICICT.2014.6781333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781333","Sign language recognition;machine learning;boosted subunits;support vector machine","Image color analysis;Support vector machines;Classification algorithms;Feature extraction;Training;Image segmentation;Indexes","computer vision;image classification;image colour analysis;image segmentation;image sequences;learning (artificial intelligence);object tracking;sign language recognition;skin;support vector machines;video signal processing","sign language gesture recognition;sign language gesture segmentation;continuous video sequence;boosted learning algorithm;vision-based sign language recognition;sign translation;English sentence;skin object tracking;signing videos;weak classifiers;feature extraction;strong classifier;joint learning strategy;sign gesture classification;skin colour model;support vector machines;SVM","","2","","17","","3 Apr 2014","","","IEEE","IEEE Conferences"
"Multilevel Training of Binary Morphological Operators","N. S. T. Hirata","University of São Paulo, São Paulo","IEEE Transactions on Pattern Analysis and Machine Intelligence","20 Feb 2009","2009","31","4","707","720","The design of binary morphological operators that are translation-invariant and locally defined by a finite neighborhood window corresponds to the problem of designing Boolean functions. As in any supervised classification problem, morphological operators designed from training sample also suffer from overfitting. Large neighborhood tends to lead to performance degradation of the designed operator. This work proposes a multi-level design approach to deal with the issue of designing large neighborhood based operators. The main idea is inspired from stacked generalization (a multi-level classifier design approach) and consists in, at each training level, combining the outcomes of the previous level operators. The final operator is a multi-level operator that ultimately depends on a larger neighborhood than of the individual operators that have been combined. Experimental results show that two-level operators obtained by combining operators designed on subwindows of a large window consistently outperforms the single-level operators designed on the full window. They also show that iterating two-level operators is an effective multi-level approach to obtain better results.","1939-3539","","10.1109/TPAMI.2008.118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4522558","Morphological;Statistical;Classifier design and evaluation;Simplification of expressions;Concept learning;Machine learning;Pattern Recognition;Image Processing and Computer Vision;Morphological;Statistical;Classifier design and evaluation;Simplification of expressions;Concept learning;Machine learning;Pattern Recognition;Image Processing and Computer Vision","Boolean functions;Degradation;Machine learning;Pattern recognition;Industrial training;Signal processing;Biomedical image processing;Geoscience and remote sensing;Industrial relations;Probes","Boolean functions;image classification;iterative methods;learning (artificial intelligence);mathematical morphology;mathematical operators","binary morphological operator;multilevel training;finite neighborhood window;Boolean function;supervised classification problem;multilevel classifier design approach;translation-invariant image operator;image processing;stacked generalization;machine learning;iterative two-level operator","","19","","35","","16 May 2008","","","IEEE","IEEE Journals"
"Automatic image hanging protocol for chest radiographs in PACS","Hui Luo; Wei Hao; D. H. Foos; C. W. Cornelius","Kodak R&D Labs., Eastman Kodak Co., Rochester, NY, USA; Kodak R&D Labs., Eastman Kodak Co., Rochester, NY, USA; Kodak R&D Labs., Eastman Kodak Co., Rochester, NY, USA; Kodak R&D Labs., Eastman Kodak Co., Rochester, NY, USA","IEEE Transactions on Information Technology in Biomedicine","3 Apr 2006","2006","10","2","302","311","Chest radiography is one of the most widely used techniques in diagnostic imaging. It comprises at least one-third of all diagnostic radiographic procedures in hospitals. However, in the picture archive and communication system, images are often stored with the projection and orientation unknown or mislabeled, which causes inefficiency for radiologists' interpretation. To address this problem, an automatic hanging protocol for chest radiographs is presented. The method targets the most effective region in a chest radiograph, and extracts a set of size-, rotation-, and translation-invariant features from it. Then, a well-trained classifier is used to recognize the projection. The orientation of the radiograph is later identified by locating the neck, heart, and abdomen positions in the radiographs. Initial experiments are performed on the radiographs collected from daily routine chest exams in hospitals and show promising results. Using the presented protocol, 98.2% of all cases could be hung correctly on projection view (without protocol, 62%), and 96.1% had correct orientation (without protocol, 75%). A workflow study on the protocol also demonstrates a significant improvement in efficiency for image display","1558-0032","","10.1109/TITB.2005.859872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1613956","Bayesian learning framework;chest radiography;hanging protocol;picture archive and communication system (PACS)","Protocols;Picture archiving and communication systems;Diagnostic radiography;Hospitals;Chromium;Biomedical imaging;Medical diagnostic imaging;X-ray imaging;Neck;Heart","Bayes methods;belief networks;diagnostic radiography;feature extraction;medical image processing;PACS","automatic image hanging protocol;chest radiograph;PACS;diagnostic radiographic imaging;picture archive;communication system;translation-invariant feature extraction;Bayesian learning framework","Algorithms;Artificial Intelligence;Bayes Theorem;Database Management Systems;Humans;Information Storage and Retrieval;Pattern Recognition, Automated;Radiographic Image Interpretation, Computer-Assisted;Radiography, Thoracic;Radiology Information Systems","13","7","28","","3 Apr 2006","","","IEEE","IEEE Journals"
"A NEW IMAGE ANALYSIS APPROACH FOR AUTOMATIC CLASSIFICATION OF AUTISTIC BRAINS","A. El-Baz; M. F. Casanova; G. Gimel'farb; M. Mott; A. E. Switwala","Dept. Bioeng., Louisville Univ., KY; NA; NA; NA; NA","2007 4th IEEE International Symposium on Biomedical Imaging: From Nano to Macro","15 May 2007","2007","","","352","355","Autism is a developmental disorder characterized by social deficits, impaired communication, and restricted and repetitive patterns of behavior. Recent neuropathological studies of autism have revealed abnormal anatomy of the cerebral white matter (CWM) in autistic brains. In this paper we introduced a novel approach to classify autistic from normal subjects based on a new shape analysis of cerebral white matter gyrifications for both normal and autistic subjects. The proposed shape analysis technique consists of three main steps. The first step is to segment cerebral white matter from proton density MRI images using a priorly learned visual appearance model for the 3D cerebral white matter in order to control the evolution of deformable boundaries. The appearance prior is modeled with a translation and rotation invariant Markov-Gibbs random field of voxel intensities with a pairwise interaction model. The second step is to extract the gyrifications of cerebral white matter from the segmented cerebral white matter. The last step is to perform shape analysis to quantify the thickness of the extracted cerebral white matter gyrifications for both autistic and normal subjects. The preliminary results of the proposed image analysis has yielded promising results that would, in the near future, supplement the use of current technologies for diagnosing autism","1945-8452","1-4244-0671-4","10.1109/ISBI.2007.356861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4193295","","Image analysis;Autism;Image segmentation;Magnetic resonance imaging;Deformable models;Protons;Image sequence analysis;Shape measurement;Shape control;Autopsy","biomedical MRI;brain;diseases;image classification;image segmentation;Markov processes;medical image processing;neurophysiology","image analysis;automatic image classification;autistic brains;autism;developmental disorder;social deficits;impaired communication;restricted behavioral patterns;repetitive behavioral patterns;neuropathological studies;abnormal anatomy;cerebral white matter;shape analysis;cerebral white matter gyrifications;normal subjects;autistic subjects;cerebral white matter segmentation;deformable boundaries;translation invariant Markov-Gibbs random field;rotation invariant Markov-Gibbs random field;voxel intensities;pairwise interaction model;gyrification extraction;autism diagnosis","","7","","13","","15 May 2007","","","IEEE","IEEE Conferences"
"Automatic Segmentation of Optic Disc Using Affine Snakes in Gradient Vector Field","S. Dey; K. Tahiliani; J. R. Harish Kumar; A. K. Pediredla; C. S. Seelamantula","Department of Electrical and Electronics Engineering, Manipal Institute of Technology, Manipal, India; Department of Electrical and Electronics Engineering, Manipal Institute of Technology, Manipal, India; Department of Electrical and Electronics Engineering, Manipal Institute of Technology, Manipal, India; Department of Electrical and Computer Engineering, Rice University, Houston, USA; Department of Electrical Engineering, Indian Institute of Science, Bangalore, India","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","1204","1208","The optic disc is one of the prominent features of a retinal fundus image, and its segmentation is a critical component in automated retinal screening systems for ophthalmic anomalies, such as diabetic retinopathy and glaucoma. In this paper, we propose a novel method for optic disc segmentation using affine snakes, where the snake evolves using an affine transformation and requires a priori knowledge of the desired object shape. We determine the affine transformation parameters by first computing a force field on the image and then deforming the snake till the net force on the snake is zero. The affine snakes technique excels in its speed of convergence. This is attributed to the fact that only six parameters require optimization, the six parameters being the horizontal and vertical scaling, shearing and translation components of an affine transformation. Localization of the optic disc is done using normalized cross-correlation and segmentation is done using the affine snakes technique. This technique is tested on publicly available fundus image datasets, such as IDRiD, Drishti-GS, RIM-ONE, DRIONS-DB, and Messidor, with Dice In-dices of 0.943, 0.958, 0.933, 0.913, and 0.912, respectively.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682408","Affine snakes;optic disc;segmentation;affine transformation;gradient vector field","Optical imaging;Optical sensors;Image segmentation;Retina;Force;Indexes;Shape","affine transforms;biomedical optical imaging;blood vessels;diseases;eye;feature extraction;image classification;image resolution;image segmentation;medical image processing","shearing;force field;affine transformation parameters;desired object shape;snake;optic disc segmentation;glaucoma;diabetic retinopathy;automated retinal screening systems;critical component;retinal fundus image;prominent features;gradient vector field;affine snakes;automatic segmentation;publicly available fundus image datasets;translation components","","2","","26","","17 Apr 2019","","","IEEE","IEEE Conferences"
"Automatic extraction of control points for digital subtraction angiography image enhancement","Y. Bentoutou; N. Taleb","Nat. Center of Space Techniques, Arzew, Algeria; NA","2003 IEEE Nuclear Science Symposium. Conference Record (IEEE Cat. No.03CH37515)","8 Nov 2004","2003","4","","2771","2775 Vol.4","In this paper, a new automatic control point selection and matching technique for digital subtraction angiography (DSA) image enhancement is proposed. The characteristic of this approach is that it uses features based on image moments and invariant to symmetric blur, translation, and rotation to establish correspondences between matched regions from two X-ray images. The automatic extraction of control points is based on an edge detection approach and on local similarity detection by means of template matching according to a combined invariants-based similarity measure. A new strategy was developed in which a 3D space-time motion detection algorithm was used for selecting movement points belonging to moving structures. The proposed technique has been successfully applied to register several clinical data sets including coronary applications. The experimental results demonstrate the efficiency and accuracy of the algorithm which have outperformed manual registration in terms of root mean square error at the movement points.","1082-3654","0-7803-8257-9","10.1109/NSSMIC.2003.1352461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1352461","","Digital control;Automatic control;Angiography;Image enhancement;Image registration;X-ray imaging;Image edge detection;Motion detection;Manuals;Root mean square","diagnostic radiography;medical image processing;image motion analysis;medical computing;cardiology;mean square error methods","automatic control point extraction;automatic control point selection;digital subtraction angiography image enhancement;image moments;symmetric blur;translation;rotation;X-ray images;edge detection approach;local similarity detection;template matching;combined invariants-based similarity measure;3D space-time motion detection algorithm;movement point selection;moving structures;clinical data sets;coronary applications;root mean square error;image registration","","1","","9","","8 Nov 2004","","","IEEE","IEEE Conferences"
"Object Tracking by Asymmetric Kernel Mean Shift with Automatic Scale and Orientation Selection","A. Yilmaz","Ohio State University, Photogrammetric Computer Vision Laboratory. yilmaz.15@osu.edu","2007 IEEE Conference on Computer Vision and Pattern Recognition","16 Jul 2007","2007","","","1","6","Tracking objects using the mean shift method is performed by iteratively translating a kernel in the image space such that the past and current object observations are similar. Traditional mean shift method requires a symmetric kernel, such as a circle or an ellipse, and assumes constancy of the object scale and orientation during the course of tracking. In a tracking scenario, it is not uncommon to observe objects with complex shapes whose scale and orientation constantly change due to the camera and object motions. In this paper, we present an object tracking method based on the asymmetric kernel mean shift, in which the scale and orientation of the kernel adaptively change depending on the observations at each iteration. Proposed method extends the traditional mean shift tracking, which is performed in the image coordinates, by including the scale and orientation as additional dimensions and simultaneously estimates all the unknowns in a few number of mean shift iterations. The experimental results show that the proposed method is superior to the traditional mean shift tracking in the following aspects: 1) it provides consistent object tracking throughout the video; 2) it is not effected by the scale and orientation changes of the tracked objects; 3) it is less prone to the background clutter.","1063-6919","1-4244-1179-3","10.1109/CVPR.2007.382987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270012","","Kernel;Bandwidth;Computer vision;Shape;Anisotropic magnetoresistance;Tracking;Robustness;Image segmentation;Data analysis;Laboratories","computer vision;iterative methods;object detection;set theory;tracking","object tracking;asymmetric kernel mean shift;automatic scale;orientation selection;iterative kernel translation;image space;image coordinates;computer vision;level set formalism","","119","5","13","","16 Jul 2007","","","IEEE","IEEE Conferences"
"Automatic Area-Based Registration of Optical and SAR Images Through Generative Adversarial Networks and a Correlation-Type Metric","L. Maggiolo; D. Solarna; G. Moser; S. B. Serpico","University of Genoa,Genoa,Italy,16145; University of Genoa,Genoa,Italy,16145; University of Genoa,Genoa,Italy,16145; University of Genoa,Genoa,Italy,16145","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2089","2092","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural properties of the input data. In the present paper, this problem is addressed in the case of optical-SAR images by proposing a novel method based on deep learning and area-based registration concepts. The method integrates a conditional generative adversarial network (cGAN), an area-based cross-correlation-type l2 similarity metric, and the COBYLA constrained maximization algorithm. Whereas correlation-type metrics are typically ineffective in the application to multisensor registration, the proposed approach allows exploiting the image translation capabilities of cGAN architectures to enable the use of an l2 similarity metric, which favors high computational efficiency. Experiments with Sentinel-1 and Sentinel-2 data suggest the effectiveness of this strategy and the capability of the proposed method to achieve accurate registration.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323235","European Space Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323235","Multisensor image registration;conditional generative adversarial network;$\ell^{2}$ similarity;COBYLA","Radar polarimetry;Optical imaging;Measurement;Optical sensors;Feature extraction;Training;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;image registration;radar imaging;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","automatic area-based registration;generative adversarial networks;correlation-type metric;automatic registration;multisensor remote sensing images;textural properties;optical-SAR images;deep learning;area-based registration concepts;conditional generative adversarial network;area-based cross-correlation-type l;image translation capabilities;Sentinel-2 data","","","","14","","17 Feb 2021","","","IEEE","IEEE Conferences"
"Semi-Automatic Tool for Dynamic Contour Tracking in Image-Guided Ultrasound Procedures","G. Correia; R. Cortesão","Instituto de Biofísica e Engenharia Biomédica, Faculdade de Ciências da Universidade de Lisboa,Lisbon,Portugal; Institute of Systems and Robotics, University of Coimbra,Electrical and Computer Engineering Department,Portugal","2020 IEEE International Conference on Industrial Technology (ICIT)","16 Apr 2020","2020","","","686","691","Tracking tools for target structures in ultrasound imaging are a crucial point in the development of semiautonomous image guided procedures. This paper presents an algorithm for semi-autonomous target-structure tracking, which uses a parametric active contour model with a Fourier descriptor formulation. The algorithm is tested in a soft real-time scenario, and its robustness is analysed w.r.t. probe translation and rotation, including also target structure deformations. In terms of temporal performance, the algorithm implemented in MATLAB has shown the ability to operate easily at 25 fps when coupled with an ultrasound probe, showing however capability to reach 40 fps when tested as a standalone algorithm.","2643-2978","978-1-7281-5754-2","10.1109/ICIT45562.2020.9067109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9067109","Ultrasound imaging;Image-guided procedures;Visual servoing;Active contours;Image moments","Active contours;Visual servoing;Mathematical model;Computational modeling;Ultrasonic imaging;Imaging","biomedical ultrasonics;edge detection;feature extraction;image motion analysis;image segmentation;medical image processing;object detection;target tracking","semiautomatic tool;dynamic contour tracking;image-guided ultrasound procedures;tracking tools;target structures;ultrasound imaging;crucial point;semiautonomous image;semiautonomous target-structure tracking;parametric active contour model;Fourier descriptor formulation;soft real-time scenario;probe translation;target structure deformations;ultrasound probe;standalone algorithm","","","","15","","16 Apr 2020","","","IEEE","IEEE Conferences"
"Hybrid neural networks for automatic target recognition","J. Waldemark; V. Becanovic; T. Lindblad; C. S. Lindsey","Dept. of Phys., R. Inst. of Technol., Stockholm, Sweden; NA; NA; NA","1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation","6 Aug 2002","1997","4","","4016","4021 vol.4","The paper presents a hybrid neural network system for automatic target recognition, or ATR. The ATR system uses a hybrid of a biological inspired neural net called the Pulse Coupled Neural Net, PCNN, and traditional feedforward neural nets. The PCNN is an iterative neural network in which, for example, a grey scale input image results in a 1D time signal invariant to rotation, scale and translation alternations. The PCNN can also extract edges, perform object segmentation and extract texture information. The PCNN pre-processor generates a 1D time signal that is input to a feedforward pattern recognition net.","1062-922X","0-7803-4053-1","10.1109/ICSMC.1997.633300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=633300","","Neural networks;Target recognition;Image segmentation;Feedforward neural networks;Feeds;Pattern recognition;Data mining;Signal generators;Physics;Object segmentation","object recognition;feedforward neural nets;multilayer perceptrons;edge detection;image texture;image segmentation","automatic target recognition;hybrid neural network system;biological inspired neural net;Pulse Coupled Neural Net;feedforward neural nets;iterative neural network;grey scale input image;1D time signal;translation invariance;scale invariance;rotation invariance;edge extraction;object segmentation;texture information;pre-processor;feedforward pattern recognition network","","5","1","27","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Advances in the automatic transcription of lectures","M. Cettolo; F. Brugnara; M. Federico","Centro per la Ricerca Scientifica e Tecnologica, ITC-irst, Povo Di Trento, Italy; Centro per la Ricerca Scientifica e Tecnologica, ITC-irst, Povo Di Trento, Italy; Centro per la Ricerca Scientifica e Tecnologica, ITC-irst, Povo Di Trento, Italy","2004 IEEE International Conference on Acoustics, Speech, and Signal Processing","30 Aug 2004","2004","1","","I","769","Transcribing lectures is a challenging task, both in acoustic and in language modeling. In this work, we present recent results on the automatic transcription of lectures from the Translanguage English Database, which contains the recordings of talks given in English at Eurospeech '93, by mostly non-native speakers. Concerning acoustic modeling, the acoustic model trained for a broadcast news transcription task was adapted on the lectures training data through maximum likelihood linear regression adaptation, including models of spontaneous speech phenomena. Moreover, a normalization procedure was embodied in the training stage, consisting of a cluster-based mean and variance normalization of the static features. Language modeling was based on adaptation of a background language model estimated on broadcast news transcripts, conference proceedings, lecture transcripts, and conversational speech transcripts. Among the examined adaptation techniques, the most effective one was obtained by exploiting the paper presented in each lecture to be processed. The best transcription performance on a 2 hours test set was 32.4% word error rate.","1520-6149","0-7803-8484-9","10.1109/ICASSP.2004.1326099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1326099","","Natural languages;Broadcasting;Speech;Databases;Loudspeakers;Training data;Maximum likelihood linear regression;Adaptation model;Conference proceedings;Testing","speech recognition;maximum likelihood estimation;regression analysis","automatic lecture transcription;acoustic language modeling;Translanguage English Database;Eurospeech '93;broadcast news transcription task;training data;maximum likelihood linear regression adaptation;spontaneous speech phenomena;cluster-based mean;variance normalization;background language model;broadcast news transcripts;conference proceedings;lecture transcripts;conversational speech transcripts;word error rate","","4","","9","","30 Aug 2004","","","IEEE","IEEE Conferences"
"Automatic Prosodic Event Detection Using Acoustic, Lexical, and Syntactic Evidence","S. Ananthakrishnan; S. S. Narayanan","Signal & Image Process. Inst. (SIPI), Univ. of Southern California, Los Angeles, CA; Signal & Image Process. Inst. (SIPI), Univ. of Southern California, Los Angeles, CA","IEEE Transactions on Audio, Speech, and Language Processing","18 Dec 2007","2008","16","1","216","228","With the advent of prosody annotation standards such as tones and break indices (ToBI), speech technologists and linguists alike have been interested in automatically detecting prosodic events in speech. This is because the prosodic tier provides an additional layer of information over the short-term segment-level features and lexical representation of an utterance. As the prosody of an utterance is closely tied to its syntactic and semantic content in addition to its lexical content, knowledge of the prosodic events within and across utterances can assist spoken language applications such as automatic speech recognition and translation. On the other hand, corpora annotated with prosodic events are useful for building natural-sounding speech synthesizers. In this paper, we build an automatic detector and classifier for prosodic events in American English, based on their acoustic, lexical, and syntactic correlates. Following previous work in this area, we focus on accent (prominence, or ldquostressrdquo) and prosodic phrase boundary detection at the syllable level. Our experiments achieved a performance rate of 86.75% agreement on the accent detection task, and 91.61% agreement on the phrase boundary detection task on the Boston University Radio News Corpus.","1558-7924","","10.1109/TASL.2007.907570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4358088","Accent;prominence;prosodic phrase boundary;prosody recognition;prosody–syntax interface;spoken language processing;stress;Accent;prominence;prosodic phrase boundary;prosody recognition;prosody-syntax interface;spoken language processing;stress","Event detection;Acoustic signal detection;Natural languages;Speech synthesis;Stress;Intensity modulation;Speech recognition;Automatic speech recognition;Synthesizers;Detectors","acoustic signal processing;natural languages;signal classification;speech processing;speech recognition;speech synthesis","automatic speech prosodic event detection;acoustic evidence;lexical evidence;syntactic evidence;prosody annotation standard;spoken language processing;automatic speech recognition;automatic speech translation;natural-sounding speech synthesizer;automatic prosodic event classifier;American English;prosodic phrase boundary detection","","54","","32","","18 Dec 2007","","","IEEE","IEEE Journals"
"Automatic segmentation of moving objects for video object plane generation","T. Meier; K. N. Ngan","Visual Commun. Res. Group, Western Australia Univ., Nedlands, WA, Australia; NA","IEEE Transactions on Circuits and Systems for Video Technology","6 Aug 2002","1998","8","5","525","538","The new video coding standard MPEG-4 is enabling content-based functionalities. It takes advantage of a prior decomposition of sequences into video object planes (VOPs) so that each VOP represents one moving object. A comprehensive review summarizes some of the most important motion segmentation and VOP generation techniques that have been proposed. Then, a new automatic video sequence segmentation algorithm that extracts moving objects is presented. The core of this algorithm is an object tracker that matches a two-dimensional (2-D) binary model of the object against subsequent frames using the Hausdorff distance. The best match found indicates the translation the object has undergone, and the model is updated every frame to accommodate for rotation and changes in shape. The initial model is derived automatically, and a new model update method based on the concept of moving connected components allows for comparatively large changes in shape. The proposed algorithm is improved by a filtering technique that removes stationary background. Finally, the binary model sequence guides the extraction objects of the VOPs from the sequence. Experimental results demonstrate the performance of our algorithm.","1558-2205","","10.1109/76.718500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=718500","","Shape;MPEG 4 Standard;Video sequences;Video compression;Partitioning algorithms;Video coding;Motion segmentation;Computer vision;Transform coding;Image segmentation","video coding;image sequences;image segmentation;image matching;filtering theory;tracking;motion estimation;feature extraction;code standards;telecommunication standards","object tracker;video object plane generation;video coding standard;MPEG-4;video sequences decomposition;motion segmentation;automatic video sequence segmentation algorithm;2D binary model;Hausdorff distance;translation;rotation;shape changes;model update method;moving connected components;filtering;stationary background;binary model sequence;experimental results;performance;moving objects extraction","","187","13","42","","6 Aug 2002","","","IEEE","IEEE Journals"
"Moment invariant features for automatic identification of critical malaria parasites","A. Ravendran; K. W. T. R. T. de Silva; R. Senanayake","Dept. of Mechanical Engineering, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Dept. of Chemistry, University of Colombo, Sri Lanka; Dept. of El. & Comp. Engineering, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka","2015 IEEE 10th International Conference on Industrial and Information Systems (ICIIS)","4 Feb 2016","2015","","","474","479","Malaria is a globally widespread mosquito-borne disease which is caused by Plasmodium parasite. Plasmodium falciparum is the most ubiquitous among few species of Plasmodium and its Gametocyte stage is the most virulent among all stages and species. Although blood films are stained for better visualisation through the microscope, the color difference between red blood cells and parasites is barely identifiable for a computer as images are represented in the RGB color space. Moreover, there are several parasites spread throughout the blood film in various orientations and sizes. The automatic classification becomes further challenging due to the presence of many artefacts in the blood film. A photomicrograph analysis method to determine the presence of the most critical parasite -Gametocyte stage of Plasmodium falciparum - in Giemsa-stained blood films is presented. Having extracted the parasite from the background of the image after a series of pre-processing operations, it is classified using both K-nearest neighbors (K-NN) and Gaussian naïve Bayes classifiers. As the key element of the research, moment invariant features are utilised to make the input features invariant to translation, rotation and scale (TRS). Based on leave-one-out cross-validation, true positive rates of 77.78% and 88.89% and, true negative rates of 95.24% and 80.95% were achieved for K-NN and Gaussian naïve Bayes classifiers respectively. Since a higher true positive rate is desirable in this application, Gaussian naïve Bayes qualifies as the classifier while moment invariant features provide robust covariates for classifying Plasmodium falciparum from other Plasmodium species.","","978-1-4799-1876-8","10.1109/ICIINFS.2015.7399058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399058","Malaria Identification;Plasmodium falciparum;Hu Moments;Microscope Imaging;Parasite Identification","Asia;Image color analysis;Meteorology;Diseases;Sensitivity","Bayes methods;biology computing;diseases;Gaussian processes;image classification;image colour analysis;learning (artificial intelligence);microorganisms","moment invariant feature;automatic identification;critical malaria parasite;mosquito-borne disease;Plasmodium parasite;Plasmodium falciparum;gametocyte stage;visualisation;color difference;red blood cell;RGB color space;automatic classification;photomicrograph analysis method;Giemsa-stained blood film;k-nearest neighbor;Gaussian naïve Bayes classifier;translation rotation and scale;TRS;leave-one-out cross-validation;Plasmodium species","","2","","36","","4 Feb 2016","","","IEEE","IEEE Conferences"
"Automatic Design of Ensembles of Window Operators for Ocular Image Segmentation","M. E. Benalcazar Palacios; M. Brun; V. L. Ballarin; R. M. Hidalgo","Consejo Nac. de Investig. Cientificas y Tec., Univ. Nac. de Mar del Plata, Mar del Plata, Argentina; Univ. Nac. de Mar del Plata, Mar del Plata, Argentina; Univ. Nac. de Mar del Plata, Mar del Plata, Argentina; Univ. Nac. de Mar del Plata, Mar del Plata, Argentina","IEEE Latin America Transactions","7 Aug 2014","2014","12","5","935","941","W-operators are a nonlinear class of locally defined and translation invariant image operators. A W-operator is completely defined and represented by a characteristic function, or classifier, that maps a set of window observations to a set of labels. In this work, we propose a new approach to design W-operators for grayscale image processing. In the proposed approach, we constrain both the space of characteristic functions using artificial feed-forward neural networks and the space of observations using the 2-D Haar wavelet transform. The goal of these constraints is to reduce the cost of design and improve the performance of W-operators for practical applications. Based on this approach, a family or set of W-operators is designed and then combined into a single operator using an ensemble method. We evaluated the performance of this approach in the segmentation of blood vessels in ocular images of the DRIVE database. The results show the suitability of this approach: It outperforms W-operators based on logistic regression without any constraint in the space of observations; aperture filters designed using support vector machines and pyramidal multiresolution, and a fast segmentation using the local thresholding method of Otsu.","1548-0992","","10.1109/TLA.2014.6872909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6872909","Classification;Ensemble of Classifiers;Image Processing;Mathematical Morphology;Neural Networks;Ocular Images;Segmentation;Wavelet Transform","Image segmentation;Apertures;Image color analysis;Vectors;Optimized production technology;Irrigation;Wavelet transforms","blood vessels;feedforward neural nets;Haar transforms;image classification;image resolution;image segmentation;mathematical operators;medical image processing;regression analysis;support vector machines;wavelet transforms","automatic window operator ensemble design;ocular image segmentation;translation invariant image operator;W-operator;grayscale image processing;characteristic functions;artificial feedforward neural network;2D Haar wavelet transform;cost reduction;blood vessel segmentation;DRIVE database;logistic regression;aperture filter design;support vector machine;pyramidal multiresolution;Otsu;image thresholding method;ensemble of classifiers","","","","","","7 Aug 2014","","","IEEE","IEEE Journals"
"Automatic Temporally Coherent Video Colorization","H. Thasarathan; K. Nazeri; M. Ebrahimi",University of Ontario Institute of Technology; University of Ontario Institute of Technology; University of Ontario Institute of Technology,"2019 16th Conference on Computer and Robot Vision (CRV)","1 Aug 2019","2019","","","189","194","Greyscale image colorization for applications in image restoration has seen significant improvements in recent years. Many of these techniques that use learning-based methods struggle to effectively colorize sparse inputs. With the consistent growth of the anime industry, the ability to colorize sparse input such as line art can reduce significant cost and redundant work for production studios by eliminating the in-between frame colorization process. Simply using existing methods yields inconsistent colors between related frames resulting in a flicker effect in the final video. In order to successfully automate key areas of large-scale anime production, the colorization of line arts must be temporally consistent between frames. This paper proposes a method to colorize line art frames in an adversarial setting, to create temporally coherent video of large anime by improving existing image to image translation methods. We show that by adding an extra condition to the generator and discriminator, we can effectively create temporally consistent video sequences from anime line arts.","","978-1-7281-1838-3","10.1109/CRV.2019.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8781608","Computer Vision;Generative Adversarial Networks","Art;Generators;Image color analysis;Training;Task analysis;Learning systems","computer animation;image colour analysis;image restoration;image segmentation;image sequences;learning (artificial intelligence);video signal processing","flicker effect;large-scale anime production;line art frames;image translation methods;temporally consistent video sequences;anime line arts;greyscale image colorization;image restoration;anime industry;frame colorization process;video colorization;learning-based methods","","4","","29","","1 Aug 2019","","","IEEE","IEEE Conferences"
"Automatic detection and recognition of traffic road signs for intelligent autonomous unmanned vehicles for urban surveillance and rescue","I. Sebanja; D. B. Megherbi","CMINDS resesrch center, Department of, Electrical and Computer Engineering, University of, Massachusetts, Lowell, USA; CMINDS research center, Department, of Electrical and Computer Engineering, University of Massachusetts, Lowell, USA","2010 IEEE International Conference on Technologies for Homeland Security (HST)","3 Dec 2010","2010","","","132","138","In this paper, we propose a system that automatically detects and recognizes road signs found in the United States, in real time or close to real-time. The proposed system has application to intelligent autonomous unmanned vehicles for urban surveillance and rescue. It is a multi-layered hierarchical scheme composed of 3 parts: road sign color segmentation, shape recognition, and classification. The system is robust and is invariant to image translation, rotation and scaling. It can deal with situations where there is partial occlusion, blurring of the image, and low visibility due to either weather or a change in lighting conditions. The road sign shape detection and sign classification/recognition are both based on the Principle Component Analysis. We show that the proposed system has correct classification rate of 99.2%. Experimental results show that with the current system, using existing standard hardware/software, it takes on average 2.5 seconds to detect, to segment, and to classify/recognize road signs in a road image scene. This is considered relatively fast. This time can easily be decreased in the future with dedicated specialized hardware and optimized software, taking advantage of the latest embedded hardware technology. Currently, in this paper the focus is on red and yellow road signs found in the United States but the proposed techniques can be generalized to be used for any other colored road signs found both in the United States of America and other countries.","","978-1-4244-6048-9","10.1109/THS.2010.5655078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5655078","Digital Image Processing and Computer Vision;Object Segmentation;Object Shape Representation and Recognition;Weather modeling;Autonomous unmanned vehicles for urban surveillance and rescue","Roads;Shape;Image color analysis;Image segmentation;Principal component analysis;Pixel;Vehicles","image colour analysis;image recognition;image segmentation;object detection;principal component analysis;road traffic;traffic engineering computing","traffic road sign detection;traffic road sign recognition;intelligent autonomous unmanned vehicles;urban surveillance;urban rescue;road sign color segmentation;shape recognition;shape classification;image translation;image rotation;image scaling;principle component analysis;shape detection","","10","","22","","3 Dec 2010","","","IEEE","IEEE Conferences"
"Automatic feature point extraction and tracking in image sequences for unknown camera motion","Q. Zheng; R. Chellappa","Center for Autom. Res., Maryland Univ., College Park, MD, USA; Center for Autom. Res., Maryland Univ., College Park, MD, USA","1993 (4th) International Conference on Computer Vision","6 Aug 2002","1993","","","335","339","An automatic ego motion compensation based feature detection and correspondence algorithm is presented. For image sequences taken from a moving camera, feature displacement over consecutive frames can be approximately decomposed into two components: the displacement due to camera motion, which can be compensated for by image rotation, scaling, and translation; and the displacement due to object motion and/or perspective projection. The authors introduce a two-step approach. First, the motion of the camera is compensated for by using a computational vision based image registration algorithm. Then consecutive frames are transformed to the same coordinate system and the feature correspondence problem is solved as though for a stationary camera. Feature points are detected using a Gabor wavelet decomposition and a local interaction based algorithm. Methods for subpixel accuracy feature matching and tracking are introduced. Experimental results on a real image sequence are presented.<>","","0-8186-3870-2","10.1109/ICCV.1993.378195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=378195","","Feature extraction;Tracking;Image sequences;Cameras;Computer vision;Automation;Motion compensation;Motion detection;Image registration;Motion estimation","image sequences;feature extraction;motion estimation;object recognition;computer vision;image registration;wavelet transforms","automatic feature point extraction;tracking;image sequences;camera motion;automatic ego motion compensation;correspondence algorithm;image rotation;scaling;translation;object motion;perspective projection;computational vision based image registration algorithm;coordinate system;feature correspondence problem;Gabor wavelet decomposition;local interaction based algorithm;subpixel accuracy feature matching","","12","10","18","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Detection of irregular QRS complexes using Hermite transform and support vector machine","Z. Vulaj; M. Brajović; A. Draganić; I. Orović","University of Montenegro, Faculty of Electrical Engineering, Džordža Vašingtona bb, 81000 Podgorica, Montenegro; University of Montenegro, Faculty of Electrical Engineering, Džordža Vašingtona bb, 81000 Podgorica, Montenegro; University of Montenegro, Faculty of Electrical Engineering, Džordža Vašingtona bb, 81000 Podgorica, Montenegro; University of Montenegro, Faculty of Electrical Engineering, Džordža Vašingtona bb, 81000 Podgorica, Montenegro","2017 International Symposium ELMAR","30 Nov 2017","2017","","","59","62","Computer based recognition and detection of abnormalities in ECG (electrocardiogram) signals is proposed. For this purpose, the Support Vector Machines (SVM) are combined with the advantages of Hermite transform representation. SVM represent a special type of classification techniques commonly used in medical applications. Automatic classification of ECG could make the work of cardiologic departments faster and more efficient. It would also reduce the number of false diagnosis and, as a result, save lives. The working principle of the SVM is based on translating the data into a high dimensional feature space and separating it using a linear classificator. In order to provide an optimal representation for SVM application, the Hermite transform domain is used. This domain is proved to be suitable because of the similarity of the QRS complex with Hermite basis functions. The maximal signal information is obtained using a small set of features that are used for detection of irregular QRS complexes. The aim of the paper is to show that these features can be employed for automatic ECG signal analysis.","","978-953-184-225-9","10.23919/ELMAR.2017.8124435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8124435","ECG;QRS Complex;Detection;Classification;Feature;SVM;Hermite Transform","Electrocardiography;Support vector machines;Transforms;Feature extraction;Sleep apnea;Signal analysis;Heart","cardiology;electrocardiography;medical signal detection;medical signal processing;support vector machines;transforms","Hermite basis functions;maximal signal information;irregular QRS complexes;automatic ECG signal analysis;vector machine;computer based recognition;ECG signals;electrocardiogram;Hermite transform representation;medical applications;cardiologic departments;high dimensional feature space;linear classificator;optimal representation;SVM application;support vector machines;QRS complex;Hermite transform domain","","2","","22","","30 Nov 2017","","","IEEE","IEEE Conferences"
"A development system for creating real-time machine vision hardware using field programmable gate arrays","T. H. Drayer; J. G. Tront; R. W. Conners; P. A. Araman","Bradley Dept. of Electr. Eng., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA; NA; NA; NA","Proceedings of the 32nd Annual Hawaii International Conference on Systems Sciences. 1999. HICSS-32. Abstracts and CD-ROM of Full Papers","6 Aug 2002","1999","Track3","","5 pp.","","In this paper, we introduce a new development system for creating real-time image processing hardware using custom computing machines with multiple Field Programmable Gate Array (FPGA) chips. Three distinct processes are accomplished within the development system: design entry, verification, and translation. A library of modules that implement common low-level machine vision functions is used to create complex designs based on a dataflow graph representation. The library's low-level image processing modules contain both gate-level and chip-level hardware components, of which the gate-level components are compiled into the functionality of available FPGA chips. Standard interfaces are established for input/output of the modules, allowing for the creation of sophisticated software support tools. Experimental results verify the utility of this development system for easily creating real-time machine vision hardware using multiple FPGA-based custom computing machines.","","0-7695-0001-3","10.1109/HICSS.1999.772885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772885","","Real time systems;Machine vision;Hardware;Image processing;Computer architecture;Field programmable gate arrays;Process design;Programmable logic arrays;Logic arrays;Printed circuits","computer vision;development systems;field programmable gate arrays;real-time systems","development system;real-time machine vision;field programmable gate arrays;image processing hardware;machine vision hardware;library of modules","","3","","7","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Visual sorting of recyclable goods using a support vector machine","M. Nawrocky; D. C. Schuurman; J. Fortuna","Redeemer University College Ancaster, Ontario, Canada; Redeemer University College Ancaster, Ontario, Canada; McMaster University Hamilton, Ontario, Canada","CCECE 2010","16 Sep 2010","2010","","","1","4","Mounting environmental concerns and changing attitudes have led to recycling programs to divert waste from entering landfill sites. This trend has led municipalities to explore improved methods and tools such as machine vision for sorting and managing the growing volume of recyclable materials. This paper describes an approach to visual sorting using image intensity data and a support vector machine applied to the unique problem of sorting polycoat containers from plastic bottles. The approach is rotation, translation and scale invariant since it uses features derived from image histograms. We also demonstrate that the approach is robust to the size, shape, varied labeling and deformation of the recycled material. An experiment is performed to verify the approach using separate test and training data. Despite the use of a modest number of training images, the system achieves a classification accuracy of over 96% using images obtained from a single grey-scale camera.","0840-7789","978-1-4244-5376-4","10.1109/CCECE.2010.5575231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575231","","Training;Containers;Histograms;Support vector machines;Plastic products;Materials;Sorting","computer vision;image classification;image colour analysis;recycling","visual sorting;recyclable goods;support vector machine;environmental concerns;landfill sites;machine vision;recyclable material sorting;recyclable material management;image intensity data;polycoat containers;plastic bottles;image histograms;image classification;grey-scale camera","","2","","9","","16 Sep 2010","","","IEEE","IEEE Conferences"
"Performance evaluation of linear brain machine interface decoders in neural state space","I. Badreldin; K. Oweiss","Department of Electrical and Computer Engineering, Michigan State University, East Lasning, MI 48824, USA; Neuroscience Program, Michigan State University, East Lasning, MI 48824, USA","2012 Cairo International Biomedical Engineering Conference (CIBEC)","7 Mar 2013","2012","","","10","13","Brain-Machine Interfaces (BMIs) have the potential to restore lost sensorimotor functions in people with severe motor disabilities. Several BMI decoding strategies have been suggested to translate activity of motor neurons into control signals that ac-tuate artificial devices. Among these, the class of linear decoders, particularly Wiener filters, is known to perform well for simple tasks, but degrades considerably as a function of increasing task complexity. In this work, we study the mathematical properties of the solution subspace of Wiener decoders in an effort to derive a desired neural state trajectory that is optimal for a given decoder and a desired biomimetic kinematic solution. We show that the error between the desired neural trajectory and the actual one measured during the performance of a 2D reach task provides reliable estimation and prediction of the performance in the task space. We demonstrate a significant correlation between the error measure in the neural state space and the error measure in the task space, which allows potential future use of this error measure as a way to estimate the true motor intent and the extent of learning the decoder by BMI subjects, and possibly as a feedback signal to improve their online decoding performance.","2156-6100","978-1-4673-2801-2","10.1109/CIBEC.2012.6473323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6473323","","Decoding;Kinematics;Measurement uncertainty;Vectors;Equations;Trajectory;Null space","brain-computer interfaces;medical signal processing;neurophysiology;Wiener filters","linear brain machine interface decoder;neural state space;sensorimotor function;BMI decoding strategy;Wiener filter;task complexity;biomimetic kinematic solution","","","","8","","7 Mar 2013","","","IEEE","IEEE Conferences"
"A Utility Human Machine Interface Using Low Cost EEG Cap and Eye Tracker","C. Liu; D. Yu; J. Zhang; S. Xie","Institute of Intelligent Media Technology, Communication University of Zhejiang,Hangzhou,China; School of Media Engineering, Communication University of Zhejiang,Hangzhou,China; Institute of Intelligent Media Technology, Communication University of Zhejiang,Hangzhou,China; School of Electronics and Information, Northwestern Polytechnical University,Xi'an,China","2021 9th International Winter Conference on Brain-Computer Interface (BCI)","5 Apr 2021","2021","","","1","5","Electroencephalogram (EEG) based Brain Computer Interface (BCI) has a huge market with big potential and wide prospect, however, the acquisition equipment is too expensive to be popular with ordinary users, which makes the majority of applications are still in the laboratory. Inspired by the development of hybrid Human-Computer Interaction (HCI), a utility HCI using low-cost EEG cap and eye tracker are investigated in this paper. The validation experiment indicates that the proposed system is able to detect and classify multiple patterns of EEG signals and translate them into control commands to interact with the environment. Furthermore, an eye tracker enables subjects to freely observe the surrounding environment and select a target object under the naked eye VR technique. Comparing to the HCI only based on eye tracker, EEG signals are used to realize the motor function in this paper to reduce the fatigue caused by long-time fixation in traditional eye tracker application. Furthermore, compared to the BCI system at the same price, the lack of electrodes can be compensated by an eye tracker. In conclusion, the hybrid HCI proposed in this paper achieves superior performance through low-cost equipment, which may promote the development of the interaction between the human and environment based on the physiological signals.","2572-7672","978-1-7281-8485-2","10.1109/BCI51272.2021.9385304","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385304","Electroencephalogram (EEG);low cost;hybrid human-computer interaction (HCI);EEG cap;eye tracker","Human computer interaction;Target tracking;Fatigue;Electroencephalography;Brain-computer interfaces;Physiology;Man-machine systems","brain-computer interfaces;electroencephalography;human computer interaction;man-machine systems;medical signal processing","human machine interface;brain computer interface;low-cost EEG cap;traditional eye tracker application;low-cost equipment;hybrid human-computer interaction;HCI;electroencephalogram;EEG signals;physiological signals","","","","19","","5 Apr 2021","","","IEEE","IEEE Conferences"
"Hardware application of human-machine interface in smart air conditioners using hand tracking","E. Acay; N. Kahraman; M. Taskiran; T. Kiyan; H. U. Yogun","Yildiz Technical University, Electronics and Communication Eng. Dept., Davutpasa, Istanbul, Turkey; Yildiz Technical University, Electronics and Communication Eng. Dept., Davutpasa, Istanbul, Turkey; Yildiz Technical University, Electronics and Communication Eng. Dept., Davutpasa, Istanbul, Turkey; Yildiz Technical University, Electronics and Communication Eng. Dept., Davutpasa, Istanbul, Turkey; ARÇELİK-LG Klima San.ve Tic. A.S. GOSB 41480 Kocaeli Turkey","2016 International Symposium ELMAR","3 Nov 2016","2016","","","269","272","The aim of this paper is to present a real time application for air condition systems using hand recognition and tracking. The hardware requirements of the system, limited by only a simple USB webcam and a Beagle Board, which is a compact and low cost hardware. Our approach uses a simple motion detection trying to eliminate unnecessary regions of the image and a recognition algorithm based on Haar classifiers. An intelligent method and a practical application for detecting four hand movement instructions have been proposed for air condition systems in smart homes using Beagle Board. The system translates the detected gestures into different functional inputs and interfaces to air conditioner. The proposed method is also intelligent because it has an optimized multi layered neural network which is also optimized by a swarm-intelligence-based global optimization algorithm, Artificial Bee Colony Algorithm.","1334-2630","978-953-184-221-1","10.1109/ELMAR.2016.7731803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7731803","Hand Tracking;Haar Classifiers;Smart Air Conditioners","Feature extraction;Real-time systems;Tracking;Training;Classification algorithms;Cameras;Human computer interaction","air conditioning;gesture recognition;Haar transforms;image classification;man-machine systems;motion compensation;neural nets;optimisation;swarm intelligence;target tracking","human machine interface;smart air conditioners;hand tracking;air condition systems;hand recognition;USB webcam;Beagle Board;motion detection;image recognition algorithm;Haar classifiers;intelligent method;smart homes;multilayered neural network;swarm intelligence based global optimization algorithm;artificial bee colony algorithm","","","","13","","3 Nov 2016","","","IEEE","IEEE Conferences"
"Machine Learning Approach to Classify Birds on the Basis of Their Sound","Y. Jadhav; V. Patil; D. Parasar","Amity University,Computer Science and Engineering,Mumbai,India; Mumbai University,Computer Engineering,Mumbai,India; Amity University,Computer Science and Engineering,Mumbai,India","2020 International Conference on Inventive Computation Technologies (ICICT)","9 Jun 2020","2020","","","69","73","Bird Classifier is a system which will use machine learning approach and allows users to record bird sounds and identify them. The System will help bird species to be identified by taking input only the sound of that bird.The framework also offers tools for translating such annotations into datasets that can be used to train a computer to identify a species ' presence or absence. Dataset recorded are preprocessed to remove unwanted noise and divide useful sounds in frames so that it can act as an input to classifier. The system was tested through different algorithms and the algorithm that gave best results was chosen for implementation.System uses audio features like MFCC,Mel- Spectra etc. This system will use different algorithms such as KNN, Random Forest, Multi layer perceptron, Bayes in classification of birds species.","","978-1-7281-4685-0","10.1109/ICICT48043.2020.9112506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9112506","Classifier;Birds;Entropy;Sampling;Visualization","","acoustic signal processing;audio signal processing;Bayes methods;biology computing;feature extraction;multilayer perceptrons;nearest neighbour methods;random forests;signal classification;speech recognition","unwanted noise;bird classifier;machine learning approach;bird sounds;bird species;KNN;random forest;multilayer perceptron;Bayes","","","","17","","9 Jun 2020","","","IEEE","IEEE Conferences"
"Gesture control and the new and intelligent man-machine interface","D. Ionescu","School of Information Technology and Engineering, University of Ottawa, Canada","2011 6th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI)","9 Jun 2011","2011","","","291","291","Summary form only given. Human-computer interfaces (HCI) have evolved from mouse-keyboard based interaction using text and mouse events to multi-touch screens and other exotic approaches such as using special gloves or other devices to translate human actions into application controls. One of the trends which are “en vogue” in our days is the control of computer applications and/or computer controlled devices using human gestures and/or body movements. There were and still are a series of attempts to produce computer control scripts via gesture based interfaces, and research literature is abundant in papers on this subject. However, little or few are of value, as the computer based control via gestures has to be robust and in real-time. Any lag in the result can lead to users abandoning it. More recently, 3D IR cameras provide a promised path to produce depth images. The depth information extracted by the 3D IR camera is mapped to different grey levels in a black and white image. This image is finally interpreted by image processing suites. 3D IR cameras use near-infrared illumination, allowing objects that are closer to the camera to appear brighter than the rest. This means that computer vision software modules can much more accurately distinguish hands and fingers from the rest of a person's body, even in complete darkness. In this talk, we will discuss various technologies to be used to build a 3D IR Camera, review which will be finalized with a discussion of the 3D IR Camera designed and implemented by our group. Based on data provided by the 3D IR Camera a series of related image processing applications will be discussed. A demonstration, of a 3D IR camera system will be presented and applications related to gaming, learning, and immersion in a 3D virtual reality space will be given at the end.","","978-1-4244-9109-4","10.1109/SACI.2011.5873016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5873016","","Cameras;Three dimensional displays;Image processing;Computers;Humans;Lighting;Mice","cameras;computer vision;feature extraction;gesture recognition;human computer interaction;infrared detectors;man-machine systems;virtual reality","intelligent man-machine interface;human-computer interface;HCI;mouse-keyboard based interaction;multitouch screen;computer controlled device;human gesture control;body movement control;gesture based interface;3D IR cameras;depth information extraction;near-infrared illumination;computer vision software module;image processing application;3D virtual reality","","2","1","","","9 Jun 2011","","","IEEE","IEEE Conferences"
"Single trial motor imagery classification for a four state brain machine interface","C. R. Hema; M. P. Paulraj; S. Yaacob; A. H. Adom; R. Nagarajan","School of Mechatronic Engineering, Universiti Malaysia Perlis, Jejawi, Malaysia; School of Mechatronic Engineering, Universiti Malaysia Perlis, Jejawi, Malaysia; School of Mechatronic Engineering, Universiti Malaysia Perlis, Jejawi, Malaysia; School of Mechatronic Engineering, Universiti Malaysia Perlis, Jejawi, Malaysia; School of Mechatronic Engineering, Universiti Malaysia Perlis, Jejawi, Malaysia","2009 5th International Colloquium on Signal Processing & Its Applications","5 Jun 2009","2009","","","39","41","Motor imagery is the mental simulation of a motor act which can be used to design brain machine interfaces [BMI]. A BMI is a digital communication system, which connects the human brain directly to an external device bypassing the peripheral nervous system and muscular system. Thus a BMI opens up possibilities for a new communication channel for people with neuromuscular disorders. The ability of an individual to control his EEG through imaginary motor tasks enables him to control devices. This paper presents a novel method for single trial motor imagery classification for a four state BMI to control a powered wheelchair. Recurrent Neural classifiers are used for classification of EEG signals during motor imagery for forward, stop, left and right hand movements. EEG is recorded using noninvasive scalp electrodes placed over the motor cortex. The performance of the proposed algorithm has an average classification efficiency of 96.15%. The proposed method can be used to translate the motor imagery signals into control signal using a four state BMI to control the directional movement of a powered wheelchair.","","978-1-4244-4151-8","10.1109/CSPA.2009.5069184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5069184","","Communication system control;Electroencephalography;Wheelchairs;Brain modeling;Digital communication;Humans;Nervous system;Communication channels;Neuromuscular;Scalp","biomedical electrodes;electroencephalography;handicapped aids;medical control systems;medical signal processing;neurophysiology;recurrent neural nets;signal classification","single trial motor imagery classification;four state brain machine interface;mental simulation;peripheral nervous system;muscular system;neuromuscular disorders;EEG;recurrent neural classifiers;noninvasive scalp electrodes;motor cortex","","7","","10","","5 Jun 2009","","","IEEE","IEEE Conferences"
"The application of machine vision in inspecting position-control accuracy of motor control systems","Wei Zhenzhong; Zhang Guangjun; Li Xin","Sch. of Autom. Sci. & Electr. Eng., Beijing Univ. of Aeronaut. & Astronaut., China; NA; NA","ICEMS'2001. Proceedings of the Fifth International Conference on Electrical Machines and Systems (IEEE Cat. No.01EX501)","6 Aug 2002","2001","2","","787","790 vol.2","In this paper, a new structured-light machine vision technique based on a radial basis function (RBF) neural network is proposed and an inspection system is established. General structured-light machine vision techniques are usually based on accurate mathematical models and have some unavoidable and inexpressible errors. The proposed new technique is based on the training and learning of high-accuracy samples and overcomes the disadvantages of the general technique and considerably improves the accuracy of machine vision inspection systems. An experiment applying this new technique to inspect the position-control accuracy of a step-motor controlled stage with one linear translation axis shows that the RBF artificial neural network (ANN) is quite suited to structured-light machine vision inspection systems and that structured-light machine vision inspection techniques are really a novel and effective means for the inspection of the position-control accuracy of motor control systems.","","7-5062-5115-9","10.1109/ICEMS.2001.971794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=971794","","Machine vision;Motor drives;Inspection;Artificial neural networks;Optical sensors;Neural networks;Control systems;Mathematical model;Nonlinear optics;Optical devices","electric motors;radial basis function networks;machine control;position control;computer vision;inspection;learning (artificial intelligence)","motor control systems;position-control accuracy inspection;machine vision application;radial basis function neural network;general structured-light machine vision technique;mathematical model;training;learning","","4","","13","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Towards a whole body brain-machine interface system for decoding expressive movement intent Challenges and Opportunities","J. L. Contreras-Vidal; J. Cruz-Garcia; A. Kopteva","Department of Electrical and Computer Engineering, University of Houston, USA; Department of Electrical and Computer Engineering, University of Houston, USA; Department of Electrical and Computer Engineering, University of Houston, USA","2017 5th International Winter Conference on Brain-Computer Interface (BCI)","20 Feb 2017","2017","","","1","4","The restoration and rehabilitation of human bipedal locomotion represent major goals for brain machine interfaces (BMIs), i.e., devices that translate neural activity into motor commands to control wearable robots to enable locomotive and non-locomotive tasks by individuals with gait disabilities. Prior BMI efforts based on scalp electroencephalography (EEG) have revealed that fluctuations in the amplitude of slow cortical potentials in the delta band contain information that can be used to infer motor intent, and more specifically, the kinematics of walking and non-locomotive tasks such as sitting and standing. However, little is known about the extent to which EEG can be used to discern the expressive qualities that influence such functional movements. Here, we discuss how novel experimental approaches integrated with machine learning techniques can deployed to decode expressive qualities of movement. Applications to artistic brain-computer interfaces (BCIs), movement aesthetics, and gait neuroprostheses endowed with expressive qualities are discussed.","","978-1-5090-5096-3","10.1109/IWW-BCI.2017.7858142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858142","expressive movements;decoding;EEG;actions","Legged locomotion","electroencephalography;legged locomotion;medical signal processing","whole body brain-machine interface system;human bipedal locomotion;BMI;wearable robots;scalp electroencephalography;EEG;artistic brain-computer interfaces;machine learning techniques","","6","","26","","20 Feb 2017","","","IEEE","IEEE Conferences"
"Feasibility of Automatic Error Detect-and-Undo System in Human Intracortical Brain–Computer Interfaces","N. Even-Chen; S. D. Stavisky; C. Pandarinath; P. Nuyujukian; C. H. Blabe; L. R. Hochberg; J. M. Henderson; K. V. Shenoy","Department of Electrical EngineeringStanford University; Department of Electrical Engineering and NeurosurgeryStanford University; Department of Electrical Engineering and NeurosurgeryStanford University; Department of BioengineeringStanford Neuroscience Institute and Bio-X ProgramsNeurosurgery, and Electrical engineeringStanford University; Department of NeurosurgeryStanford University; Center for Neurorestoration and NeurotechnologyRehabilitation R&D Service, VA Medical Center, School of Engineering and the Institute for Brain Science, Brown University; Department of Neurosurgery and Stanford Neuroscience InstituteStanford University; Department of Electrical Engineering, Bioengineering and Neurobiology, Stanford Neuroscience Institute and the Neurosciences and Bio-X Programs, Stanford","IEEE Transactions on Biomedical Engineering","17 Jul 2018","2018","65","8","1771","1784","Objective: Brain-computer interfaces (BCIs) aim to help people with impaired movement ability by directly translating their movement intentions into command signals for assistive technologies. Despite large performance improvements over the last two decades, BCI systems still make errors that need to be corrected manually by the user. This decreases system performance and is also frustrating for the user. The deleterious effects of errors could be mitigated if the system automatically detected when the user perceives that an error was made and automatically intervened with a corrective action; thus, sparing users from having to make the correction themselves. Our previous preclinical work with monkeys demonstrated that task-outcome correlates exist in motor cortical spiking activity and can be utilized to improve BCI performance. Here, we asked if these signals also exist in the human hand area of motor cortex, and whether they can be decoded with high accuracy. Methods: We analyzed posthoc the intracortical neural activity of two BrainGate2 clinical trial participants who were neurally controlling a computer cursor to perform a grid target selection task and a keyboard-typing task. Results: Our key findings are that: 1) there exists a putative outcome error signal reflected in both the action potentials and local field potentials of the human hand area of motor cortex, and 2) target selection outcomes can be classified with high accuracy (70-85%) of errors successfully detected with minimal (0-3%) misclassifications of success trials, based on neural activity alone. Significance: These offline results suggest that it will be possible to improve the performance of clinical intracortical BCIs by incorporating a real-time error detect-and-undo system alongside the decoding of movement intention.","1558-2531","","10.1109/TBME.2017.2776204","Stanford Office of Postdoctoral Affairs; ALS Association Milton Safenowtiz Postdoctoral Fellowship; Neuro-Innovation and Translational Neuroscience; Samuel and Betsy Reeves; Rehabilitation Research and Development Service; The Executive Committee on Research of Massachusetts General Hospital; MGH-Deane Institute for Integrated Research on Atrial Fibrillation and Stroke; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8116673","Brain-computer interface;clinical trial;error detection;motor cortex;task outcome","Decoding;Electrical engineering;Neurosurgery;Clinical trials;Brain-computer interfaces","brain;brain-computer interfaces;electroencephalography;medical signal processing;neurophysiology","movement intention;BCI systems;system performance;deleterious effects;task-outcome correlates;motor cortical spiking activity;BCI performance;human hand area;motor cortex;intracortical neural activity;BrainGate2 clinical trial participants;computer cursor;grid target selection task;keyboard-typing task;putative outcome error signal;action potentials;local field potentials;clinical intracortical BCIs;real-time error;human intracortical brain-computer interfaces;impaired movement ability;automatic error detect-and-undo system;real-time error detect-and-undo system;clinical intracortical BCI","Amyotrophic Lateral Sclerosis;Brain-Computer Interfaces;Electrodes, Implanted;Electroencephalography;Female;Hand;Humans;Male;Middle Aged;Motor Cortex;Self-Help Devices;Signal Processing, Computer-Assisted;Spinal Cord Injuries;Task Performance and Analysis","1","","78","","21 Nov 2017","","","IEEE","IEEE Journals"
"Slippage Detection Generalizing to Grasping of Unknown Objects Using Machine Learning With Novel Features","I. Agriomallos; S. Doltsinis; I. Mitsioni; Z. Doulgeri","Center of Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Center of Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Center of Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece","IEEE Robotics and Automation Letters","2 Feb 2018","2018","3","2","942","948","Real-time grasp stability is based on successful slippage detection. In this work, we consider slippage detection as a binary problem (slip, stable) and we propose a novel set of temporal and frequential features, extracted from force norm profiles and collected during reliable ground truth labeling processes, finally employed within the machine learning classification techniques. Classification performance of the proposed scheme, with respect to its success and generalization ability, is assessed systematically utilizing different performance metrics that clarify class predictions as opposed to most of the reported works. We show that our proposed feature extraction method improves classification performance over the commonly used feature sets, even when trained with one surface, and generalizes successfully to unseen ones. The trained classifier is tested on a completely new task and object, for real-time slippage detection, showing high detection accuracy. Finally, the classifier is tested in a different experimental layout with a different force sensor. Experiments are conducted on unseen surfaces for a variety of sampling frequencies, for both translational and rotational slippage, with the proposed approach showing fast and accurate detection in all cases.","2377-3766","","10.1109/LRA.2018.2793346","EU Horizon 2020; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8258901","Slippage detection;perception for grasping and manipulation;learning and adaptive systems","Feature extraction;Grasping;Force;Friction;Sensors;Time-frequency analysis","feature extraction;force sensors;image classification;learning (artificial intelligence);manipulators;object detection;signal sampling;stability","classification performance;class predictions;feature extraction method;trained classifier;real-time slippage detection;translational slippage;real-time grasp stability;binary problem;force norm profiles;performance metrics;force sensor;unknown objects grasping;ground truth labeling process;slippage detection generalization;novel set features;machine learning classification technique;sampling frequency;adaptive system;manipulation","","5","","22","","15 Jan 2018","","","IEEE","IEEE Journals"
"Towards Machine-Learning Assisted Asset Generation for Games: A Study on Pixel Art Sprite Sheets","Y. Rebouças Serpa; M. A. Formico Rodrigues",Universidade de Fortaleza; Universidade de Fortaleza,"2019 18th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)","9 Dec 2019","2019","","","182","191","Game development is simultaneously a technical and an artistic challenge. The past two decades have brought many improvements to general-purpose game engines, reducing the new games development effort considerably. However, the amount of artistic work per title has continuously grown ever since, as a result of increased audience's expectations. The cost of asset-making is further increased based on the aesthetics chosen by the design team and the availability of professionals capable of understanding the nuances of the specific visual language chosen. In this paper, we dig into the topic of deep-learning assets generation to reduce the costs of the asset making pipeline, a major concern for game development teams. More specifically, we tackle the challenge of generating pixel art sprites from line art sketches using state-of-the-art image translation techniques. We set this work within the pipeline of Trajes Fatais: Suits of Fate, a 2D pixel-art fighting game inspired by the late nineties classics of the fighting genre. The results show that our deep-learning assets generation technique is able to generate sprites that look similar to those created by the artists' team. Moreover, by means of qualitative and quantitative analyses, as well as character designers evaluation, we demonstrate the similarity of the generated results to the ground truth.","2159-6662","978-1-7281-4637-9","10.1109/SBGames.2019.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924853","Deep Learning, Generative Adversarial Networks, Asset Generation;Procedural Content Generation;Qualitative and Quantitative Analyses, Character Designers Evaluation","Sprites (computer);Games;Image color analysis;Art;Pipelines;Generators;Gallium nitride","art;computer games;image classification;learning (artificial intelligence);visual languages","game development teams;pixel art sprites;line art sketches;image translation techniques;2D pixel-art fighting game;deep-learning assets generation technique;machine-learning assisted asset generation;pixel art sprite sheets;design team;visual language;game engines;aesthetics","","","","28","","9 Dec 2019","","","IEEE","IEEE Conferences"
"Color is not a metric space implications for pattern recognition, machine learning, and computer vision","T. Kinsman; M. Fairchild; J. Pelz","Multidisciplinary Vision Research Labs, Rochester Institute of Technology, Rochester, NY 14623; Associate Dean, College of Science, Rochester Institute of Technology, Rochester, NY 14623; Multidisciplinary Vision Research Labs, Rochester Institute of Technology, Rochester, NY 14623","2012 Western New York Image Processing Workshop","21 Feb 2013","2012","","","37","40","Using a metric feature space for pattern recognition, data mining, and machine learning greatly simplifies the mathematics because distances are preserved under rotation and translation in feature space. A metric space also provides a “ruler”, or absolute measure of how different two feature vectors are. In the computer vision community color can easily be miss-treated as a metric distance. This paper serves as an introduction to why using a non-metric space is a challenge, and provides details of why color is not a valid Euclidean distance metric.","","978-1-4673-5600-8","10.1109/WNYIPW.2012.6466642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6466642","metric dimension;pattern recognition;feature space;distance learning;color space;CIELAB","Image color analysis;Extraterrestrial measurements;Color;Vectors;Computer vision;Standards","computer vision;feature extraction;image colour analysis;image recognition;learning (artificial intelligence)","metric feature space;pattern recognition;data mining;machine learning;rotation feature;translation feature;ruler;absolute measure;feature vectors;computer vision community;nonmetric space;Euclidean distance metric","","5","","20","","21 Feb 2013","","","IEEE","IEEE Conferences"
"Functional interface between brain and central pattern generator for application in human-machine system","D. Zhang; L. Yao; Y. Wang; X. Zhu","State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, 200240, China; State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, 200240, China; State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, 200240, China; State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, 200240, China","2011 IEEE International Conference on Robotics and Biomimetics","12 Apr 2012","2011","","","1873","1877","In this paper, a functional interface between brain and central pattern generator (CPG) is designed in an engineering perspective, which may serve in a human-machine system. Steady-state visual evoked potential (SSVEP) based brain-computer interface (BCI) is used to recognize five types of intention related to human walking. After feature extraction, classification and command translation on electroencephalography (EEG) signals, the human intention can control CPG to generate desired motor patterns for walking. Four subjects take part in BCI experiment, the successful classification accuracy are above 80%. Also the CPG model can accomplish the desired changes using online EEG data.","","978-1-4577-2138-0","10.1109/ROBIO.2011.6181563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181563","","Electroencephalography;Training;Legged locomotion;Feature extraction;Humans;Visualization","brain-computer interfaces;electroencephalography;feature extraction;medical signal processing","functional interface;brain and central pattern generator;human-machine system;steady-state visual evoked potential;brain-computer interface;human walking;feature extraction;classification and command translation;electroencephalography signals;EEG signals;BCI","","2","","8","","12 Apr 2012","","","IEEE","IEEE Conferences"
"Estimating Neural Modulation via Adaptive Point Process Method in Brain-machine Interface*","S. Chen; X. Zhang; X. Shen; Y. Huang; Y. Wang","the Hong Kong University of Science and Technology,Program of Bioengineering; the Hong Kong University of Science and Technology,department of Electronic and Computer Engineering; the Hong Kong University of Science and Technology,department of Electronic and Computer Engineering; the Hong Kong University of Science and Technology,department of Electronic and Computer Engineering; the Hong Kong University of Science and Technology,department of Electronic and Computer Engineering","2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)","27 Aug 2020","2020","","","3078","3081","Brain-machine interfaces (BMIs) translate neural signals into digital commands to control external devices. During the use of BMI, neurons may change their activity corresponding to the same stimuli or movement. The changes are represented by the neural tuning parameters which may change gradually and abruptly. Adaptive algorithms were proposed to estimate the time-varying parameters in order to keep decoding performance stable. The existing methods only searched new parameters locally which failed to detect the abrupt changes. Global search helps but requires the known boundary of estimated parameter which is hard to be defined in many cases. We propose to estimate the neural modulation parameter by the global search using adaptive point process estimation. This neural modulation parameter represents the similarity between the kinematics and the neural preferred hyper tuning direction with finite range [0,1]. The preferred hyper tuning direction is then decoupled from the neural modulation parameter by gradient descent method. We apply the proposed method on real data to detect the abrupt change of the neural tuning parameter when the subject switched from manual control to brain control mode. The proposed method demonstrates better tracking on the neural hyper tuning parameters than local searching method and validated by KS statistical test.","2694-0604","978-1-7281-1990-8","10.1109/EMBC44109.2020.9175240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9175240","","Tuning;Modulation;Kinematics;Estimation;Decoding;Task analysis;Firing","adaptive estimation;brain-computer interfaces;gradient methods;medical signal processing;neural nets;search problems","adaptive point process method;brain-machine interface;neural tuning parameter;time-varying parameter estimation;global search;neural modulation parameter;adaptive point process estimation;neural preferred hyper tuning direction;brain control mode;local searching method;neural signal translation;BMI;adaptive algorithms;decoding performance stability;gradient descent method;neural hypertuning parameter;KS statistical test","Action Potentials;Algorithms;Brain-Computer Interfaces;Movement;Neurons","","","12","","27 Aug 2020","","","IEEE","IEEE Conferences"
"Classification of hand movement imagery tasks for brain machine interface using feed-forward network","M. S. Z. Azalan; M. P. Paulraj; S. bin Yaacob","School of Mechatronic Engineering, University Malaysia Perlis, Ulu Pauh Permanent Campus, 02600 Arau, Perlis, Malaysia; School of Mechatronic Engineering, University Malaysia Perlis, Ulu Pauh Permanent Campus, 02600 Arau, Perlis, Malaysia; School of Mechatronic Engineering, University Malaysia Perlis, Ulu Pauh Permanent Campus, 02600 Arau, Perlis, Malaysia","2014 2nd International Conference on Electronic Design (ICED)","22 Jan 2015","2014","","","431","436","In this paper, a simple Brain Machine Interface (BMI) system that translates a change of rhythm from brain signal while performing a simulation of hand movement mentally into a real activity movement command is proposed. Four different imaginary tasks are used in the analysis process. A non-stimulus-based BCI approach is used to acquire the brain signal from ten different subjects using 19 channel EEG electrodes. Five spectral band features from each channel are extracted and associated to the respective mental tasks. The features are then classified using Feed-Forward Neural Network. The training is conducted using different ratio of training/testing data set. The developed network models are then tested for its validity. The performance of the developed network models are evaluated through simulation. The result shows that the proposed of both protocol approach and frequency sub band range selection can be an alternative general procedure to classify motor imagery task for a simple BMI system.","","978-1-4799-6103-0","10.1109/ICED.2014.7015844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7015844","Brain Computer;Interface;Motor Imagery;EEG Band Power;Feed-Forward Neural Network","Feature extraction;Accuracy;Electroencephalography;Brain models;Training;Biological neural networks","associative processing;biomechanics;brain-computer interfaces;cognition;electroencephalography;feature extraction;feature selection;feedforward neural nets;learning (artificial intelligence);man-machine systems;medical control systems;medical signal processing;motion control;neurophysiology;protocols;signal classification;spectral analysis","hand movement imagery task classification;brain machine interface system;feedforward neural network;BMI system;brain signal rhythm change translation;hand movement simulation;mental simulation;movement command;imaginary task;nonstimulus-based BCI approach;brain signal acquisition;EEG electrode;spectral band feature extraction;spectral band feature-mental task association;feature classification;training-testing data set ratio;network model validity testing;network model performance evaluation;both protocol approach;frequency sub band range selection;motor imagery task classification","","4","","19","","22 Jan 2015","","","IEEE","IEEE Conferences"
"Generalized virtual fixtures for shared-control grasping in brain-machine interfaces","S. T. Clanton; R. G. Rasmussen; Z. Zohny; M. Velliste","Rehabilitation Institute of Chicago, Chicago, Illinois; Department of Bioengineering, University of Pittsburgh; Washington University Department of Neurosurgery, St. Louis, Missouri; Department of Neurobiology and Systems Neuroscience Institute, University of Pittsburgh, Pittsburgh, Pennsylvania","2013 IEEE/RSJ International Conference on Intelligent Robots and Systems","2 Jan 2014","2013","","","323","328","In brain-machine interface (BMI) prosthetic systems, recordings of brain activity are used to control external devices such as computers or robots. BMI systems that have shown the highest fidelity of control use neural signals recorded directly from microelectrodes in the brain to control upper-limb prostheses. These have progressed from allowing control of 2 and 3 dimensional movement of a cursor on a computer screen [1], [2] to control of robot arms in first four [3], [4] and more recently seven degrees-of-freedom (DoF) (Fig. 1) [5], [6]. These types of systems require methods to train users to control large numbers of DoF simultaneously. In this paper we present a new method for shared-control guidance. This method of ""Positive-Span"" Virtual Fixturing extends the concept of Virtual Fixtures to guide both translational and rotational DoF of a brain-controlled robot hand toward whole sets of robot poses that would allow an object to be grasped. This system was used to successfully train monkeys to operate the 7-DoF BMI [5], leading directly to the simplified system of ""ortho-impedance"" used to guide human subject BMI control in a similar experiment [6].","2153-0866","978-1-4673-6358-7","10.1109/IROS.2013.6696371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696371","","Fixtures;Vectors;Robot kinematics;Grasping;Manipulators;Aerospace electronics","artificial limbs;biomedical electrodes;brain;brain-computer interfaces;manipulators;medical robotics;medical signal processing;neurophysiology;pose estimation","upper-limb prostheses;2 dimensional movement;3 dimensional movement;cursor;computer screen;robot arms control;degrees-of-freedom;shared-control guidance;positive-span virtual fixturing;translational DoF;rotational DoF;brain-controlled robot hand;robot poses;7-DoF BMI;ortho-impedance;microelectrodes;neural signals;external devices;brain activity recordings;BMI prosthetic systems;brain-machine interface;shared-control grasping;generalized virtual fixtures","","1","","7","","2 Jan 2014","","","IEEE","IEEE Conferences"
"Detecting Clinically Meaningful Shape Clusters in Medical Image Data: Metrics Analysis for Hierarchical Clustering Applied to Healthy and Pathological Aortic Arches","J. L. Bruse; M. A. Zuluaga; A. Khushnood; K. McLeod; H. N. Ntsinjana; T. -Y. Hsia; M. Sermesant; X. Pennec; A. M. Taylor; S. Schievano","Centre for Cardiovascular Imaging, UCL Institute of Cardiovascular Science and Cardiorespiratory Unit, Great Ormond Street Hospital for Children, London, U.K.; Translational Imaging Group, Centre for Medical Image ComputingUniversity College London; Centre for Cardiovascular Imaging, UCL Institute of Cardiovascular Science and Cardiorespiratory Unit, Great Ormond Street Hospital for Children; Cardiac Modelling DepartmentSimula Research Laboratory, and KardioMe s.r.o.; Centre for Cardiovascular Imaging, UCL Institute of Cardiovascular Science and Cardiorespiratory Unit, Great Ormond Street Hospital for Children; Centre for Cardiovascular Imaging, UCL Institute of Cardiovascular Science and Cardiorespiratory Unit, Great Ormond Street Hospital for Children; Inria Sophia Antipolis-Méditeranée, ASCLEPIOS Project, Université Côte d'Azur; Inria Sophia Antipolis-Méditeranée, ASCLEPIOS Project, Université Côte d'Azur; Centre for Cardiovascular Imaging, UCL Institute of Cardiovascular Science and Cardiorespiratory Unit, Great Ormond Street Hospital for Children; Centre for Cardiovascular Imaging, UCL Institute of Cardiovascular Science and Cardiorespiratory Unit, Great Ormond Street Hospital for Children","IEEE Transactions on Biomedical Engineering","20 Sep 2017","2017","64","10","2373","2383","Objective: Today's growing medical image databases call for novel processing tools to structure the bulk of data and extract clinically relevant information. Unsupervised hierarchical clustering may reveal clusters within anatomical shape data of patient populations as required for modern precision medicine strategies. Few studies have applied hierarchical clustering techniques to three-dimensional patient shape data and results depend heavily on the chosen clustering distance metrics and linkage functions. In this study, we sought to assess clustering classification performance of various distance/linkage combinations and of different types of input data to obtain clinically meaningful shape clusters. Methods: We present a processing pipeline combining automatic segmentation, statistical shape modeling, and agglomerative hierarchical clustering to automatically subdivide a set of 60 aortic arch anatomical models into healthy controls, two groups affected by congenital heart disease, and their respective subgroups as defined by clinical diagnosis. Results were compared with traditional morphometrics and principal component analysis of shape features. Results: Our pipeline achieved automatic division of input shape data according to primary clinical diagnosis with high F-score (0.902 ± 0.042) and Matthews correlation coefficient (0.851 ± 0.064) using the correlation/weighted distance/linkage combination. Meaningful subgroups within the three patient groups were obtained and benchmark scores for automatic segmentation and classification performance are reported. Conclusion: Clustering results vary depending on the distance/linkage combination used to divide the data. Yet, clinically relevant shape clusters and subgroups could be found with high specificity and low misclassification rates. Significance: Detecting disease-specific clusters within medical image data could improve image-based risk assessment, treatment planning, and medical device development in complex disease.","1558-2531","","10.1109/TBME.2017.2655364","Fondation Leducq; Wellcome Trust; Engineering and Physical Sciences Research Council; Commonwealth Scholarships; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7857751","Aortic arch;automatic segmentation;cardiovascular magnetic resonance imaging;clinical decision support;congenital heart disease;hierarchical clustering;statistical shape analysis","Shape;Principal component analysis;Three-dimensional displays;Medical diagnostic imaging;Sociology","biomedical MRI;cardiovascular system;diseases;image segmentation;medical image processing;pattern clustering;statistical analysis","complex disease;medical device;treatment planning;image-based risk assessment;disease-specific clusters;patient groups;correlation-weighted distance-linkage combination;Matthews correlation coefficient;high F-score;primary clinical diagnosis;congenital heart disease;aortic arch anatomical models;agglomerative hierarchical clustering;statistical shape modeling;link-age functions;three-dimensional patient shape data;applied hierarchical clustering techniques;unsupervised hierarchical clustering;medical image databases;healthy aortic arches;pathological aortic arches;metrics analysis;medical image data;shape clusters","Adolescent;Algorithms;Aorta;Child;Female;Heart Defects, Congenital;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Machine Learning;Magnetic Resonance Imaging, Cine;Male;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity","7","","50","CCBY","16 Feb 2017","","","IEEE","IEEE Journals"
"Recognizing Indonesian Sign Language Gestures Using Features Generated by Elliptical Model Tracking and Angular Projection","D. M. Adimas; E. Rakun; D. Hardianto",University of Indonesia; University of Indonesia; University of Indonesia,"2019 2nd International Conference on Intelligent Autonomous Systems (ICoIAS)","1 Aug 2019","2019","","","25","31","In this paper, we propose a method of feature extraction applied to the hands, fingers and arms, intended to improve the accuracy of Sistem Isyarat Bahasa Indonesia (SIBI) gesture recognition. Using common smartphone camera, we recorded multiple sequence of gestures used to sign for affixes and root words in SIBI then applied image processing and tracking methods to extract features from the shape and location of the hands. To extract the features, skin color segmentation was applied to separate hands and face blob from the background. Then the object would be registered to an ellipse model and tracked through the videos using elliptical model tracking. Finally, the video was then processed frame by frame, and each successfully tracked object is subjected to angular projection to generate the aforementioned features. The model that was used to recognize SIBI gestures is 2-layers Long Short-Term Memory (LSTM) neural network. Accuracy of the proposed method is measured by comparing the prediction with the actual gesture of the testing data. The highest level of accuracy achieved for the prefix, root and suffix datasets are 91.74%, 98.94%, and 97.71% respectively.","","978-1-7281-2662-3","10.1109/ICoIAS.2019.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8782535","Computer Vision;Pattern Recognition;Image Processing;Machine Learning;Deep Learning;Sign Language;Feature Extraction","Autonomous systems","feature extraction;gesture recognition;image classification;image colour analysis;image segmentation;image thinning;language translation;learning (artificial intelligence);natural language processing;recurrent neural nets;sign language recognition;vector quantisation","elliptical model tracking;angular projection;feature extraction;Sistem Isyarat Bahasa Indonesia gesture recognition;root words;image processing;skin color segmentation;ellipse model;SIBI gestures;smartphone camera;long short-term memory neural network;Indonesian sign language gestures","","","","18","","1 Aug 2019","","","IEEE","IEEE Conferences"
"Automatic fiber tractography from DTI and its validation","B. C. Vemuri; Y. Chen; M. Rao; Z. Wang; T. McGraw; T. Mareci; S. J. Blackband; P. Reier","Dept. of CISE, Florida Univ., Gainesville, FL, USA; NA; NA; NA; NA; NA; NA; NA","Proceedings IEEE International Symposium on Biomedical Imaging","7 Nov 2002","2002","","","501","504","To understand evolving pathology in the central nervous system (CNS) and develop effective treatments, it is essential to correlate the nerve fiber connectivity with the visualization of function. Such information is fundamental in CNS processes since anatomical connections determine where information is passed and processed Diffusion tensor imaging (DTI) can provide the fundamental information required for viewing structural connectivity. However robust and accurate acquisition and processing algorithms are needed to accurately map the nerve connectivity. In this paper we present a novel, algorithm for automatic fiber tract mapping in the CNS specifically, a rat spinal cord as well as validate the mapped fibers using ex-vivo fluoro images of the excised rat. The novelty of our work lies in the fiber tract mapping as well as the validation experiment. The automatic fiber tract mapping problem will be solved in two phases, namely a data smoothing phase and a fiber tract mapping phase. In the former smoothing is achieved via a new weighted TV-norm minimization which strives to smooth while retaining all relevant detail. For the fiber tract mapping, a smooth 3D vector field indicating the dominant anisotropic direction at each spatial location is computed from the smoothed data. Fiber tracts are then determined as the smooth integral curves of this vector field in a variational framework Examples are presented for DTI data sets from a normal and injured rat spinal cords respectively.","","0-7803-7584-X","10.1109/ISBI.2002.1029304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1029304","","Diffusion tensor imaging;Smoothing methods;Tensile stress;Visualization;Organisms;Nonlinear equations;Mathematics;Biochemistry;Neuroscience;Robustness","biomedical MRI;biodiffusion;neurophysiology;smoothing methods;medical image processing;vectors;variational techniques;brain","automatic fiber tractography;DTI;evolving pathology;central nervous system;CNS;nerve fiber connectivity;function visualization;anatomical connections;diffusion tensor imaging;structural connectivity;automatic fiber tract mapping;normal rat spinal cord;injured rat spinal cord;ex-vivo fluoro images;excised rat;data smoothing phase;fiber tract mapping phase;weighted TV-norm minimization;smooth 3D vector field;MR measurement;dominant anisotropic direction;water translational self-diffusion;spatial location;brain;white matter fibers;smooth integral curves;vector field;variational framework","","4","","27","","7 Nov 2002","","","IEEE","IEEE Conferences"
"Development of speechreading supplements based on automatic speech recognition","P. Duchnowski; D. S. Lum; J. C. Krause; M. G. Sexton; M. S. Bratakos; L. D. Braida","Res. Lab. of Electron., MIT, Cambridge, MA, USA; NA; NA; NA; NA; NA","IEEE Transactions on Biomedical Engineering","6 Aug 2002","2000","47","4","487","496","In manual-cued speech (MCS) a speaker produces hand gestures to resolve ambiguities among speech elements that are often confused by speechreaders. The shape of the hand distinguishes among consonants; the position of the hand relative to the face distinguishes among vowels. Experienced receivers of MCS achieve nearly perfect reception of everyday connected speech. MCS has been taught to very young deaf children and greatly facilitates language learning, communication, and general education. This manuscript describes a system that can produce a form of cued speech automatically in real time and reports on its evaluation by trained receivers of MCS. Cues are derived by a hidden markov models (HMM)-based speaker-dependent phonetic speech recognizer that uses context-dependent phone models and are presented visually by superimposing animated handshapes on the face of the talker. The benefit provided by these cues strongly depends on articulation of hand movements and on precise synchronization of the actions of the hands and the face, Using the system reported here, experienced cue receivers can recognize roughly two-thirds of the keywords in cued low-context sentences correctly, compared to roughly one-third by speechreading alone (SA). The practical significance of these improvements is to support fairly normal rates of reception of conversational speech, a task that is often difficult via SA.","1558-2531","","10.1109/10.828148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=828148","","Hidden Markov models;Facial animation;Face recognition;Shape;Deafness;Speech analysis;Real time systems;Speech recognition;Automatic speech recognition;Context modeling","speech recognition;hidden Markov models;handicapped aids","speechreading supplements development;automatic speech recognition;manual-cued speech;ambiguities resolution;hand shape;consonants;everyday connected speech;very young deaf children;language learning;general education;cued speech;speaker-dependent phonetic speech recognizer;context-dependent phone models;hand movements articulation;transliteration;autocuer","Adult;Child;Communication Aids for Disabled;Computer Simulation;Cues;Data Display;Deafness;Gestures;Humans;Lipreading;Markov Chains;Models, Biological;Natural Language Processing;Sign Language;Speech Intelligibility;Speech Perception;Speech Production Measurement;User-Computer Interface","16","","43","","6 Aug 2002","","","IEEE","IEEE Journals"
"Autonomous Unobtrusive Detection of Mild Cognitive Impairment in Older Adults","A. Akl; B. Taati; A. Mihailidis","Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, ON, Canada; Toronto Rehabilitation Institute; Institute of Biomaterials and Biomedical Engineering, University of Toronto","IEEE Transactions on Biomedical Engineering","17 Apr 2015","2015","62","5","1383","1394","The current diagnosis process of dementia is resulting in a high percentage of cases with delayed detection. To address this problem, in this paper, we explore the feasibility of autonomously detecting mild cognitive impairment (MCI) in the older adult population. We implement a signal processing approach equipped with a machine learning paradigm to process and analyze real-world data acquired using home-based unobtrusive sensing technologies. Using the sensor and clinical data pertaining to 97 subjects, acquired over an average period of three years, a number of measures associated with the subjects' walking speed and general activity in the home were calculated. Different time spans of these measures were used to generate feature vectors to train and test two machine learning algorithms namely support vector machines and random forests. We were able to autonomously detect MCI in older adults with an area under the ROC curve of 0.97 and an area under the precision-recall curve of 0.93 using a time window of 24 weeks. This study is of great significance since it can potentially assist in the early detection of cognitive impairment in older adults.","1558-2531","","10.1109/TBME.2015.2389149","Oregon Roybal Center for Translational Research on Aging; Oregon Alzheimer's Disease Center; BRP; SAAC Intelligent Systems for Assessing Aging Changes, and Intel Corporation BRP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005481","Mild Cognitive Impairment;Walking Speed;Home Activity;Unobtrusive Sensing Technologies;Older Population;Signal Processing;Smart Systems;Machine Learning;Home activity;machine learning;mild cognitive impairment (MCI);older population;signal processing;smart systems;unobtrusive sensing technologies;walking speed","Vectors;Legged locomotion;Dementia;Biomedical measurement;Sensors;Feature extraction;Monitoring","brain;cognition;diseases;geriatrics;learning (artificial intelligence);medical signal detection;medical signal processing;support vector machines","autonomous unobtrusive detection;mild cognitive impairment;older adults;dementia;signal processing;home-based unobtrusive sensing technologies;feature vectors;machine learning algorithms;support vector machines;random forests;ROC curve;precision-recall curve","Aged;Aged, 80 and over;Algorithms;Artificial Intelligence;Female;Humans;Male;Mild Cognitive Impairment;ROC Curve;Remote Sensing Technology;Signal Processing, Computer-Assisted;Support Vector Machine","54","","31","","9 Jan 2015","","","IEEE","IEEE Journals"
"Multi-Step Ahead Predictions for Critical Levels in Physiological Time Series","H. ElMoaqet; D. M. Tilbury; S. K. Ramachandran","Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of AnesthesiologyMedical School, University of Michigan, Ann Arbor, MI, USA","IEEE Transactions on Cybernetics","20 May 2017","2016","46","7","1704","1714","Standard modeling and evaluation methods have been classically used in analyzing engineering dynamical systems where the fundamental problem is to minimize the (mean) error between the real and predicted systems. Although these methods have been applied to multi-step ahead predictions of physiological signals, it is often more important to predict clinically relevant events than just to match these signals. Adverse clinical events, which occur after a physiological signal breaches a clinically defined critical threshold, are a popular class of such events. This paper presents a framework for multi-step ahead predictions of critical levels of abnormality in physiological signals. First, a performance metric is presented for evaluating multi-step ahead predictions. Then, this metric is used to identify personalized models optimized with respect to predictions of critical levels of abnormality. To address the paucity of adverse events, weighted support vector machines and cost-sensitive learning are used to optimize the proposed framework with respect to statistical metrics that can take into account the relative rarity of such events.","2168-2275","","10.1109/TCYB.2016.2561974","National Center for Advancing Translational Sciences of the National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478676","Predictive modeling;physiological time series;multi-step ahead prediction;prediction performance metrics;support vector machines;Predictive modeling;physiological time series;multi-step ahead prediction;prediction performance metrics;support vector machines","Biomedical monitoring;Predictive models;Time series analysis;Monitoring;Blood;Computational modeling","learning (artificial intelligence);medical signal processing;physiology;prediction theory;support vector machines;time series","multistep ahead predictions;critical levels;physiological time series;physiological signals;clinically relevant events prediction;performance metric;weighted support vector machines;cost-sensitive learning;statistical metrics","","24","","40","","25 May 2016","","","IEEE","IEEE Journals"
"Speech Recognition to Build Context: A Survey","S. Raju; V. Jagtap; P. Kulkarni; M. Ravikanth; M. Rafeeq","CMR Technical Campus,Department of Computer Science Engineering,Hyderabad,Telangana,India; College of Engineering, Pune,Department of Computer Engineering and Information Technology,Pune,Maharashtra,India; iKnowlation Research Labs Pvt. Ltd.,Pune,Maharashtra,India; CMR Technical Campus,Department of Computer Science Engineering,Hyderabad,Telangana,India; CMR Technical Campus,Department of Computer Science Engineering,Hyderabad,Telangana,India","2020 International Conference on Computer Science, Engineering and Applications (ICCSEA)","3 Jul 2020","2020","","","1","7","In era Computer evolution many problems can be solved using computer vision and signal processing. These domains are typically Digitized in binary files like Images, Audio, and Videos. The translation, recognition and synthesis are required while understating the meaning of the binary content. The recognition process is also having many problems in case of audio processing. The missing context is the major reason in pattern-based matching. This is due to unclear or low-quality input, as well as training model on different frequencies but by using context some of the accuracy may improve. Context finding from binary files is a challenge as it works in temporal and space domain. Binary data like images contain special information, while audio files contain temporal information. Video files have both time and space domains. Updating context in the temporal domain, to find proper context from the audio corpus, speech recognition is applied. Over the time period, there are different models adapted like Hidden Markov Model (HMM), Rule Based models with fuzzy support, pattern-based models including machine learning techniques K-nearest neighbor, Support Vector Machine, also latest techniques like Artificial Neural Network (ANN). These technologies are typically included in Automatic Speech Recognition (ASR). ASR uses Language resources with any one of the above models. Here, an in-depth survey on ASR and available APIs. Technologies used to build APIs also discussed.","","978-1-7281-5830-3","10.1109/ICCSEA49143.2020.9132848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9132848","Automatic Speech Recognition (ASR);Language Resource;Machine Learning;Hidden Markov Model (HMM);Artificial Neural Network (ANN);Sequence Modeling;Context determination","","application program interfaces;computer vision;hidden Markov models;nearest neighbour methods;neural nets;speech recognition;support vector machines;video signal processing","artificial neural network;K-nearest neighbor;pattern-based matching models;automatic speech recognition;support vector machine;machine learning techniques;rule based models;hidden Markov model;audio corpus speech recognition;temporal domain;space domains;video files;temporal information;audio files;binary data;temporal space domain;low-quality input;audio processing;binary content;binary files;signal processing;computer vision","","","","27","","3 Jul 2020","","","IEEE","IEEE Conferences"
"A Minimum Spanning Forest-Based Method for Noninvasive Cancer Detection With Hyperspectral Imaging","R. Pike; G. Lu; D. Wang; Z. G. Chen; B. Fei","Emory University School of Medicine; Emory University and the Georgia Institute of Technology; Emory University School of Medicine; Emory University School of Medicine; Department of Radiology and Imaging Sciences, Emory University School of Medicine, Atlanta, GA, USA","IEEE Transactions on Biomedical Engineering","18 Feb 2016","2016","63","3","653","663","Goal: The purpose of this paper is to develop a classification method that combines both spectral and spatial information for distinguishing cancer from healthy tissue on hyperspectral images in an animal model. Methods: An automated algorithm based on a minimum spanning forest (MSF) and optimal band selection has been proposed to classify healthy and cancerous tissue on hyperspectral images. A support vector machine classifier is trained to create a pixel-wise classification probability map of cancerous and healthy tissue. This map is then used to identify markers that are used to compute mutual information for a range of bands in the hyperspectral image and thus select the optimal bands. An MSF is finally grown to segment the image using spatial and spectral information. Conclusion: The MSF based method with automatically selected bands proved to be accurate in determining the tumor boundary on hyperspectral images. Significance: Hyperspectral imaging combined with the proposed classification technique has the potential to provide a noninvasive tool for cancer detection.","1558-2531","","10.1109/TBME.2015.2468578","NIH; Georgia Research Alliance Distinguished Scientists Award; Emory SPORE in Head and Neck Cancer; NIH; Emory Molecular and Translational Imaging Center; NIH; Emory Center for Systems Imaging of the Emory University School of Medicine; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202847","Hyperspectral imaging;image classification;mutual information;minimum spanning forest;noninvasive cancer detection;support vector machine;Hyperspectral imaging;image classification;minimum spanning forest;mutual information;noninvasive cancer detection;support vector machine","Hyperspectral imaging;Support vector machines;Cancer;Classification algorithms;Imaging;Mutual information;Vegetation","cancer;edge detection;hyperspectral imaging;image classification;image segmentation;medical image processing;probability;support vector machines;tumours","minimum spanning forest-based method;noninvasive cancer detection;hyperspectral imaging;animal model;optimal band selection;support vector machine classifier;pixel-wise classification probability map;image segmentation;tumor boundary determination","Animals;Decision Trees;Female;Fluorescent Dyes;Image Interpretation, Computer-Assisted;Mice;Neoplasms;Optical Imaging;Support Vector Machine","39","","52","","14 Aug 2015","","","IEEE","IEEE Journals"
"Automatic Quantification of Radiographic Wrist Joint Space Width of Patients With Rheumatoid Arthritis","Y. Huo; K. L. Vincken; D. van der Heijde; M. J. H. de Hair; F. P. Lafeber; M. A. Viergever","Image Sciences Institute, University Medical Center Utrecht, Utrecht 3584, CX, The Netherlands; Image Sciences InstituteUniversity Medical Center Utrecht; Department of RheumatologyLeiden University Medical Center; Department of Rheumatology and Clinical ImmunologyUniversity Medical Center Utrecht; Department of Rheumatology and Clinical ImmunologyUniversity Medical Center Utrecht; Image Sciences InstituteUniversity Medical Center Utrecht","IEEE Transactions on Biomedical Engineering","18 Oct 2017","2017","64","11","2695","2703","Objective: Wrist joint space narrowing is a main radiographic outcome of rheumatoid arthritis (RA). Yet, automatic radiographic wrist joint space width (JSW) quantification for RA patients has not been widely investigated. The aim of this paper is to present an automatic method to quantify the JSW of three wrist joints that are least affected by bone overlapping and are frequently involved in RA. These joints are located around the scaphoid bone, viz. the multangular-navicular, capitate-navicular-lunate, and radiocarpal joints. Methods: The joint space around the scaphoid bone is detected by using consecutive searches of separate path segments, where each segment location aids in constraining the subsequent one. For joint margin delineation, first the boundary not affected by X-ray projection is extracted, followed by a backtrace process to obtain the actual joint margin. The accuracy of the quantified JSW is evaluated by comparison with the manually obtained ground truth. Results: Two of the 50 radiographs used for evaluation of the method did not yield a correct path through all three wrist joints. The delineated joint margins of the remaining 48 radiographs were used for JSW quantification. It was found that 90% of the joints had a JSW deviating less than 20% from the mean JSW of manual indications, with the mean JSW error less than 10%. Conclusion: The proposed method is able to automatically quantify the JSW of radiographic wrist joints reliably. Significance: The proposed method may aid clinical researchers to study the progression of wrist joint damage in RA studies.","1558-2531","","10.1109/TBME.2017.2659223","Center for Translational Molecular Medicine; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835223","Hand bone segmentation;joint space width quantification;rheumatoid arthritis;wrist joint detection","Joints;Bones;Wrist;Diagnostic radiography;Feature extraction;Indexes","bone;diagnostic radiography;diseases;edge detection;medical image processing","radiographic wrist joint space width;rheumatoid arthritis;scaphoid bone;multangular-navicular joint;capitate-navicular-lunate joint;radiocarpal joint;joint margin delineation;boundary extraction;backtrace process","Algorithms;Arthritis, Rheumatoid;Humans;Radiographic Image Interpretation, Computer-Assisted;Radiography;Wrist Joint","","","30","Traditional","26 Jan 2017","","","IEEE","IEEE Journals"
"Automatic Quantification of Radiographic Finger Joint Space Width of Patients With Early Rheumatoid Arthritis","Y. Huo; K. L. Vincken; D. van der Heijde; M. J. H. De Hair; F. P. Lafeber; M. A. Viergever","Image Sciences Institute, University Medical Center Utrecht, Utrecht, The Netherlands; Image Sciences Institute, University Medical Center Utrecht; Department of Rheumatology, Leiden University Medical Center; Department of Rheumatology and Clinical Immunology, University Medical Center Utrecht; Department of Rheumatology and Clinical Immunology, University Medical Center Utrecht; Image Sciences Institute, University Medical Center Utrecht","IEEE Transactions on Biomedical Engineering","21 Sep 2016","2016","63","10","2177","2186","The assessment of joint space width (JSW) on hand X-ray images of patients suffering from rheumatoid arthritis (RA) is a time-consuming task. Manual assessment is semiquantitative and is observer dependent which hinders an accurate evaluation of joint damage, particularly in the early stages. Automated analysis of the JSW is an important step forward since it is observer independent and might improve the assessment sensitivity in the early RA stage. This study proposes a fully automatic method for both joint location and margin detection in RA hand radiographs. The location detection procedure is based on image features of the joint region and is aided by geometric relationship of finger joints. More than 99% of joint locations are detected with an error smaller than 3 mm with respect to the manually indicated gold standard. The joint margins are detected by combining intensity values and spatially constrained intensity derivatives, refined by an active contour model. More than 96% of the joints are successfully delineated. The JSW is calculated over the middle 60% of a landmark-defined joint span. The overall JSW error compared with the ground truth is 6.8%. In conclusion, the proposed method is able to automatically locate the finger joints in RA hand radiographs, and to quantify the JSW of these joints.","1558-2531","","10.1109/TBME.2015.2512941","Center for Translational Molecular Medicine; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7368104","Finger joint detection;hand bone segmentation;joint space width quantification;rheumatoid arthritis","Thumb;Joints;Radiography;Bones;Biomedical imaging;Electronic mail","bone;diagnostic radiography;diseases;image segmentation;medical image processing;orthopaedics","automatic quantification;radiographic finger joint space width;early rheumatoid arthritis;hand X-ray imaging;time-consuming task;joint damage;automated analysis;fully automatic method;joint location;RA hand radiography;location detection procedure;geometric relationship;manually indicated gold standard;active contour model;landmark-defined joint span;JSW error;ground truth","Adult;Arthritis, Rheumatoid;Female;Finger Joint;Finger Phalanges;Humans;Male;Radiographic Image Interpretation, Computer-Assisted","18","","39","","29 Dec 2015","","","IEEE","IEEE Journals"
"Automatic Identification of Breast Ultrasound Image Based on Supervised Block-Based Region Segmentation Algorithm and Features Combination Migration Deep Learning Model","W. -X. Liao; P. He; J. Hao; X. -Y. Wang; R. -L. Yang; D. An; L. -G. Cui","College of Information and Electrical Engineering, China Agricultural University, Beijing, China; Department of Ultrasound, Peking University Third Hospital, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; Department of Ultrasound, Peking University Third Hospital, Beijing, China; College of Information and Electrical Engineering, China Agricultural University, Beijing, China; Department of Ultrasound, Peking University Third Hospital, Beijing, China","IEEE Journal of Biomedical and Health Informatics","3 Apr 2020","2020","24","4","984","993","Breast cancer is a high-incidence type of cancer for women. Early diagnosis plays a crucial role in the successful treatment of the disease and the effective reduction of deaths. In this paper, deep learning technology combined with ultrasound imaging diagnosis was used to identify and determine whether the tumors were benign or malignant. First, the tumor regions were segmented from the breast ultrasound (BUS) images using the supervised block-based region segmentation algorithm. Then, a VGG-19 network pretrained on the ImageNet dataset was applied to the segmented BUS images to predict whether the breast tumor was benign or malignant. The benchmark data for bio-validation were obtained from 141 patients with 199 breast tumors, including 69 cases of malignancy and 130 cases of benign tumors. The experiment showed that the accuracy of the supervised block-based region segmentation algorithm was almost the same as that of manual segmentation; therefore, it can replace manual work. The diagnostic effect of the combination feature model established based on the depth feature of the B-mode ultrasonic imaging and strain elastography was better than that of the model established based on these two images alone. The correct recognition rate was 92.95%, and the AUC was 0.98 for the combination feature model.","2168-2208","","10.1109/JBHI.2019.2960821","National Key Research and Development Program of China Stem Cell and Translational Research; Wireless Probe Type Handheld Intelligent Ultrasound Imager; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936948","Breast tumor;Elastography;Ultrasound images;Convolutional neural network;identification","Ultrasonic imaging;Image segmentation;Breast tumors;Cancer;Feature extraction;Elastography","biomedical ultrasonics;cancer;diseases;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing;tumours;ultrasonic imaging","B-mode ultrasonic imaging;migration deep learning model;breast tumor;segmented BUS images;tumor regions;ultrasound imaging diagnosis;breast cancer;supervised block-based region segmentation algorithm;breast ultrasound image","Algorithms;Breast;Breast Neoplasms;Deep Learning;Female;Humans;Image Interpretation, Computer-Assisted;Sensitivity and Specificity;Supervised Machine Learning;Ultrasonography, Mammary","","","37","IEEE","19 Dec 2019","","","IEEE","IEEE Journals"
"An Improved LSTM Structure for Natural Language Processing","L. Yao; Y. Guan","Qingdao No.2 Middle School, Qingdao, China; Intelligence and Big Data Research Center, Global Wisdom Inc., Beijing, China","2018 IEEE International Conference of Safety Produce Informatization (IICSPI)","14 Apr 2019","2018","","","565","569","Natural language processing technology is widely used in artificial intelligence fields such as machine translation, human-computer interaction and speech recognition. Natural language processing is a daunting task due to the variability, ambiguity and context-dependent interpretation of human language. The current deep learning technology has made great progress in NLP technology. However, many NLP systems still have practical problems, such as high training complexity, computational difficulties in large-scale content scenarios, high retrieval complexity and lack of probabilistic significance. This paper proposes an improved NLP method based on long short-term memory (LSTM) structure, whose parameters are randomly discarded when they are passed backwards in the recursive projection layer. Compared with baseline and other LSTM, the improved method has better F1 score results on the Wall Street Journal dataset, including the word2vec word vector and the one-hot word vector, which indicates that our method is more suitable for NLP in limited computing resources and high amount of data.","","978-1-5386-5514-6","10.1109/IICSPI.2018.8690387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8690387","NLP;LSTM;machine translation;deep learning;recursive projection layer","Natural language processing;Deep learning;Grammar;Task analysis;Semantics;Neural networks","learning (artificial intelligence);natural language processing;neural nets;speech recognition","natural language processing;artificial intelligence fields;human-computer interaction;speech recognition;context-dependent interpretation;human language;high training complexity;computational difficulties;high retrieval complexity;improved NLP method;short-term memory structure;improved LSTM structure;deep learning technology","","1","","12","","14 Apr 2019","","","IEEE","IEEE Conferences"
"A Coaxial Alignment Method for Large Flange Parts Assembly Using Multiple Local Images","W. Sun; Z. Zhang; W. Zhang","School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China","IEEE Access","29 Jan 2021","2021","9","","16716","16727","The large flange parts in aero-engines are usually manually assembled. Collision damage caused by assembly alignment error often occurs in the assembly process, which affects the assembly accuracy and product reliability. The machine vision-based alignment methods usually achieve the high-precision measurement of the parts by obtaining the high-resolution images of the whole parts with a combination of laser distance sensors. Hence, existing methods are high costly and inefficient. In this paper, a new alignment method based on the principle of coaxial alignment for large flange parts is proposed. The proposed method first obtains multiple high-precision image pairs from the local field of views at the fitting surfaces of flanges. The clearance and bolt holes in each image pair are then extracted via edge recognition and Hough Transformation. Two optimization models are built to calculate the translation adjustment and rotation adjustment. The optimization model 1 is built with the translation adjustments of the flange as the variables and the consistency of the clearances as the objective function. The algorithm to solve the model based on the gradient-descent method is proposed. The positions of the bolt holes in the images are subsequently adjusted based on the translation adjustment, and the rotation adjustment is calculated by solving the built optimization model 2. The experiments show that the proposed method can be applied to the assembly process of large flange parts, and the visual servo control model based on this method also has good stability.","2169-3536","","10.1109/ACCESS.2021.3054618","Joint Fund of Advanced Aerospace Manufacturing Technology Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336000","Coaxial alignment;large flange parts;assembly;machine vision","Cameras;Feature extraction;Fasteners;Optimization;Sensors;Image edge detection;Robotic assembly","aerospace engines;assembling;computer vision;edge detection;flanges;gradient methods;Hough transforms;optimisation;visual servoing","translation adjustment;gradient-descent method;bolt holes;rotation adjustment;assembly process;coaxial alignment method;assembly alignment error;product reliability;high-precision measurement;high-resolution images;high-precision image pairs;machine vision-based alignment methods;flange part assembly;visual servo control model;collision damage;laser distance sensors;edge recognition;Hough transformation;aero-engines","","","","28","CCBYNCND","26 Jan 2021","","","IEEE","IEEE Journals"
"A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction","T. Wiatowski; H. Bölcskei","Department of Information Technology and Electrical Engineering, ETH Zürich, Zürich, Switzerland; Department of Information Technology and Electrical Engineering, ETH Zürich, Zürich, Switzerland","IEEE Transactions on Information Theory","16 Feb 2018","2018","64","3","1845","1866","Deep convolutional neural networks (DCNNs) have led to breakthrough results in numerous practical machine learning tasks, such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a classifier. The mathematical analysis of DCNNs for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory that encompasses general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned filters), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating, e.g., sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor, we prove a translation invariance result of vertical nature in the sense of the features becoming progressively more translation-invariant with increasing network depth, and we establish deformation sensitivity bounds that apply to signal classes such as, e.g., band-limited functions, cartoon functions, and Lipschitz functions.","1557-9654","","10.1109/TIT.2017.2776228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8116648","Machine learning;deep convolutional neural networks;scattering networks;feature extraction;frame theory","Feature extraction;Strain;Wavelet transforms;Convolution;Sensitivity;Scattering","feature extraction;image classification;learning (artificial intelligence);mathematical analysis;neural nets;wavelet transforms","image captioning;feature extraction;mathematical analysis;DCNNs;scattering networks;modulus nonlinearity;network layer;wavelet scale parameter;general convolutional transforms;learned filters;rectified linear units;general Lipschitz-continuous pooling operators;network depth;mathematical theory;deep convolutional neural networks;ImageNet data set;control-policy-learning;Atari games;board game;deformation sensitivity bounds;general Lipschitz-continuous nonlinearities;semidiscrete frames;deformation stability;wavelet transform;image classification;translation-invariant;machine learning tasks;feature extractor","","29","","84","","21 Nov 2017","","","IEEE","IEEE Journals"
"Full Parameter Time Complexity (FPTC): A Method to Evaluate the Running Time of Machine Learning Classifiers for Land Use/Land Cover Classification","X. Zheng; J. Jia; S. Guo; J. Chen; L. Sun; Y. Xiong; W. Xu","Center for Geo-Spatial Information, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Center for Geo-Spatial Information, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Center for Geo-Spatial Information, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Center for Geo-Spatial Information, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Center for Geo-Spatial Information, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Center for Geo-Spatial Information, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Center for Geo-Spatial Information, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","10 Feb 2021","2021","14","","2222","2235","In emergency responses to natural disasters, actionable information provided by remote sensing images is crucial to help emergency managers become aware of the situation and assess the magnitude of the damage. Without the accurate prediction of time consumption, choosing an algorithm for land use/land cover (LULC) classification under these emergency circumstances could be blind and subjective. Here, we proposed a full parameter time complexity (FPTC) analysis and the corresponding coefficient ω to estimate the actual running time of the LULC classification without actually running the code. The FPTC of five general algorithms is derived in this article. After derivation, the FPTC of k-nearest neighbors (kNN) is F(nv + nlog2 u), the FPTC of logistic regression (LR) is F(Qm2vn), the FPTC of classification and regression tree (CART) is F((m + 1)nvlog2n), the FPTC of random forest (RF) is F(s(m + 1)nvlog2n), and the FPTC of support vector machine (SVM) is F(m2Qv (n + k)). The results show a strong linear relationship between the actual running time and FPTC [R-squared: kNN (0.991), LR (0.997), CART (0.999), RF (1.000), and SVM (0.999)], with different data size. The average root-mean-squared error between the real running time and the estimated running time is 3.34 s, which demonstrates the effectiveness of FPTC. Combining FPTC with the corresponding coefficient ω, the running time of the classification can be precisely predicted, which will help emergency managers quickly choose algorithms in response to natural disasters with available remote sensing data and limited time.","2151-1535","","10.1109/JSTARS.2021.3050166","Strategic Priority Research Program of the Chinese Academy of Sciences; National Key Research and Development Program of China Stem Cell and Translational Research; National Natural Science Foundation of China; Fundamental Research Foundation of Shenzhen Technology and Innovation Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9317826","Algorithm running time;full parameter time complexity (FPTC);land use/land cover (LULC) classification;Sentinel-2a;traditional time complexity (TTC)","Prediction algorithms;Support vector machines;Time complexity;Training;Remote sensing;Radio frequency;Task analysis","computational complexity;emergency management;geophysical image processing;image classification;land cover;learning (artificial intelligence);mean square error methods;regression analysis;remote sensing;support vector machines","emergency managers;FPTC;full parameter time complexity analysis;machine learning classifiers;land use/land cover classification;LULC classification;k-nearest neighbor;logistic regression;classification and regression tree;random forest;support vector machine;root-mean-squared error;natural disaster response;time 3.34 s","","","","35","CCBY","8 Jan 2021","","","IEEE","IEEE Journals"
"3D Human Pose Machines with Self-Supervised Learning","K. Wang; L. Lin; C. Jiang; C. Qian; P. Wei","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; SenseTime Group, Hong KongChina; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","2 Apr 2020","2020","42","5","1069","1082","Driven by recent computer vision and robotic applications, recovering 3D human poses has become increasingly important and attracted growing interests. In fact, completing this task is quite challenging due to the diverse appearances, viewpoints, occlusions and inherently geometric ambiguities inside monocular images. Most of the existing methods focus on designing some elaborate priors /constraints to directly regress 3D human poses based on the corresponding 2D human pose-aware features or 2D pose predictions. However, due to the insufficient 3D pose data for training and the domain gap between 2D space and 3D space, these methods have limited scalabilities for all practical scenarios (e.g., outdoor scene). Attempt to address this issue, this paper proposes a simple yet effective self-supervised correction mechanism to learn all intrinsic structures of human poses from abundant images. Specifically, the proposed mechanism involves two dual learning tasks, i.e., the 2D-to-3D pose transformation and 3D-to-2D pose projection, to serve as a bridge between 3D and 2D human poses in a type of “free” self-supervision for accurate 3D human pose estimation. The 2D-to-3D pose implies to sequentially regress intermediate 3D poses by transforming the pose representation from the 2D domain to the 3D domain under the sequence-dependent temporal context, while the 3D-to-2D pose projection contributes to refining the intermediate 3D poses by maintaining geometric consistency between the 2D projections of 3D poses and the estimated 2D poses. Therefore, these two dual learning tasks enable our model to adaptively learn from 3D human pose data and external large-scale 2D human pose data. We further apply our self-supervised correction mechanism to develop a 3D human pose machine, which jointly integrates the 2D spatial relationship, temporal smoothness of predictions and 3D geometric knowledge. Extensive evaluations on the Human3.6M and HumanEva-I benchmarks demonstrate the superior performance and efficiency of our framework over all the compared competing methods.","1939-3539","","10.1109/TPAMI.2019.2892452","National Key Research and Development Program of China Stem Cell and Translational Research; National High Level Talents Special Support Plan; National Natural Science Foundation of China; Ministry of Public Security Science and Technology Police Foundation; Guandong “Climbing Program”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611195","Human pose estimation;convolutional neural networks;spatio-temporal modeling;self-supervised learning;geometric deep learning","Three-dimensional displays;Two dimensional displays;Pose estimation;Solid modeling;Task analysis;Deep learning;Feature extraction","computer vision;image motion analysis;image representation;learning (artificial intelligence);object tracking;pose estimation;regression analysis","dual learning tasks;3D-to-2D pose projection;intermediate 3D poses;pose representation;estimated 2D poses;3D human pose machine;2D spatial relationship;3D geometric knowledge;self-supervised correction mechanism;2D human pose-aware features;3D human pose machines;large-scale 2D human pose data;Human3.6M benchmarks;HumanEva-I benchmarks;computer vision;robotic applications","Algorithms;Databases, Factual;Deep Learning;Humans;Imaging, Three-Dimensional;Posture;Supervised Machine Learning;Video Recording","4","","61","IEEE","13 Jan 2019","","","IEEE","IEEE Journals"
"Evaluation of time-domain features for motor imagery movements using FCM and SVM","A. Khorshidtalab; M. J. E. Salami; M. Hamedi","Department of Mechatronics Engineering, International Islamic University Malaysia, Gombak, Malaysia; Department of Mechatronics Engineering, International Islamic University Malaysia, Gombak, Malaysia; Faculty of Biomedical and Health Science Engineering, Universiti Teknologi Malaysia, Johor Bahru, Malaysia","2012 Ninth International Conference on Computer Science and Software Engineering (JCSSE)","9 Aug 2012","2012","","","17","22","Brain-Machine Interface is a direct communication pathway between brain and an external electronic device. BMIs aim to translate brain activities into control commands. To design a system that translates brain waves and its activities to desired commands, motor imagery tasks classification is the core part. Classification accuracy not only depends on how capable the classifier is but also it is about the input data. Feature extraction is to highlight the properties of signal that make it distinct from the signal of the other mental tasks. Performance of BMIs directly depends on the effectiveness of the feature extraction and classification algorithms. If a feature provides large interclass difference for different classes, the applied classifier exhibits a better performance. In order to attain less computational complexity, five time-domain procedure, namely: Mean Absolute Value, Maximum peak value, Simple Square Integral, Willison Amplitude, and Waveform Length are used for feature extraction of EEG signals. Two classifiers are applied to assess the performance of each feature-subject. SVM with polynomial kernel is one of the applied nonlinear classifier and supervised FCM is the other one. The performance of each feature for input data are evaluated with both classifiers and classification accuracy is the considered common comparison parameter.","","978-1-4673-1921-8","10.1109/JCSSE.2012.6261918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261918","Electroencephalogram;Feature extraction;Motor imagery;Brain-Machine Interface;Support Vector Machine;Fuzzy C-Means","Support vector machines;Accuracy;Feature extraction;Electroencephalography;Bars;Brain;Standards","brain-computer interfaces;electroencephalography;feature extraction;medical signal processing;pattern clustering;polynomials;signal classification;support vector machines;time-domain analysis","time-domain features evaluation;motor imagery movements;FCM;SVM;brain-machine interface;motor imagery tasks classification;feature extraction;mean absolute value;maximum peak value;simple square integral;Willison amplitude;waveform length;EEG signals;polynomial kernel;nonlinear classifier;fuzzy c-means","","6","","20","","9 Aug 2012","","","IEEE","IEEE Conferences"
"Estimation of Cardiac Respiratory-Motion by Semi-Automatic Segmentation and Registration of Non-Contrast-Enhanced 4D-CT Cardiac Datasets","J. Dey; T. Pan; D. J. Choi; D. Robotis; M. S. Smyczynski; P. H. Pretorius; M. A. King","Med. Sch., Dept. of Radiol., Univ. of Massachusetts, Worcester, MA, USA; Dept. of Imaging Phys., Univ. of Texas, Houston, TX, USA; Dept. of Radiol., Brigham & Women's Hosp., Boston, MA, USA; Div. of Cardiology, Univ. of Massachusetts Memorial Health Care, Worcester, MA, USA; Med. Sch., Dept. of Radiol., Univ. of Massachusetts, Worcester, MA, USA; Med. Sch., Dept. of Radiol., Univ. of Massachusetts, Worcester, MA, USA; Med. Sch., Dept. of Radiol., Univ. of Massachusetts, Worcester, MA, USA","IEEE Transactions on Nuclear Science","8 Dec 2009","2009","56","6","3662","3671","The goal of this work is to investigate, for a large set of patients, the motion of the heart with respiration during free-breathing supine medical imaging. For this purpose we analyzed the motion of the heart in 32 non-contrast enhanced respiratory-gated 4D-CT datasets acquired during quiet unconstrained breathing. The respiratory-gated CT images covered the cardiac region and were acquired at each of 10 stages of the respiratory cycle, with the first stage being end-inspiration. We devised a 3-D semi-automated segmentation algorithm that segments the heart in the 4D-CT datasets acquired without contrast enhancement for use in estimating respiratory motion of the heart. Our semi-automated segmentation results were compared against interactive hand segmentations of the coronal slices by a cardiologist and a radiologist. The pairwise difference in segmentation among the algorithm and the physicians was on the average 11% and 10% of the total average segmented volume across the patient, with a couple of patients as outliers above the 95% agreement limit. The mean difference among the two physicians was 8% with an outlier above the 95% agreement limit. The 3-D segmentation was an order of magnitude faster than the Physicians' manual segmentation and represents significant reduction of Physicians' time. The segmented first stages of respiration were used in 12 degree-of-freedom (DOF) affine registration to estimate the motion at each subsequent stage of respiration. The registration results from the 32 patients indicate that the translation in the superior-inferior direction was the largest component motion, with a maximum of 10.7 mm, mean of 6.4 mm, and standard deviation of 2.2 mm. Translation in the anterior-posterior direction was the next largest component of motion, with a maximum of 4.0 mm, mean of 1.7 mm, and standard deviation of 1.0 mm. Rotation about the right-left axis was on average the largest component of rotation observed, with a maximum of 4.6 degrees, mean of 1.6 degrees, and standard deviation of 2.1 degrees. The other rotation and shear parameters were all close to zero on average indicting the motion could be reasonably well approximated by rigid-body motion. However, the product of the three scale factors averaged about 0.97 indicating the possibility of a small decrease in heart volume with expiration. The motion results were similar whether we used the Physician's segmentation or the 3-D algorithm.","1558-1578","","10.1109/TNS.2009.2031642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5341445","Image segmentation;imaging applications;PET/CT;SPECT/CT","Heart;Biomedical imaging;Image segmentation;Computed tomography;Medical robotics;Motion estimation;Cardiology;Lungs;Blood;Biomedical engineering","affine transforms;biomechanics;cardiology;computerised tomography;diagnostic radiography;image enhancement;image registration;image segmentation;medical image processing;motion estimation;pneumodynamics","cardiac respiratory-motion estimation;3D semiautomatic segmentation algorithm;semiautomatic registration;noncontrast-enhanced 4D-CT cardiac datasets;free-breathing supine medical imaging;heart motion;quiet unconstrained breathing;coronal slices;12-degree-of-freedom affine registration;superior-inferior direction;anterior-posterior direction","","11","","29","","8 Dec 2009","","","IEEE","IEEE Journals"
"A New Approach to Automatic Clothing Matting from Mannequins","B. Yuan; Z. Lu; J. -H. Xue; Q. Liao","Tsinghua University, China; Tsinghua University, China; University College London, UK; Tsinghua University, China","2019 IEEE International Conference on Multimedia and Expo (ICME)","5 Aug 2019","2019","","","880","885","It is crucial to extract retail clothes from images of mannequins when building a database of clothing images for virtual try-on systems. However, clothes often have complex texture and translucent material, such as holes and laces. It is thus difficult to extract clothes as foreground by existing generic natural image matting methods. Hence in this paper, we present a novel approach to automatic clothing matting from mannequins, with auxiliary information from a rough background image of the mannequin only. Experiments show that we can achieve remarkable improvement on the alpha matte near challenging regions of complex texture and translucent material of clothes. Moreover, our approach can automatically generate trimaps to facilitate the development and evaluation of other image matting algorithms.","1945-788X","978-1-5386-9552-4","10.1109/ICME.2019.00156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784984","Clothing image matting;Virtual clothing try-on","Clothing;Image color analysis;Databases;Buildings;Memory management;Bayes methods;Measurement","clothing;computer graphics;feature extraction;image colour analysis;image segmentation;image texture","clothing images;automatic clothing matting;mannequin;image matting algorithms;background image;retail clothes extraction;virtual try-on systems","","","","16","","5 Aug 2019","","","IEEE","IEEE Conferences"
"A New Method for Automatic Sleep Stage Classification","J. Zhang; Y. Wu","College of Electronics and Information Engineering, Tongji University, Shanghai, China; College of Electronics and Information Engineering, Tongji University, Shanghai, China","IEEE Transactions on Biomedical Circuits and Systems","25 Sep 2017","2017","11","5","1097","1110","Traditionally, automatic sleep stage classification is quite a challenging task because of the difficulty in translating open-textured standards to mathematical models and the limitations of handcrafted features. In this paper, a new system for automatic sleep stage classification is presented. Compared with existing sleep stage methods, our method can capture the sleep information hidden inside electroencephalography (EEG) signals and automatically extract features from raw data. To translate open sleep stage standards into machine rules recognized by computers, a new model named fast discriminative complex-valued convolutional neural network (FDCCNN) is proposed to extract features from raw EEG data and classify sleep stages. The new model combines complex-valued backpropagation and the Fisher criterion. It can learn discriminative features and overcome the negative effect of imbalance dataset. More importantly, the orthogonal decision boundaries for the real and imaginary parts of a complex-valued convolutional neuron are proven. A speed-up algorithm is proposed to reduce computational workload and yield improvements of over an order of magnitude compared to the normal convolution algorithm. The classification performances of handcrafted features and different convolutional neural networks are compared with that of the FDCCNN. The total accuracy and kappa coefficient of the proposed method are 92% and 0.84, respectively. Experiment results demonstrated that the performance of our system is comparable to those of human experts.","1940-9990","","10.1109/TBCAS.2017.2719631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010320","Convolutional neural network;electroencephalography;feature extraction;fisher criterion;sleep stage","Sleep;Convolution;Electroencephalography;Feature extraction;Biological neural networks;Brain modeling;Neurons","backpropagation;electroencephalography;feature extraction;medical signal processing;neural nets;signal classification;sleep","automatic sleep stage classification;open-textured standards;mathematical models;handcrafted features;kappa coefficient;classification performances;normal convolution algorithm;order of magnitude;computational workload;speed-up algorithm;complex-valued convolutional neuron;orthogonal decision boundaries;imbalance dataset;Fisher criterion;complex-valued backpropagation;raw EEG data;FDCCNN;fast discriminative complex-valued convolutional neural network;machine rules;open sleep stage standards;electroencephalography signals;sleep information;sleep stage methods","Algorithms;Electroencephalography;Humans;Machine Learning;Models, Theoretical;Neural Networks (Computer);Signal Processing, Computer-Assisted;Sleep Stages","12","","85","Traditional","14 Aug 2017","","","IEEE","IEEE Journals"
"A High-Precision Automatic Wire Wrapping Approach Based on Microscopic Vision and Force Information","S. Liu; Y. Li","Department of Electrical Engineering–Electrophysics, University of Southern California, Los Angeles, CA, USA; Department of Mechanical Engineering, City University of Hong Kong, Kowloon, Hong Kong","IEEE Transactions on Industrial Informatics","8 Jan 2020","2020","16","1","161","170","This paper proposes a wire wrapping approach, which enables high-precision, fully automatic, quality controllable, and visually monitored wire wrapping by actively rotating the rod and coordinately translating the wire based on microscopic vision and force information. Viewing the wire as a one-dimensional object, the proposed paper contributes to both the precision manipulation field and the engineering utilities to fabricate precision helical structures used in many fields. The basic technical contents involved are the active rotation of the rod and the coordinated translation of the manipulator, both of which are designed to keep the relative spatial relationship and the interactive force between the rod and the wire. Extensive experiments were conducted to validate the effectiveness of the proposed method to achieve high-precision wire wrapping. Experimental results show that with discretized active rotation increment about the rod axis of 6°, the local tensile deformation of the wire can be controlled within ±2 μm error range, while the average local helical angle can be controlled within ±0.3° error range with standard deviation less than 1.5°.","1941-0050","","10.1109/TII.2019.2914470","Research Grants Council of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704906","Force servo control;microscope;precision manipulation;wire wrapping","Wires;Wrapping;Force;Microscopy;Manipulators;Jacobian matrices;Informatics","computer vision;deformation;electronic engineering computing;precision engineering;quality control;wires","tensile deformation;relative spatial relationship;interactive force;precision helical structures;precision manipulation field;force information;microscopic vision;high-precision automatic wire wrapping approach","","","","25","IEEE","2 May 2019","","","IEEE","IEEE Journals"
"Automatic tracking of cervical spine using fluoroscopic sequences","M. Nauman; A. Hassan; F. Riaz; S. Rehman; R. W. Nedergard; K. Holt; H. Haavik; I. K. Niazi","Department of Computer Engineering, College of Electrical and Mechanical Engineering, National University of Sciences &Technology (NUST), Islamabad, Pakistan; Department of Computer Engineering, College of Electrical and Mechanical Engineering, National University of Sciences &Technology (NUST), Islamabad, Pakistan; Department of Computer Engineering, College of Electrical and Mechanical Engineering, National University of Sciences &Technology (NUST), Islamabad, Pakistan; Department of Computer Engineering, College of Electrical and Mechanical Engineering, National University of Sciences &Technology (NUST), Islamabad, Pakistan; Centre for Chiropractic Research, New Zealand College of Chiropractic Auckland, New Zealand; Centre for Chiropractic Research, New Zealand College of Chiropractic Auckland, New Zealand; Centre for Chiropractic Research, New Zealand College of Chiropractic Auckland, New Zealand; Centre for Chiropractic Research, New Zealand College of Chiropractic Auckland, New Zealand","2017 Intelligent Systems Conference (IntelliSys)","26 Mar 2018","2017","","","592","598","In this paper an automatic tracking approach is proposed for the measurement of the cervical spine using Kanade-Lucas-Tomasi (KLT) feature tracking algorithm through fluoroscopic sequences. Previous research related to cervical vertebrae shows that abnormalities in the cervical vertebrae structures may affect the movement of the cervical spine. The aim of this paper is to automate the detection of range of movement in a lateral view of the spine during a flexion-extension cycle. The parameters analyzed were translation and in-plane rotation of the individual vertebrae. For analysis of these parameters, fluoroscopic recordings of three individuals were used. The algorithm marked landmarks on first frame, considered them as reference position and extracted translation and rotation of these vertebrae landmarks in the successive frames using Harris corner detector and use link motion vector for trajectory. Manual selection of vertebrae C3 to C6 (annotated data) were used for the validation of the proposed algorithm. The automated results are very close and uniform to the manual selection.","","978-1-5090-6435-9","10.1109/IntelliSys.2017.8324355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8324355","Cervical vertebrae;range of motion;flexion-extension;segmentation;annotated data","Manuals;Tracking;Motion segmentation;Intelligent systems;Feature extraction;Spine;Transforms","biomechanics;bone;diagnostic radiography;feature extraction;image motion analysis;image sequences;medical image processing;neurophysiology;orthopaedics;physiological models","Kanade-Lucas-Tomasi feature tracking algorithm;fluoroscopic recordings;cervical vertebrae structures;fluoroscopic sequences;cervical spine;automatic tracking approach","","","","15","","26 Mar 2018","","","IEEE","IEEE Conferences"
"Espresso: A Fast End-to-End Neural Speech Recognition Toolkit","Y. Wang; T. Chen; H. Xu; S. Ding; H. Lv; Y. Shao; N. Peng; L. Xie; S. Watanabe; S. Khudanpur","Center of Language and Speech Processing, Johns Hopkins University,Baltimore,MD,USA; Center of Language and Speech Processing, Johns Hopkins University,Baltimore,MD,USA; Center of Language and Speech Processing, Johns Hopkins University,Baltimore,MD,USA; Center of Language and Speech Processing, Johns Hopkins University,Baltimore,MD,USA; ASLP@NPU, School of Computer Science, Northwestern Polytechnical University,Xian,China; Center of Language and Speech Processing, Johns Hopkins University,Baltimore,MD,USA; Information Sciences Institute, University of Southern California,Los Angeles,CA,USA; ASLP@NPU, School of Computer Science, Northwestern Polytechnical University,Xian,China; Center of Language and Speech Processing, Johns Hopkins University,Baltimore,MD,USA; Human Language Technology Center of Excellence, Johns Hopkins University,Baltimore,MD,USA","2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)","20 Feb 2020","2019","","","136","143","We present Espresso, an open-source, modular, extensible end-to-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit FAIRSEQ. ESRESSO supports distributed training across GPUs and computing nodes, and features various decoding approaches commonly employed in ASR, including look-ahead word-based language model fusion, for which a fast, parallelized decoder is implemented. Espresso achieves state-of-the-art ASR performance on the WSJ, LibriSpeech, and Switchboard data sets among other end-to-end systems without data augmentation, and is 4-11x faster for decoding than similar systems (e.g. ESPNET).","","978-1-7281-0306-8","10.1109/ASRU46091.2019.9003968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003968","automatic speech recognition;end-to-end;parallel decoding;language model fusion","Decoding;Speech recognition;Training;Computational modeling;Gold;Python;Smoothing methods","natural language processing;neural nets;speech recognition","Espresso;end-to-end neural automatic speech recognition toolkit;deep learning library PyTorch;word-based language model fusion;ASR performance;end-to-end systems;neural machine translation toolkit FAIRSEQ;WSJ data sets;Switchboard data sets;LibriSpeech data sets","","5","","60","","20 Feb 2020","","","IEEE","IEEE Conferences"
"AeGAN: Time-Frequency Speech Denoising via Generative Adversarial Networks","S. Abdulatif; K. Armanious; K. Guirguis; J. T. Sajeev; B. Yang","University of Stuttgart,Institute of Signal Processing and System Theory,Stuttgart,Germany; University of Stuttgart,Institute of Signal Processing and System Theory,Stuttgart,Germany; University of Stuttgart,Institute of Signal Processing and System Theory,Stuttgart,Germany; University of Stuttgart,Institute of Signal Processing and System Theory,Stuttgart,Germany; University of Stuttgart,Institute of Signal Processing and System Theory,Stuttgart,Germany","2020 28th European Signal Processing Conference (EUSIPCO)","18 Dec 2020","2021","","","451","455","Automatic speech recognition (ASR) systems are of vital importance nowadays in commonplace tasks such as speech-to-text processing and language translation. This created the need for an ASR system that can operate in realistic crowded environments. Thus, speech enhancement is a valuable building block in ASR systems and other applications such as hearing aids, smartphones and teleconferencing systems. In this paper, a generative adversarial network (GAN) based framework is investigated for the task of speech enhancement, more specifically speech denoising of audio tracks. A new architecture based on CasNet generator and an additional feature-based loss are incorporated to get realistically denoised speech phonetics. Finally, the proposed framework is shown to outperform other learning and traditional model-based speech enhancement approaches.","2076-1465","978-9-0827-9705-3","10.23919/Eusipco47968.2020.9287606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9287606","Speech enhancement;generative adversarial networks;automatic speech recognition;deep learning","Training;Time-frequency analysis;Noise reduction;Speech enhancement;Generative adversarial networks;Generators;Task analysis","hearing aids;signal denoising;speech enhancement;speech recognition;teleconferencing;text analysis","time-frequency speech;generative adversarial networks;automatic speech recognition systems;commonplace tasks;speech-to-text processing;language translation;ASR system;realistic crowded environments;valuable building block;teleconferencing systems;generative adversarial network based framework;speech denoising;CasNet generator;additional feature-based loss;realistically denoised speech phonetics;traditional model-based speech enhancement approaches","","","","28","","18 Dec 2020","","","IEEE","IEEE Conferences"
"Disambiguation and Error Resolution in Call Transcripts","J. Hosier; V. K. Gurbani; N. Milstead","Vail Systems, Inc.; Vail Systems, Inc.; Vail Systems, Inc.","2019 IEEE International Conference on Big Data (Big Data)","24 Feb 2020","2019","","","4602","4607","Ambiguity is inherent to human language and poses a unique challenge both to human listeners as well as natural language processing (NLP) systems. Ambiguity is understood as a type of uncertainty which allows for more than one plausible interpretation of utterances. Ambiguity can introduce problems for NLP systems designed to, for example, execute machine translation, determine sentiment, and perform automatic speech recognition (ASR). We seek to identify and resolve mis-transcriptions that arise from phonetic ambiguity and degraded acoustic signals in 87,000 call transcripts. We first present an alignment algorithm which identifies mis-transcriptions generated by ASR systems when compared against verified human transcriptions. This method not only allows for a general evaluation of ASR performance but also highlights specific areas of difficulty for such systems (e.g. “considerate” vs. “consider it”). We further present an error resolution algorithm which, given a mis-transcribed word, uses contextual cues to suggest a more likely, phonetically similar word. This work has the potential to not only evaluate existing ASR systems, but also to immediately improve their performance.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9005993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005993","homophone disambiguation;automatic speech recognition;text processing","Phonetics;Encoding;Natural language processing;Engines;Approximation algorithms;Big Data;Acoustics","natural language processing;speech recognition","call transcripts;human language;human listeners;natural language processing systems;NLP systems;machine translation;automatic speech recognition;mis-transcriptions;phonetic ambiguity;acoustic signals;alignment algorithm;verified human transcriptions;ASR performance;error resolution algorithm;mis-transcribed word;phonetically similar word;ASR systems","","","","10","","24 Feb 2020","","","IEEE","IEEE Conferences"
"Deep Learning Methods for Image Decomposition of Cervical Cells","T. L. Mahyari; R. M. Dansereau","Carleton University,Department of Systems and Computer Engineering,Ottawa,Canada; Carleton University,Department of Systems and Computer Engineering,Ottawa,Canada","2020 28th European Signal Processing Conference (EUSIPCO)","18 Dec 2020","2021","","","1110","1114","One way to solve under-determined image decomposition is to use statistical information about the type of data to be decomposed. This information can be obtained by a deep learning where convolutional neural networks (CNN) are a subset recently used widely in image processing. In this paper, we have designed a two-stage CNN that takes cytology images of overlapped cervical cells and attempts to separate the cell images. In the first stage, we designed a CNN to segment overlapping cells. In the second stage, we designed a CNN that uses this segmentation and the original image to separate the regions. We implemented a CNN similar to U-Net for image segmentation and implemented a new network for the image separation. To train and test the proposed networks, we simulated 50000 cervical cell cytology images by overlaying individual images of real cervical cells using the Beer-Lambert law. Of these 50000 images, we used 49000 images for training and evaluated the method with 1000 test images. Results on these synthetic images give more than 97% segmentation accuracy and gives decomposition SSIM scores of more than 0.99 and PSNR score of more than 30 dB. Despite these positive results, the permutation problem that commonly effects signal separation occasionally occurred resulting in some cell structure mis-separation (for example, one cell given two nucleoli and the other given none). In addition, when the segmentation was poor from the first stage, the resulting separation was poor.","2076-1465","978-9-0827-9705-3","10.23919/Eusipco47968.2020.9287435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9287435","Machine learning;deep learning;image segmentation;image separation;translucent overlapped images","Deep learning;Training;Image segmentation;Source separation;Europe;Image decomposition;Convolutional neural networks","biomedical optical imaging;cellular biophysics;convolutional neural nets;image segmentation;learning (artificial intelligence);medical image processing","under-determined image decomposition;statistical information;convolutional neural networks;image processing;two-stage CNN;overlapped cervical cells;cell images;segment overlapping cells;image segmentation;image separation;decomposition SSIM scores;cell structure mis-separation;deep learning methods;cervical cell cytology images;Beer-Lambert law","","","","26","","18 Dec 2020","","","IEEE","IEEE Conferences"
"Multidimensional control using a mobile-phone based brain-muscle-computer interface","S. Vernon; S. S. Joshi","Electrical and Computer Engineering Graduate Group, University of California, Davis, USA 95616; Electrical and Computer Engineering Graduate Group, University of California, Davis, USA 95616","2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","1 Dec 2011","2011","","","5188","5194","Many well-known brain-computer interfaces measure signals at the brain, and then rely on the brain's ability to learn via operant conditioning in order to control objects in the environment. In our lab, we have been developing brain-muscle-computer interfaces, which measure signals at a single muscle and then rely on the brain's ability to learn neuromuscular skills via operant conditioning. Here, we report a new mobile-phone based brain-muscle-computer interface prototype for severely paralyzed persons, based on previous results from our group showing that humans may actively create specified power levels in two separate frequency bands of a single sEMG signal. Electromyographic activity on the surface of a single face muscle (Auricularis superior) is recorded with a standard electrode. This analog electrical signal is imported into an Android-based mobile phone. User-modulated power in two separate frequency band serves as two separate and simultaneous control channels for machine control. After signal processing, the Android phone sends commands to external devices via Bluetooth. Users are trained to use the device via biofeedback, with simple cursor-to-target activities on the phone screen.","1558-4615","978-1-4577-1589-1","10.1109/IEMBS.2011.6091284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6091284","human-machine interface;sEMG;mobile phone;translational research;BCI","Muscles;Sensors;Humans;Bluetooth;Band pass filters;Mobile handsets;Training","biomedical electrodes;Bluetooth;brain-computer interfaces;electromyography;medical signal processing;mobile computing;mobile handsets;multidimensional signal processing;operating systems (computers)","multidimensional control;mobile-phone based brain-muscle-computer interface;neuromuscular skills;operant conditioning;paralyzed persons;single sEMG signal;electromyographic activity;single face muscle;standard electrode;electrical signal;Android-based mobile phone;user-modulated power;machine control;signal processing;Bluetooth;biofeedback;simple cursor-to-target activity;phone screen;control channel","Adult;Biofeedback, Psychology;Brain;Cellular Phone;Electroencephalography;Electromyography;Equipment Design;Equipment Failure Analysis;Humans;Male;Muscle Contraction;Muscle, Skeletal;Muscular Atrophy, Spinal;Muscular Atrophy, Spinal;Therapy, Computer-Assisted;User-Computer Interface","1","","24","","1 Dec 2011","","","IEEE","IEEE Conferences"
"Sign language recognition using Microsoft Kinect","A. Agarwal; M. K. Thakur","Department of Computer Science and Engineering, Jaypee Institute of Information Technology, Noida, India - 201307; Department of Computer Science and Engineering, Jaypee Institute of Information Technology, Noida, India - 201307","2013 Sixth International Conference on Contemporary Computing (IC3)","26 Sep 2013","2013","","","181","185","In last decade lot of efforts had been made by research community to create sign language recognition system which provide a medium of communication for differently-abled people and their machine translations help others having trouble in understanding such sign languages. Computer vision and machine learning can be collectively applied to create such systems. In this paper, we present a sign language recognition system which makes use of depth images that were captured using a Microsoft Kinect® camera. Using computer vision algorithms, we develop a characteristic depth and motion profile for each sign language gesture. The feature matrix thus generated was trained using a multi-class SVM classifier and the final results were compared with existing techniques. The dataset used is of sign language gestures for the digits 0-9.","","978-1-4799-0192-0","10.1109/IC3.2013.6612186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612186","gesture recognition;machine learning;computer vision;support vector machine;kernel;sign language recognition","Gesture recognition;Assistive technology;Training;Support vector machines;Kernel;Accuracy;Feature extraction","computer vision;learning (artificial intelligence);pattern classification;sign language recognition;support vector machines","sign language recognition system;Microsoft Kinect camera;depth images;computer vision algorithms;characteristic depth;motion profile;sign language gesture;feature matrix;multiclass SVM classifier;machine learning","","43","","18","","26 Sep 2013","","","IEEE","IEEE Conferences"
"Machine Vision-Based Positioning and Inspection Using Expectation–Maximization Technique","D. Tsai; Y. Hsieh","Department of Industrial Engineering and Management, Yuan-Ze University, Taoyuan, Taiwan; Department of Industrial Engineering and Management, Yuan-Ze University, Taoyuan, Taiwan","IEEE Transactions on Instrumentation and Measurement","9 Oct 2017","2017","66","11","2858","2868","Precision positioning is very important for automatic assembly and inspection in the electronic manufacturing process. In this paper, we propose a fast image alignment method using the expectation-maximization (E-M) technique. The proposed algorithm is especially applied to positioning and defect inspection of printed circuit boards (PCBs). It can well handle deformed or incomplete object shapes with translation, rotation, and scale changes. The Canny edge detector is used to generate the edge maps of images. The E-step of the E-M procedure finds mutual edge points in both compared images by assigning weights to individual edge points. The mutual edge points give larger weights, while the foreign edge points in two images have smaller weights. The M-step then calculates the geometric transformation parameters using the weighted edge points in individual images. For an edge point in one image, a fast spiral search is proposed to find its corresponding edge point with the shortest distance in the other image. The spiral search is carried out by a predetermined lookup table, and no computation is involved in the search process. The weight of each edge point is inversely proportional to the neighboring distance. Experimental results indicate that the proposed E-M positioning method can achieve a translation error less than 1 pixel and a rotation error smaller than 1° for PCB positioning.","1557-9662","","10.1109/TIM.2017.2717284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968345","Defect inspection;image alignment;printed circuit boards (PCBs);quality measurement;vision-based measurement (VBM)","Image edge detection;Inspection;Liquid crystal displays;Spirals;Manufacturing;Image registration;Feature extraction","assembling;computer vision;edge detection;expectation-maximisation algorithm;inspection;printed circuit manufacture;printed circuits","image edge map;printed circuit board defect inspection;electronic manufacturing automatic assembly;electronic manufacturing process inspection;expectation-maximization technique;machine vision-based inspection;machine vision-based positioning;PCB positioning;E-M positioning method;Canny edge detector;fast image alignment method","","15","","46","Traditional","4 Jul 2017","","","IEEE","IEEE Journals"
"An automated 3D-atlas-based registration towards the anatomical segmentation of pulmonary pleural surface","K. Chaisaowong; M. Jiang","Department of Electrical and Computer Engineering, King Mongkut's University of Technology, North Bangkok Thailand; RWTH Aachen University, Germany","2018 International ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunications Engineering (ECTI-NCON)","11 Jun 2018","2018","","","85","88","A computer-assisted diagnosis system to automatically detect pleural thickening has been developed. The CAD system also carries out the assessment of each detected pleural thickening by specifying its size and location. Moreover, anatomical details i.e. part of pleura (pars costalis, medistinalis, diaphragmatica, spinalis), lung side and position will be automatically determined. For this purpose, a 3D atlas of lung surface has been manually constructed. This paper describes the overall registration chain of patient lung to the 3D atlas which consists of three sub-steps. First, a rigid registration aims a translational alignment of both 3D data. A distance map leads then to an affine registration in the second step. The last final step is a non-rigid registration to assure a better registration quality. After the alignment, each detected pleural thickening will be labeled to describe its anatomical characters which will yield a higher precision and reproducible assessment of pleural thickenings.","","978-1-5386-3552-0","10.1109/ECTI-NCON.2018.8378287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8378287","pleural thickening;thoracic spiral computed tomography;computer-assisted diagnosis;automatic image processing algorithms;3D atlas model;rigid registration;affine registration;non-rigid registration","Lung;Three-dimensional displays;Computed tomography;Image segmentation;Computer aided diagnosis;Surface treatment","computerised tomography;image registration;image segmentation;lung;medical image processing","anatomical characters;anatomical segmentation;pulmonary pleural surface;CAD system;pars costalis;lung side;lung surface;registration chain;patient lung;rigid registration;affine registration;nonrigid registration;registration quality;computer-assisted diagnosis system;pleural thickening;automated 3D-atlas-based registration;medistinalis;diaphragmatica;spinalis","","","","11","","11 Jun 2018","","","IEEE","IEEE Conferences"
"End-to-End Target Detection and Classification with Data Augmentation in SAR Images","M. Dong; Y. Cui; X. Jing; X. Liu; J. Li","Beijing University of Post and Telecommunications; Beijing University of Post and Telecommunications; Beijing University of Post and Telecommunications; Beijing Ulrapower Software Co., Ltd; Beijing Ulrapower Software Co., Ltd","2019 IEEE International Conference on Computational Electromagnetics (ICCEM)","29 Jul 2019","2019","","","1","3","While applying traditional algorithm to synthetic aperture radar automatic target recognition (SAR-ATR) is facing difficulties, deep learning-based end-to-end object detection algorithms are becoming better options due to the automatic feature extraction and availability of high-quality data. In this paper, both single-staged and two-staged end-to-end models are experimented. We proposed modified Faster R-CNN models and SSD models to address SAR-ATR. Data augmentation techniques including random flipping, multiplying, rotation, translation, and flipping are applied to MSTAR SAR dataset to solve problems related to limited training samples. Transfer learning of SSD models and Faster R-CNN models on COCO dataset are utilized. Both existing algorithms and proposed algorithms are tested in ten-class MSTAR dataset. Experimental results show that SSD-Inception with widened network and MobileNet-SSD with light weight structure perform with much faster speed and cheaper computational cost, hundreds of times faster than Faster R-CNNs. MobileNet-SSD is especially suitable for mobile devices with 0.028 second per batch*step. Faster R-CNN with ResNet-101 and Inception ResNet perform in slightly higher accuracy than SSDs, reaching 99.4% mAP. MobileNet-SSD and SSD-Inception reach 96.79% and 99.16% mAP respectively.","","978-1-5386-7111-5","10.1109/COMPEM.2019.8779096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8779096","automatic target detection;synthetic aperture radar;convolutional neural network;target recognition","Feature extraction;Object detection;Radar polarimetry;Synthetic aperture radar;Target recognition;Computational modeling;Proposals","convolutional neural nets;feature extraction;image classification;image recognition;learning (artificial intelligence);object detection;radar computing;radar detection;radar imaging;radar target recognition;synthetic aperture radar","MobileNet-SSD;SAR-ATR;automatic feature extraction;Faster R-CNN models;SSD models;data augmentation techniques;MSTAR SAR dataset;mobile devices;end-to-end target detection;SSD-Inception;automatic target recognition;deep learning;end-to-end object detection algorithms;synthetic aperture radar","","2","","10","","29 Jul 2019","","","IEEE","IEEE Conferences"
"Hand Gesture Controlled Drones: An Open Source Library","K. Natarajan; T. D. Nguyen; M. Mete","Dept. of Comput. Sci., Texas A&M Univ., Commerce, CA, USA; Dept. of Comput. & Inf. Sci., Fordham Univ., New York, NY, USA; Dept. of Comput. Sci., Texas A&M Univ., Commerce, CA, USA","2018 1st International Conference on Data Intelligence and Security (ICDIS)","28 May 2018","2018","","","168","175","Drones are conventionally controlled using joysticks, remote controllers, mobile applications, and embedded computers. A few significant issues with these approaches are that drone control is limited by the range of electromagnetic radiation and susceptible to interference noise. In this study we propose the use of hand gestures as a method to control drones. We investigate the use of computer vision methods to develop an intuitive way of agent-less communication between a drone and its operator. Computer vision-based methods rely on the ability of a drone's camera to capture surrounding images and use pattern recognition to translate images to meaningful and/or actionable information. The proposed framework involves a few key parts toward an ultimate action to be taken. They are: image segregation from the video streams of front camera, creating a robust and reliable image recognition based on segregated images, and finally conversion of classified gestures into actionable drone movement, such as takeoff, landing, hovering and so forth. A set of five gestures are studied in this work. Haar feature-based AdaBoost classifier[1] is employed for gesture recognition. We also envisage safety of the operator and drone's action calculating the distance based on computer vision for this task. A series of experiments are conducted to measure gesture recognition accuracies considering the major scene variabilities, illumination, background, and distance. Classification accuracies show that well-lit, clear background, and within 3 ft gestures are recognized correctly over 90%. Limitations of current framework and feasible solutions for better gesture recognition are discussed, too. The software library we developed, and hand gesture datasets are open-sourced at project website.","","978-1-5386-5762-1","10.1109/ICDIS.2018.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367759","drone;machine vision;image processing;machine learning","Drones;Cameras;Streaming media;Feature extraction;Training;Gesture recognition;Real-time systems","cameras;computer vision;gesture recognition;Haar transforms;image classification;learning (artificial intelligence);object detection;public domain software;remotely operated vehicles","computer vision methods;image segregation;robust image recognition;reliable image recognition;actionable drone movement;gesture recognition accuracies;hand gesture datasets;hand gesture controlled drones;open source;remote controllers;drone control;pattern recognition;Hand Gesture Controlled Drones;electromagnetic radiation;interference noise;video streams;Haar feature-based AdaBoost classifier;software library","","5","","39","","28 May 2018","","","IEEE","IEEE Conferences"
"Subject Recognition Based on Ground Reaction Force Measurements of Gait Signals","S. P. Moustakidis; J. B. Theocharis; G. Giakas","Dept. of Electr. & Comput. Eng., Aristotle Univ. of Thessaloniki, Thessaloniki; Dept. of Electr. & Comput. Eng., Aristotle Univ. of Thessaloniki, Thessaloniki; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","18 Nov 2008","2008","38","6","1476","1485","An effective subject recognition approach is designed in this paper, using ground reaction force (GRF) measurements of human gait. The method is a three-stage procedure: 1) The original GRF data are translated through wavelet packet (WP) transform in the time-frequency domain. Using a fuzzy-set-based criterion, we determine an optimal WP decomposition, involving feature subspaces with distinguishing gait characteristics. 2) A feature extraction scheme is employed next for wavelet feature ranking, according to discrimination power. 3) The classification task is accomplished by means of a kernel-based support vector machine. The design parameters of the classifier are tuned through a genetic algorithm to improve recognition rates. The method is evaluated on a database comprising GRF records obtained from 40 subjects. To account for the natural variability of human gait, the experimental setup is designed, allowing different walking speeds and loading conditions. Simulation results demonstrate that high recognition rates can be achieved with moderate number of features and for different training/testing settings. Finally, the performance of our approach is favorably compared with the one obtained using other traditional classification algorithms.","1941-0492","","10.1109/TSMCB.2008.927722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4637291","Feature selection;ground reaction forces (GRFs) of gait;human gait analysis;kernel-based support vector machine (SVM) classification;subject recognition;wavelet packet (WP) decomposition;Feature selection;ground reaction forces (GRFs) of gait;human gait analysis;kernel-based support vector machine (SVM) classification;subject recognition;wavelet packet (WP) decomposition","Force measurement;Humans;Anthropometry;Wavelet packets;Wavelet domain;Wavelet transforms;Time frequency analysis;Feature extraction;Support vector machines;Support vector machine classification","feature extraction;fuzzy set theory;gait analysis;genetic algorithms;image classification;support vector machines;time-frequency analysis;wavelet transforms","subject recognition;ground reaction force measurements;gait signals;wavelet packet transform;time-frequency domain;fuzzy-set-based criterion;optimal WP decomposition;feature extraction;wavelet feature ranking;kernel-based support vector machine;genetic algorithm","Algorithms;Artificial Intelligence;Biometry;Computer Simulation;Gait;Humans;Models, Biological;Pattern Recognition, Automated;Stress, Mechanical","52","","22","","3 Oct 2008","","","IEEE","IEEE Journals"
"Improving face gender classification by adding deliberately misaligned faces to the training data","M. Mayo; E. Zhang","Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; Dept. of Computer Science, University of Waikato, Hamilton, New Zealand","2008 23rd International Conference Image and Vision Computing New Zealand","23 Jan 2009","2008","","","1","5","A novel method of face gender classifier construction is proposed and evaluated. Previously, researchers have assumed that a computationally expensive face alignment step (in which the face image is transformed so that facial landmarks such as the eyes, nose, chin, etc, are in uniform locations in the image) is required in order to maximize the accuracy of predictions on new face images. We, however, argue that this step is not necessary, and that machine learning classifiers can be made robust to face misalignments by automatically expanding the training data with examples of faces that have been deliberately misaligned (for example, translated or rotated). To test our hypothesis, we evaluate this automatic training dataset expansion method with two types of image classifier, the first based on weak features such as Local Binary Pattern histograms, and the second based on SIFT keypoints. Using a benchmark face gender classification dataset recently proposed in the literature, we obtain a state-of-the-art accuracy of 92.5%, thus validating our approach.","2151-2205","978-1-4244-3780-1","10.1109/IVCNZ.2008.4762066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4762066","Gender classification;face detection;face alignment;face classification;Spatial Pyramid;Local Binary Pattern;SIFT keypoints;Support Vector Machines;image classification;machine learning","Training data;Face detection;Robustness;Automatic testing;Support vector machines;Support vector machine classification;Eyes;Nose;Machine learning;Histograms","face recognition;image classification;learning (artificial intelligence)","face gender classification;deliberately misaligned faces;machine learning classifiers;automatic training dataset expansion method;image classifier;local binary pattern;SIFT keypoints","","10","","14","","23 Jan 2009","","","IEEE","IEEE Conferences"
"Evaluating the effectiveness of time-domain features for motor imagery movements using SVM","A. Khorshidtalab; M. J. E. Salami; M. Hamedi","Department of Mechatronics Engineering, International Islamic University Malaysia, Gombak, Malaysia; Department of Mechatronics Engineering, International Islamic University Malaysia, Gombak, Malaysia; Faculty of Biomedical and Health Science Engineering, Universiti Teknologi Malaysia, Johor Bahru, Malaysia","2012 International Conference on Computer and Communication Engineering (ICCCE)","20 Aug 2012","2012","","","909","913","Motor imagery electroencephalogram signals are the only bio-signals that enable locked-in patients, who have lost control over every motor output, to communicate with and control their surroundings. Brain Machine Interface is collaboration between a human and machines, which translates brain waves to desired, understandable commands for a machine. Classification of motor imagery tasks for BMIs is the crucial part. Classification accuracy not only depends on how accurate and robust the classifier is; it is also about data. For well separated data, classifiers such as kernel SVM can handle classification and deliver acceptable results. If a feature provides large interclass difference for different classes, immunity to random noise and chaotic behavior of EEG signal is rationally conformed, which means the applied feature is suitable for classifying EEG signals. In this work, in order to have less computational complexity, time-domain algorithms are employed to motor imagery signals. Extracted features are: Mean Absolute Value, Maximum peak value, Simple Square Integral, Willison Amplitude, and Waveform Length. Support Vector Machine with polynomial kernel is applied for classification of four different classes of data. The obtained results show that these features have acceptable, distinct values for different these four motor imagery tasks. Maximum classification accuracy belongs to contribution of Willison amplitude as feature and SVM as classifier, with 95.1 percentages accuracy. Where, the lowest is the contribution of Waveform Length and SVM with 31.67 percentages classification accuracy.","","978-1-4673-0479-5","10.1109/ICCCE.2012.6271348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271348","Brain-machine Interface;Electroencephalogram;Feature extraction;Motor imagery;Support Vector Machine","Feature extraction;Electroencephalography;Support vector machines;Tongue;Accuracy;Time domain analysis;Classification algorithms","electric motors;electroencephalography;feature extraction;medical signal processing;support vector machines;time-domain analysis","motor imagery movements;SVM;motor imagery electroencephalogram signals;biosignals;brain machine interface;BMI;classifier;EEG signal;computational complexity;time-domain algorithms;mean absolute value;maximum peak value;simple square integral;feature extraction;Willison amplitude;waveform length;support vector machine;polynomial kernel","","4","","16","","20 Aug 2012","","","IEEE","IEEE Conferences"
"Rock beats Scissor: SVM based gesture recognition with data gloves","P. Achenbach; P. N. Müller; T. A. Wach; T. Tregel; S. Göbel","Multimedia Communications Lab, Technical University of Darmstadt,Darmstadt,Germany,64283; Multimedia Communications Lab, Technical University of Darmstadt,Darmstadt,Germany,64283; Multimedia Communications Lab, Technical University of Darmstadt,Darmstadt,Germany,64283; Multimedia Communications Lab, Technical University of Darmstadt,Darmstadt,Germany,64283; Multimedia Communications Lab, Technical University of Darmstadt,Darmstadt,Germany,64283","2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)","24 May 2021","2021","","","617","622","Hand gestures play an important role in human communication, particularly when auditory communication is limited. Akin to speech recognition, hand gesture recognition can therefore be a useful tool to facilitate communication and for more immersive computer interaction. In this paper, we examine the mobile recognition of hand gestures using data recorded with sensor gloves. We design a system based on Support Vector Machines (SVM), capable of recognizing 5 different hand gestures. In an experiment with 11 participants, we determine applicable hyperparameters based on performance on the training set which translates into 100% classification accuracy on the test set. In an additional practical experiment with 9 participants, our system achieves up to 98% in a personalized and up to 87.5% in a generalized model setting.","","978-1-6654-0424-2","10.1109/PerComWorkshops51409.2021.9430962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9430962","gesture recognition;wearable;machine learning;data glove;support vector machine;rock-paper-scissor","Support vector machines;Training;Pervasive computing;Conferences;Gesture recognition;Speech recognition;Data gloves","data gloves;gesture recognition;human computer interaction;speech recognition;support vector machines","SVM based gesture recognition;data gloves;human communication;auditory communication;speech recognition;hand gesture recognition;immersive computer interaction;mobile recognition;sensor gloves;Support Vector Machines;5 different hand gestures","","","","16","","24 May 2021","","","IEEE","IEEE Conferences"
"Road Navigation System Using Automatic Speech Recognition (ASR) And Natural Language Processing (NLP)","P. Withanage; T. Liyanage; N. Deeyakaduwe; E. Dias; S. Thelijjagoda","Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Computing, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Faculty of Business, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka","2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC)","31 Jan 2019","2018","","","1","6","In a highly evolving technical era, Voice-based Navigation Systems play a major role to bridge the gap between human and machine. To overcome the difficulty in taking and understanding user's voice commands, simulating the natural language, process the route with user's turn by turn directions while mentioning key entities like street names, landmarks, point of interests, junctions and map the route in an interactive interface, we propose a user-centric roadmap navigation mobile application called “Direct Me”. The approach of generating the user preferred route, system will first convert the audio streams into text through Automatic Speech Recognizer (ASR) using Pocket Sphinx Library, followed by Natural Language Processing (NLP) by utilizing Stanford CoreNLP Framework to retrieve the navigation-associated information and process the route in the Map using Google Map API upon the user request. This system is used to provide an efficient approach to translate natural language directions to a machine-understandable format and will benefit the development of voice-based navigation-oriented humanmachine interface.","2572-7621","978-1-5386-5051-6","10.1109/R10-HTC.2018.8629859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629859","Speech Recognition;Natural Language Processing;Road Navigation;Route Mapping;Navigational Information Extraction","Navigation;Speech recognition;Natural language processing;Automobiles;Dictionaries;Google","application program interfaces;human computer interaction;information retrieval;mobile computing;natural language processing;speech recognition","user-centric roadmap navigation mobile application;ASR;NLP;navigation-associated information;Google Map API;natural language directions;machine-understandable format;natural language processing;automatic speech recognition;road navigation system;voice-based navigation systems;voice-based navigation-oriented human machine interface;information retrieval","","3","","11","","31 Jan 2019","","","IEEE","IEEE Conferences"
"Automatic 3D-2D image registration using partial digitally reconstructed radiographs along projected anatomic contours","X. Chen; M. R. Varley; L. Shark; G. S. Shentall; M. C. Kirby","University of Central Lancashire, Preston, U.K.; University of Central Lancashire, Preston, U.K.; University of Central Lancashire, Preston, U.K.; Rosemere Cancer Centre, Royal Preston Hospital, U.K.; Rosemere Cancer Centre, Royal Preston Hospital, U.K.","International Conference on Medical Information Visualisation - BioMedical Visualisation (MediVis 2007)","16 Jul 2007","2007","","","3","8","The paper presents a new intensity-based 3D-2D image registration algorithm for automatic pretreatment validation in radiotherapy. The novel aspects of the algorithm includes a hybrid cost function developed based on partial digitally reconstructed radiographs (DRRs) generated along projected anatomic contours and level set for similarity measurement, and a fast search method developed based on parabola fitting and sensitivity based search order. Using CT and orthogonal X-ray images from a skull phantom, the proposed algorithm is compared with the conventional ray-casting full DRR based registration method. Not only is the algorithm shown to be computationally more efficient with registration time being reduced by a factor of 8, but also the algorithm is shown to offer 50% higher capture range allowing the initial patient displacement up to 15 mm (measured by mean target registration error) and high registration accuracy with average errors of 0.53plusmn 0.12 mm for translation and 0.61degplusmn0.29deg for rotation within the capture range.","","0-7695-2904-6","10.1109/MEDIVIS.2007.7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4272103","","Image registration;Image reconstruction;Radiography;Cost function;Hybrid power systems;Level set;Search methods;Computed tomography;X-ray imaging;Skull","computerised tomography;image reconstruction;image registration;medical image processing;radiation therapy;X-ray imaging","3D-2D image registration;digitally reconstructed radiographs;anatomic contours;automatic pretreatment validation;radiotherapy;search method;parabola fitting;sensitivity based search order;CT images;orthogonal X-ray images;skull phantom;mean target registration error","","2","3","8","","16 Jul 2007","","","IEEE","IEEE Conferences"
"Automatic Markerless Registration and Tracking of the Bone for Computer-Assisted Orthopaedic Surgery","H. Liu; F. R. Y. Baena","Mechatronics in Medicine Laboratory, Imperial College London, London, U.K.; Mechatronics in Medicine Laboratory, Imperial College London, London, U.K.","IEEE Access","9 Mar 2020","2020","8","","42010","42020","To achieve a simple and less invasive registration procedure in computer-assisted orthopaedic surgery, we propose an automatic, markerless registration and tracking method based on depth imaging and deep learning. A depth camera is used to continuously capture RGB and depth images of the exposed bone during surgery, and deep neural networks are trained to first localise the surgical target using the RGB image, then segment the target area of the corresponding depth image, from which the surface geometry of the target bone can be extracted. The extracted surface is then compared to a pre-operative model of the same bone for registration. This process can be performed dynamically during the procedure at a rate of 5-6 Hz, without any need for surgeon intervention or invasive optical markers. Ex vivo registration experiments were performed on a cadaveric knee, and accuracy measurements against an optically tracked ground truth resulted in a mean translational error of 2.74 mm and a mean rotational error of 6.66°. Our results are the first to describe a promising new way to achieve automatic markerless registration and tracking in computer-assisted orthopaedic surgery, demonstrating that truly seamless registration and tracking of the limb is within reach. Our method reduces invasiveness by removing the need for percutaneous markers. The surgeon is also exempted from inserting markers and collecting registration points manually, which contributes to a more efficient surgical workflow and shorter procedure time in the operating room.","2169-3536","","10.1109/ACCESS.2020.2977072","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018195","Computer-assisted orthopaedic surgery;deep learning;depth imaging;markerless registration","Surgery;Bones;Image segmentation;Cameras;Optical imaging;Deep learning","biomedical optical imaging;bone;image registration;learning (artificial intelligence);medical image processing;optical tracking;orthopaedics;surgery","automatic markerless registration;computer-assisted orthopaedic surgery;invasive registration procedure;tracking method;depth imaging;depth camera;depth images;exposed bone;deep neural networks;surgical target;RGB image;deep learning;optically tracked ground truth;ex vivo registration;invasive optical markers;size 2.74 mm;frequency 5.0 Hz to 6.0 Hz","","2","","33","CCBY","28 Feb 2020","","","IEEE","IEEE Journals"
"Automatic Sublingual Vein Feature Extraction System","H. Lin; Y. Chen; N. Damdinsuren; T. Tan; T. Liu; J. Y. Chiang","NA; NA; Manba Datsan Clinic & Training Centre for Mongolian Traditional Med., Ulaanbaatar, Mongolia; Dept. of Electr. Eng., Nat. Taipei Univ. of Technol., Taipei, Taiwan; NA; Dept. of Comput. Sci. & Eng., Nat. Sun Yat-sen Univ., Kaohsiung, Taiwan","2014 International Conference on Medical Biometrics","30 Jun 2014","2014","","","55","62","The quintessence of the diagnosis in traditional Chinese medicine is syndrome differentiation and treatment. Syndrome differentiation consists of four methods: observing, hearing as well as smelling, asking, and touching. The examination of the observing is the most important procedure in the method of ""tongue."" In recent years, numerous medical studies have identified the close relations between sublingual veins and human organs. Therefore, sublingual pathological symptoms, as well as demographical information of patients, imply pathological changes in the organs, and early diagnosis is beneficial for early treatment. However, the diagnosis of sublingual pathological symptoms is usually influenced by the doctor's subjective interpretation, experience, and environmental factors. The results can easily be limited by subjective factors such as knowledge, experience, mentality, diagnostic techniques, color perception and interpretation. Different doctors may make different judgments on the same tongue, presenting less than desirable repeatability. Therefore, assisting doctors' diagnoses with scientific methods and standardizing the differentiating process to obtain reliable diagnoses and enhance the clinical applicability of Chinese medicine is an important issue. In its wake, this study aims to construct an Automatic Sublingual Vein Feature Extraction System based on image processing technologies to allow objective and quantified computer readings. The extraction of sublingual vein features mainly captures the back of the tongue and extract the sublingual vein area for feature expression analysis. Firstly, the patient's back of the tongue is photographed and color-graded to compensate for color distortion, and then the tongue-back area is extracted. This study extracts tongue-back imagery by analyzing the RGB color expression of the back of the tongue, lips, teeth and skin, translating it into the HSI color space easily perceived by the human eye, along with skin area removal, rectangle detection, teeth area removal, black area removal and control point detection. The captured tongue-back image goes through histogram equalization and hue shift to enhance color contrast. Sublingual veins are extracted through analyzing RGB color component shift, hues, saturation and brightness. Then the sublingual vein color information and positioning are used to differentiate hues, lengths and branches. Thinning analysis is used to determine the presence of varicose veins. At the same time, the surrounding features of sublingual veins, such as columnar vein, bubbly vein, petechiae and bloodshot, are extracted. The information regarding features and lingual vein conditions are integrated and analyzed for doctors' clinical reference. This study utilizes 199 lingual images for statistic testing and three lingual diagnostic experts in Chinese medicine for lingual reading. The accuracy for the extractions are: tongue back 86%, sublingual vein 80%, varicose veins 90%, branches 87%, and the accuracy rates for columnar veins and bubbly veins are 87%, 88% and 73% respectively.","","978-1-4799-4013-4","10.1109/ICMB.2014.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6845825","tongue diagnosis;sublingual veins;tongue-back imagery;histogram equalization","Veins;Tongue;Image color analysis;Feature extraction;Skin;Medical diagnostic imaging","biological organs;biomedical optical imaging;blood vessels;brightness;feature extraction;image colour analysis;image thinning;medical image processing;skin","automatic sublingual vein feature extraction system;traditional Chinese medicine;syndrome differentiation;human organs;sublingual pathological symptoms;patient demographical information;patient diagnosis;patient treatment;image processing;feature expression analysis;color distortion;tongue-back imagery;RGB color expression;lips;HSI color space;skin area removal;teeth area removal;rectangle detection;black area removal;control point detection;histogram equalization;hue shift;color contrast enhancement;RGB color component shift;saturation;brightness;sublingual vein color information;thinning analysis;varicose veins;columnar vein;bubbly vein;petechiae;bloodshot;lingual reading","","","","17","","30 Jun 2014","","","IEEE","IEEE Conferences"
"Correction of Automatic Speech Recognition with Transformer Sequence-To-Sequence Model","O. Hrinchuk; M. Popova; B. Ginsburg","Moscow Institute of Physics and Technology,Moscow,Russia; NVIDIA,Santa Clara,CA,USA; NVIDIA,Santa Clara,CA,USA","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","7074","7078","In this work, we introduce a simple yet efficient post-processing model for automatic speech recognition. Our model has Transformer-based encoder-decoder architecture which ""translates"" acoustic model output into grammatically and semantically correct text. We investigate different strategies for regularizing and optimizing the model and show that extensive data augmentation and the initialization with pretrained weights are required to achieve good performance. On the LibriSpeech benchmark, our method demonstrates significant improvement in word error rate over the baseline acoustic model with greedy decoding, especially on much noisier dev-other and test-other portions of the evaluation dataset. Our model also outperforms baseline with 6-gram language model re-scoring and approaches the performance of re-scoring with Transformer-XL neural language model.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053051","speech recognition;spelling correction;pre-trained language models","Training;Analytical models;Data models;Acoustics;Decoding;Speech processing;Automatic speech recognition","decoding;natural language processing;speech recognition","automatic speech recognition;acoustic model output;data augmentation;baseline acoustic model;greedy decoding;6-gram language model;Transformer-XL neural language model;transformer sequence-to-sequence model;post-processing model;transformer-based encoder-decoder architecture","","5","","26","","9 Apr 2020","","","IEEE","IEEE Conferences"
"A method for automatic detection and classification of stroke from brain CT images","M. Chawla; S. Sharma; J. Sivaswamy; L. T. Kishore","Centre for Visual Information Technology, International Institute of Information Technology, Hyderabad, India; Centre for Visual Information Technology, International Institute of Information Technology, Hyderabad, India; Centre for Visual Information Technology, International Institute of Information Technology, Hyderabad, India; Institute of Medical Sciences, CARE Hospital, Hyderabad, India","2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","13 Nov 2009","2009","","","3581","3584","Computed tomographic (CT) images are widely used in the diagnosis of stroke. In this paper, we present an automated method to detect and classify an abnormality into acute infarct, chronic infarct and hemorrhage at the slice level of non-contrast CT images. The proposed method consists of three main steps: image enhancement, detection of mid-line symmetry and classification of abnormal slices. A windowing operation is performed on the intensity distribution to enhance the region of interest. Domain knowledge about the anatomical structure of the skull and the brain is used to detect abnormalities in a rotation- and translation-invariant manner. A two-level classification scheme is used to detect abnormalities using features derived in the intensity and the wavelet domain. The proposed method has been evaluated on a dataset of 15 patients (347 image slices). The method gives 90% accuracy and 100% recall in detecting abnormality at patient level; and achieves an average precision of 91% and recall of 90% at the slice level.","1558-4615","978-1-4244-3296-7","10.1109/IEMBS.2009.5335289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5335289","","Computed tomography;Hemorrhaging;Magnetic resonance imaging;Biomedical imaging;Blood vessels;Shape;Brain;Information technology;Histograms;USA Councils","brain;computerised tomography;image classification;medical image processing","automatic stroke detection;automatic stroke classification;brain CT images;computed tomography;image enhancement;anatomical structure;skull","Artificial Intelligence;Brain;False Positive Reactions;Fuzzy Logic;Humans;Image Enhancement;Image Processing, Computer-Assisted;Imaging, Three-Dimensional;Models, Theoretical;Neural Networks (Computer);Pattern Recognition, Automated;Radiographic Image Interpretation, Computer-Assisted;Reproducibility of Results;Stroke;Stroke;Tomography, X-Ray Computed","45","2","12","","13 Nov 2009","","","IEEE","IEEE Conferences"
"Toward the automatic generation of mathematical morphology procedures using predicate logic","H. Joo; R. M. Haralick; L. G. Shapiro","Dept. of Electr. Eng., Washington Univ., Seattle, WA, USA; Dept. of Electr. Eng., Washington Univ., Seattle, WA, USA; Dept. of Electr. Eng., Washington Univ., Seattle, WA, USA","[1990] Proceedings Third International Conference on Computer Vision","6 Aug 2002","1990","","","156","165","A discussion is presented of the design of a system that can input a vision task specification and use its knowledge of the operations of mathematical morphology to automatically construct a procedure that can execute the task. To do this, the authors develop a predicate calculus representation to describe the essence of the states of all the images that are created during the execution of the morphological procedure and the states of the relationships among them. The authors translate the English descriptions of morphological procedures into predicate logic. In so doing they gain an understanding of the goal of each procedure and the exact conditions under which a procedure achieves its goal. With this knowledge of the operations of mathematical morphology represented in predicate logic, a search procedure can be used to automatically produce vision procedures.<>","","0-8186-2057-9","10.1109/ICCV.1990.139514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=139514","","Morphology;Automatic logic units;Morphological operations;Logic design;Intelligent systems;Laboratories;Computed tomography;Calculus;Machine vision;Image processing","computer vision;computerised picture processing;formal logic","automatic generation;mathematical morphology;predicate logic;vision task specification;predicate calculus;English descriptions;morphological procedures;search procedure","","8","","14","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic Classification of Spider Images in Natural Background","Y. Jian; S. Peng; L. Zhenpeng; Z. Yu; Z. Chenggui; Y. Zizhong","College of Mathematics and Computer Science, Dali University, Dali, Yunnan Province, 671003, P.R.China; College of Mathematics and Computer Science, Dali University, Dali, Yunnan Province, 671003, P.R.China; College of Mathematics and Computer Science, Dali University, Dali, Yunnan Province, 671003, P.R.China; Provincial Key Lab of Entomological Bio. R&D, Dali University, Dali, Yunnan Province, 671003, P.R.China; Provincial Key Lab of Entomological Bio. R&D, Dali University, Dali, Yunnan Province, 671003, P.R.China; Provincial Key Lab of Entomological Bio. R&D, Dali University, Dali, Yunnan Province, 671003, P.R.China","2019 IEEE 4th International Conference on Signal and Image Processing (ICSIP)","17 Oct 2019","2019","","","158","164","Spiders are the most abundant predatory natural enemies in terrestrial ecosystems. As an important natural enemy of many agricultural and forestry pests, spiders play a very significant role in the biological control of pests. In order to make rational use of spider resources, it is necessary to observe and study the population characteristics of it. Direct observation method is time-consuming and laborious. If we can take the videos or images of spiders by surveillance cameras, and then use computer vision technology to identify and classify automatically, the efficiency of image data acquisition and pest biological control will be greatly improved. Motivated by this, we studied the classification and recognition of spider images in natural background obtained by common surveillance equipment. However, the images of some species of spiders in natural background are difficult to be collected, and the inadequate clarity and contrast of the subjects in images will also affect the recognition accuracy. So, firstly, histogram equalization was used to enhance the contrast of the image; the dataset of spider images was expanded by image rotation, reflection, flipping, zooming, translating and increasing the pixel noise appropriately, and so on; the contour detection was carried out for assistant recognition. Secondly, taken the deep convolutional neural networks (CNN) as our basic framework, two automatic recognition models of spider images, that is the 8-layer deep CNN model and the transfer learning model based on Inception-v3, were constructed. After that, the two models were trained, evaluated and compared under a dataset with 4478 pre-processed images. The experimental results show that the first model has a limited effect on image feature extraction of spiders in natural background, while the second model based on transfer learning can achieve better recognition accuracy when combining image contour features as an auxiliary input. In the second model, the accuracy of training set and testing set can reach more than 90%, and the recognition speed can be controlled within 1 second, which meets the practical requirements of automatic classification and recognition of spider images in natural background.","","978-1-7281-3660-8","10.1109/SIPROCESS.2019.8868601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8868601","classification of spider images;image preprocessing;deep convolutional neural networks;transfer learning;contour detection","Image recognition;Feature extraction;Insects;Biological system modeling;Target recognition;Training;Sociology","cameras;computer vision;convolutional neural nets;data acquisition;feature extraction;forestry;image classification;learning (artificial intelligence);pest control;video surveillance","automatic classification;spider images;natural background;spider resources;image data acquisition;pest biological control;image feature extraction;image contour features;predatory natural enemies;forestry pests;agricultural pests;terrestrial ecosystems;surveillance cameras;computer vision technology;deep convolutional neural networks;deep CNN model;transfer learning model;Inception-v3","","","","27","","17 Oct 2019","","","IEEE","IEEE Conferences"
"Automatic tracking measurement system on human lumbar vertebral motion","Yingyao Zhang; Xiaobo Xie; Hongyan Cui; Yong Hu; Fuge Sui; Lifeng Zhao; Dongjun Wang","Institute of Biomedical Engineering, Perking Union Medical College & Chinese Academy of Medical Sciences, Tianjin 300192, China; Institute of Biomedical Engineering, Perking Union Medical College & Chinese Academy of Medical Sciences, Tianjin 300192, China; Institute of Biomedical Engineering, Perking Union Medical College & Chinese Academy of Medical Sciences, Tianjin 300192, China; Institute of Biomedical Engineering, Perking Union Medical College & Chinese Academy of Medical Sciences, Tianjin 300192, China; Dept. of Orthopaedic Surgery, Longnan Hospital, Daqing, Helongjiang, 163453 China; Dept. of Orthopaedic Surgery, Longnan Hospital, Daqing, Helongjiang, 163453 China; Dept. of Orthopaedic Surgery, Longnan Hospital, Daqing, Helongjiang, 163453 China","2009 IEEE International Conference on Virtual Environments, Human-Computer Interfaces and Measurements Systems","5 Jun 2009","2009","","","43","46","An automated tracking system was developed to measure the flexion-extension motion of lumbar vertebrae using the algorithm of Sequential Important Resampling Particle Filter. In vitro validity was performed in 10 samples under digitized video fluoroscopy. Each sample was taken 2 full flexion-extension cycles under a special guide device in fixed speed, while the calibration model of lumbar vertebrae was fixed on a metal base. The trajectories are recorded by real-time depiction of the vertebral body with rigid fixation pens when the digitized video fluoroscopy collection is finished. A special system that can perform automatic tracking on the motion of lumbar vertebrae was developed. Reliability of the measurement was evaluated by a saw bone model experiment. In comparing with real-time motion recording, the correlation coefficient of test-retest and the accuracy were calculated, which showed satisfactory stability and consistency. The results suggested this new developed automatic tracking system could be a reliable tool to monitor the dynamic lumbar motion; to analyze the translation and rotational angle of vertebral from video-fluoroscopic images automatically and accurately. The proposed system may have a potential value in the evaluation of spinal medical application.","1944-9410","978-1-4244-3808-2","10.1109/VECIMS.2009.5068863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5068863","component;particle filter;sequential important resampling;spine;vertebral body;auto tracking","Motion measurement;Anthropometry;Humans;Spine;Particle tracking;Particle measurements;Particle filters;In vitro;Calibration;Bones","diagnostic radiography;image sampling;medical image processing;particle filtering (numerical methods);video signal processing","automatic tracking measurement system;human lumbar vertebral motion;flexion-extension motion;sequential particle filter;important resampling particle filter;digitized video fluoroscopy;rigid fixation pens;lumbar vertebrae motion;video-fluoroscopic images","","1","","27","","5 Jun 2009","","","IEEE","IEEE Conferences"
"Retinal image automatic registration based on local bifurcation structure","K. Zhang; E. Zhang; J. Li; G. Chen","Key Laboratory of OptoElectonic Science and Technology for Medicine, Ministry of Education, Fujian Normal University, Fuzhou, 350007, China; Key Laboratory of OptoElectonic Science and Technology for Medicine, Ministry of Education, Fujian Normal University, Fuzhou, 350007, China; Key Laboratory of OptoElectonic Science and Technology for Medicine, Ministry of Education, Fujian Normal University, Fuzhou, 350007, China; Key Laboratory of OptoElectonic Science and Technology for Medicine, Ministry of Education, Fujian Normal University, Fuzhou, 350007, China","2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","16 Feb 2017","2016","","","1418","1422","Retinal images can be used for helping ophthalmologist to diagnose the eye diseases, and glaucoma, hypertension, coronary heart diseases and so on, so it's very important. The most important content of this article is a new structural feature for retinal registration. There are many methods of retinal image registration; some are also used by this time. Lots of methods used the main bifurcation points and angles. This method is well and effective, but there are some problems, for example, the angles about bifurcation maybe repeat, so in two images, the bifurcation registration may be failed. On the basis of this, in this paper we proposed a structure-matching registration method. The bifurcation structures are consists of a main bifurcation point and its three connected branches. The each numerical value of bifurcation is composed of the bifurcation angles and its three lengths by normalized. On the other hand the bifurcation structure will be invariant against rotation, translation and scaling. By this method, we can reduce the matching error caused by repeated. This method could be used alone, also could be used with other methods.","","978-1-5090-3710-0","10.1109/CISP-BMEI.2016.7852939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852939","retinal registration;image segmentation;retina image;local bifurcation structure","Bifurcation;Retina;Image registration;Feature extraction;Diseases;Biomedical imaging;Periodic structures","diseases;eye;image matching;image registration;medical image processing;patient diagnosis;retinal recognition","retinal image automatic registration;local bifurcation structure;eye diseases;glaucoma;hypertension;coronary heart diseases;structure-matching registration","","","","13","","16 Feb 2017","","","IEEE","IEEE Conferences"
"Automatic crown surface reconstruction using tooth statistical model for dental prosthesis planning","M. R. Soltaninejad; R. A. Zoroofi; G. Shirani","Control and Intelligent Processing Center of Excellence, School of ECE, University of Tehran, Iran; Control and Intelligent Processing Center of Excellence, School of ECE, University of Tehran, Iran; Oral and Maxillofacial Surgery Department, Faculty of Dentistry, Tehran University of Medical Science, Iran","2012 19th Iranian Conference of Biomedical Engineering (ICBME)","27 May 2013","2012","","","218","222","In modern dentistry, computerized procedures are widely used for simulating the operation and treatment processes. Computed tomography (CT) images provide dentists with useful information in maxillofacial and dental implantation surgeries. In the case of implantation, the size and shape of prosthesis is extracted from the geometrical features of other teeth, in dental cast. In this paper, we intend to reconstruct the shape of missing tooth using only dental CT images. To meet this goal, an atlas of the desired tooth must be created from a set of training shapes beforehand. Statistical shape models (SSM) provide good information of three-dimensional (3D) structure of teeth that can be used for shape reconstruction. We use an automatic algorithm to generate the SSM for each individual tooth from intact datasets. The proposed method for reconstruction of the missing tooth from SSM is mainly comprised of three parts: initial translation of atlas, rigid locating and surface deformation with contour matching. Edentulous region is automatically determined using panorex line. The atlas is then registered on the region rigidly. To establish compatibility of the tooth shape with other adjacent teeth, we introduce a contour matching process. During the process, the surface of crown is deformed so that it is fitted on the estimated contours. The evaluation of the system is performed for two main parts: analyzing the generated SSMs for teeth and evaluation of reconstructed shape. In case of SSM, the results show that the reconstruction error increases in teeth with more complex geometrical shape. Therefore, the number of required training shapes to reach desired statistical model increases. The tooth shapes estimation results show that they are very similar o real ones. For more complex shapes, the results show acceptability of the similarity measure between estimated and real crown surfaces.","","978-1-4673-3130-2","10.1109/ICBME.2012.6519684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519684","statistical shape models;shape reconstruction;image registration;3D visualization;dental CT scan","","computerised tomography;dentistry;image reconstruction;medical image processing;orthotics;prosthetics","automatic crown surface reconstruction;tooth statistical model;dental prosthesis planning;dentistry;computed tomography;maxillofacial implantation surgery;dental implantation surgery;prosthesis size;prosthesis shape;dental cast;statistical shape model;3D teeth structure;shape reconstruction;rigid locating;surface deformation;contour matching;edentulous region;panorex line;similarity measure","","","","12","","27 May 2013","","","IEEE","IEEE Conferences"
"Automatic initialization and dynamic tracking of surgical suture threads","R. C. Jackson; R. Yuan; D. -L. Chow; W. Newman; M. C. Çavuşoğlu","Department of Electrical Engineering and Computer Science (EECS) at Case Western Reserve University in Cleveland, OH, USA; Department of Electrical Engineering and Computer Science (EECS) at Case Western Reserve University in Cleveland, OH, USA; Department of Electrical Engineering and Computer Science (EECS) at Case Western Reserve University in Cleveland, OH, USA; Department of Electrical Engineering and Computer Science (EECS) at Case Western Reserve University in Cleveland, OH, USA; Department of Electrical Engineering and Computer Science (EECS) at Case Western Reserve University in Cleveland, OH, USA","2015 IEEE International Conference on Robotics and Automation (ICRA)","2 Jul 2015","2015","","","4710","4716","In order to realize many of the potential benefits associated with robotically assisted minimally invasive surgery, the robot must be more than a remote controlled device. Currently using a surgical robot can be challenging, fatiguing, and time consuming. Teaching the robot to actively assist surgical tasks, such as suturing, has the potential to vastly improve both patient outlook and the surgeon's efficiency. One obstacle to completing surgical sutures autonomously is the difficulty in tracking surgical suture threads. This paper proposes an algorithm which uses a Non-Uniform Rational B-Spline (NURBS) curve to model a suture thread. The NURBS model is initialized from a single selected point located on the thread. The NURBS curve is optimized by minimizing the image match energy between the projected stereo NURBS image and the segmented thread image. The algorithm is able to accurately track a suture thread as it translates, deforms, and changes length in real-time.","1050-4729","978-1-4799-6923-4","10.1109/ICRA.2015.7139853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139853","","Splines (mathematics);Surface topography;Surface reconstruction;Image segmentation;Yarn;Robots;Tracking","curve fitting;image matching;image segmentation;medical robotics;object tracking;robot vision;splines (mathematics);surgery","surgical suture thread;robotically assisted minimally invasive surgery;surgical robot;patient outlook;surgeon efficiency;NURBS curve;non-uniform rational B-spline;image match;stereo NURBS image;segmented thread image","","9","","16","","2 Jul 2015","","","IEEE","IEEE Conferences"
"Automatic Human Movement Assessment With Switching Linear Dynamic System: Motion Segmentation and Motor Performance","R. de Souza Baptista; A. P. L. Bó; M. Hayashibe","Automation and Robotics Lab, LARA, Universidade de Brasília, Brasília, Brazil; Automation and Robotics Lab, LARA, Universidade de Brasília, Brasília, Brazil; INRIA CAMIN Team and LIRMM, CNRS/University of Montpellier, Montpellier, France","IEEE Transactions on Neural Systems and Rehabilitation Engineering","20 Jun 2017","2017","25","6","628","640","Performance assessment of human movement is critical in diagnosis and motor-control rehabilitation. Recent developments in portable sensor technology enable clinicians to measure spatiotemporal aspects to aid in the neurological assessment. However, the extraction of quantitative information from such measurements is usually done manually through visual inspection. This paper presents a novel framework for automatic human movement assessment that executes segmentation and motor performance parameter extraction in time-series of measurements from a sequence of human movements. We use the elements of a Switching Linear Dynamic System model as building blocks to translate formal definitions and procedures from human movement analysis. Our approach provides a method for users with no expertise in signal processing to create models for movements using labeled dataset and later use it for automatic assessment. We validated our framework on preliminary tests involving six healthy adult subjects that executed common movements in functional tests and rehabilitation exercise sessions, such as sit-to-stand and lateral elevation of the arms and five elderly subjects, two of which with limited mobility, that executed the sit-to-stand movement. The proposed method worked on random motion sequences for the dual purpose of movement segmentation (accuracy of 72%-100%) and motor performance assessment (mean error of 0%-12%).","1558-0210","","10.1109/TNSRE.2016.2591783","Coordenação deAperfeiçoamento de Pessoal de Nível Superior; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7513405","Humanmovement assessment;motor performance;segmentation;switching lineardynamic systems","Hidden Markov models;Motion segmentation;Parameter extraction;Superluminescent diodes;Switches;State-space methods;Time series analysis","biomechanics;geriatrics;medical signal processing;neurophysiology;patient rehabilitation;physiological models;sensors","automatic human movement assessment;switching linear dynamic system;motion segmentation;motor performance;performance assessment;diagnosis;motor-control rehabilitation;portable sensor technology;neurological assessment;visual inspection;parameter extraction;signal processing;functional tests;rehabilitation exercise sessions;sit-to-stand;lateral elevation;elderly subjects;random motion sequences","Actigraphy;Adult;Aged;Aged, 80 and over;Algorithms;Computer Simulation;Female;Humans;Linear Models;Male;Middle Aged;Mobility Limitation;Models, Biological;Movement;Pattern Recognition, Automated;Psychomotor Performance;Reproducibility of Results;Sensitivity and Specificity","8","","27","","14 Jul 2016","","","IEEE","IEEE Journals"
"Automatic extraction of control points for digital subtraction angiography image enhancement","Y. Bentoutou; N. Taleb","Nat. Center for Space Techniques, Arzew, Algeria; NA","IEEE Transactions on Nuclear Science","11 Apr 2005","2005","52","1","238","246","In this paper, a new automatic control point selection and matching technique for digital subtraction angiography image enhancement is proposed. The characteristic of this approach is that it uses features that are based on image moments and are invariant to symmetric blur, translation, and rotation to establish correspondences between matched regions from two X-ray images. The automatic extraction of control points is based on an edge detection approach and on local similarity detection by means of template matching according to a combined invariants-based similarity measure. A new strategy was developed in which a 3-D space-time motion detection algorithm was used for selecting movement points (MPs) belonging to moving structures. The proposed technique has been successfully applied to register several clinical data sets including coronary applications. The experimental results demonstrate the efficiency and accuracy of the algorithm which have outperformed manual registration in terms of root mean square error at the MPs. In addition, the results of the proposed registration algorithm, using the combined invariants-based similarity measure are compared to those of the same proposed algorithm but using the energy of the histogram of differences (EHD) measure. These results clearly indicate that the combined invariants-based measure may be better suited for subpixel registration as it produces more accurate results than EHD.","1558-1578","","10.1109/TNS.2004.843120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1417136","Digital subtractions angiography;image registration;medical imaging;motion correction;template matching;thin plate spine","Digital control;Automatic control;Angiography;Image enhancement;Image edge detection;Energy measurement;X-ray imaging;Motion detection;Root mean square;Histograms","diagnostic radiography;image enhancement;medical image processing;edge detection;image registration","automatic control point selection;digital subtraction angiography image enhancement;image moments;symmetric blur;X-ray images;automatic extraction;edge detection approach;template matching;combined invariants-based similarity measure;3-D space-time motion detection algorithm;movement points;clinical data sets;coronary applications;root mean square error;histogram of difference measure;image registration;medical imaging;motion correction;thin plate spine","","32","1","9","","11 Apr 2005","","","IEEE","IEEE Journals"
"Automatic Spine Ultrasound Segmentation for Scoliosis Visualization and Measurement","T. Ungi; H. Greer; K. R. Sunderland; V. Wu; Z. M. C. Baum; C. Schlenger; M. Oetgen; K. Cleary; S. R. Aylward; G. Fichtinger","Laboratory for Percutaneous Surgery at the School of Computing, Queen's University, Kingston, ON, Canada; Kitware Inc; Laboratory for Percutaneous Surgery at the School of ComputingQueen's University; Laboratory for Percutaneous Surgery at the School of ComputingQueen's University; Laboratory for Percutaneous Surgery at the School of ComputingQueen's University; Verdure Imaging, Inc; Children's National Medical Center; Children's National Medical Center; Kitware Inc; Laboratory for Percutaneous Surgery at the School of ComputingQueen's University","IEEE Transactions on Biomedical Engineering","20 Oct 2020","2020","67","11","3234","3241","Objective: Integrate tracked ultrasound and AI methods to provide a safer and more accessible alternative to X-ray for scoliosis measurement. We propose automatic ultrasound segmentation for 3-dimensional spine visualization and scoliosis measurement to address difficulties in using ultrasound for spine imaging. Methods: We trained a convolutional neural network for spine segmentation on ultrasound scans using data from eight healthy adult volunteers. We tested the trained network on eight pediatric patients. We evaluated image segmentation and 3-dimensional volume reconstruction for scoliosis measurement. Results: As expected, fuzzy segmentation metrics reduced when trained networks were translated from healthy volunteers to patients. Recall decreased from 0.72 to 0.64 (8.2% decrease), and precision from 0.31 to 0.27 (3.7% decrease). However, after finding optimal thresholds for prediction maps, binary segmentation metrics performed better on patient data. Recall decreased from 0.98 to 0.97 (1.6% decrease), and precision from 0.10 to 0.06 (4.5% decrease). Segmentation prediction maps were reconstructed to 3-dimensional volumes and scoliosis was measured in all patients. Measurement in these reconstructions took less than 1 minute and had a maximum error of 2.2° compared to X-ray. Conclusion: automatic spine segmentation makes scoliosis measurement both efficient and accurate in tracked ultrasound scans. Significance: Automatic segmentation may overcome the limitations of tracked ultrasound that so far prevented its use as an alternative of X-ray in scoliosis measurement.","1558-2531","","10.1109/TBME.2020.2980540","NIH/NIBIB and NIH/NIGMS; CANARIE's Research Software Program; Canada Research Chair in Computer-Integrated Surgery; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9034149","Artificial neural networks;Pediatrics;Medical diagnostic imaging","Ultrasonic imaging;Image segmentation;Ultrasonic variables measurement;Biomedical measurement;Bones;X-ray imaging;Training","biomedical ultrasonics;bone;convolutional neural nets;data visualisation;diseases;image reconstruction;image segmentation;medical image processing;neurophysiology;orthopaedics;paediatrics","scoliosis visualization;3-dimensional spine visualization;spine imaging;image segmentation;3-dimensional volume reconstruction;fuzzy segmentation metrics;binary segmentation metrics;segmentation prediction maps;automatic spine ultrasound segmentation;integrate tracked ultrasound;convolutional neural network;pediatric patients","","","","30","IEEE","12 Mar 2020","","","IEEE","IEEE Journals"
"Automatic design of W-operators using LVQ - application to morphological image segmentation","F. C. Flores; S. M. Peres; F. J. Von Zuben","Dept. of Comput. Eng. & Ind. Autom., State Univ. of Campinas, Sao Paulo, Brazil; Dept. of Comput. Eng. & Ind. Autom., State Univ. of Campinas, Sao Paulo, Brazil; Dept. of Comput. Eng. & Ind. Autom., State Univ. of Campinas, Sao Paulo, Brazil","Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290)","7 Aug 2002","2002","1","","755","760 vol.1","W-operators compose a family of translation invariant and locally defined operators. This work proposes a new approach to an automatic design of W-operators using learning vector quantization neural networks. It is also presented here some experimental results associated with the application of these operators in morphological image segmentation.","1098-7576","0-7803-7278-6","10.1109/IJCNN.2002.1005568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1005568","","Image segmentation;Neural networks;Application software;Apertures;Computer industry;Automation;Filtering;Pattern recognition;Design optimization;Decision trees","neural nets;vector quantisation;learning (artificial intelligence);image segmentation;mathematical morphology","morphological operators;W-operators;LVQ neural networks;image segmentation;watershed with markers;learning;learning vector quantization","","","","23","","7 Aug 2002","","","IEEE","IEEE Conferences"
"Automatic fetal organs segmentation using multilayer super pixel and image moment feature","R. Rahmatullah; M. A. Ma'sum; Aprinaldi; P. Mursanto; B. Wiweko",Faculty of Computer Science Universitas Indonesia; Faculty of Computer Science Universitas Indonesia; Faculty of Computer Science Universitas Indonesia; Faculty of Computer Science Universitas Indonesia; Faculty of Medicine Universitas Indonesia,"2014 International Conference on Advanced Computer Science and Information System","26 Mar 2015","2014","","","420","426","Segmentation of fetal organs such as head, femur, and humérus on ultrasound image is one of the challenges in realization of automated system for fetal biometry measurements. Although many methods have been developed to overcome this problem, most of them are generally specific to one organ of the body alone. The research in this paper will focus on a machine learning method that has been available before: multilayer super pixel classification using random forest. The focus of this study is to improve the accuracy by exploring compactness parameter in the formation of super-pixels. In addition, we also add moment image features such as translation, rotation, and scale invariant to improve the segmentation performance. The experimental results showed that the difference in compactness parameters will provide different result for the accuracy, Fl-score, recall, and specificity. The addition of moment features can also improve the performance of image segmentation of fetal organs even though increase was not significant. Fetal head segmentation using proposed method has higher Fl-score and specificity, but lower accuracy and recall compared to previous methods. Whereas fetal femur and humérus segmentation using proposed method has higher accuracy, Fl-score, recall and specificity compared to previous method.","","978-1-4799-8075-8","10.1109/ICACSIS.2014.7065883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065883","","Decision support systems;Feature extraction","biomedical ultrasonics;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing","automatic fetal organs segmentation;multilayer super pixel;image moment feature;head;femur;humerus;ultrasound image;fetal biometry measurements;machine learning method;multilayer super pixel classification;random forest;compactness parameter;moment image features;segmentation performance;F1-score","","","","12","","26 Mar 2015","","","IEEE","IEEE Conferences"
"Towards computer vision based ancient coin recognition in the wild — Automatic reliable image preprocessing and normalization","B. Conn; O. Arandjelović","School of Computer Science, University of St Andrews St Andrews, KY16 9SX, Fife, Scotland, United Kingdom; School of Computer Science, University of St Andrews St Andrews, KY16 9SX, Fife, Scotland, United Kingdom","2017 International Joint Conference on Neural Networks (IJCNN)","3 Jul 2017","2017","","","1457","1464","As an attractive area of application in the sphere of cultural heritage, in recent years automatic analysis of ancient coins has been attracting an increasing amount of research attention from the computer vision community. Recent work has demonstrated that the existing state of the art performs extremely poorly when applied on images acquired in realistic conditions. One of the reasons behind this lies in the (often implicit) assumptions made by many of the proposed algorithms - a lack of background clutter, and a uniform scale, orientation, and translation of coins across different images. These assumptions are not satisfied by default and before any further progress in the realm of more complex analysis is made, a robust method capable of preprocessing and normalizing images of coins acquired `in the wild' is needed. In this paper we introduce an algorithm capable of localizing and accurately segmenting out a coin from a cluttered image acquired by an amateur collector. Specifically, we propose a two stage approach which first uses a simple shape hypothesis to localize the coin roughly and then arrives at the final, accurate result by refining this initial estimate using a statistical model learnt from large amounts of data. Our results on data collected `in the wild' demonstrate excellent accuracy even when the proposed algorithm is applied on highly challenging images.","2161-4407","978-1-5090-6182-2","10.1109/IJCNN.2017.7966024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966024","","Computer vision;Clutter;Shape;Visualization;Algorithm design and analysis;Image segmentation;Metals","computer vision;history;image segmentation;object recognition;statistical analysis","computer vision based ancient coin recognition;automatic reliable image preprocessing;image normalization;cultural heritage;ancient coin automatic analysis;coin localization;coin segmentation;cluttered image;amateur collector;two stage approach;shape hypothesis;statistical model","","3","","29","","3 Jul 2017","","","IEEE","IEEE Conferences"
"Automatic landmark detection in cephalometry using a modified Active Shape Model with sub image matching","Rahele Kafieh; Alireza Mehri; Saeed Sadri","Isfahan University of Medicine, Hezar jareeb St, Iran; Isfahan University of Medicine, Hezar jareeb St, Iran; Isfahan University of Technology, daneshgah sanati St, Iran","2007 International Conference on Machine Vision","7 Mar 2008","2007","","","73","78","This paper introduces a modification on using active shape models (ASM) for automatic landmark detection in cephalometry and combines many new ideas to improve its performance. In first step, some feature points are extracted to model the size, rotation, and translation of skull. A learning vector quantization (LVQ) neural network is used to classify images according to their geometrical specifications. Using LVQ for every new image, the possible coordinates of landmarks are estimated, knowing the class of new image. Then a modified ASM with a multi resolution approach is applied and a principal component analysis (PCA) is incorporated to analyze each template and the mean shape is calculated. The local search to find the best match to the intensity profile is then used and every point is moved to get the best location. Finally a sub image matching procedure, based on cross correlation, is applied to pinpoint the exact location of each landmark after the template has converged. On average It percent of the landmarks are within 1 mm of correct coordinates,percent within 1 mm, and percent within 1 mm, which shows a distinct improvement on other proposed methods.","","978-1-4244-1624-0","10.1109/ICMV.2007.4469276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4469276","cephalometry;Active Shape Model;non linear diffusion;Principal Component Analysis;sub image matching","Active shape model;Image matching;Principal component analysis;Radiography;Image edge detection;Morphology;Biomedical imaging;Electronic mail;Feature extraction;Skull","biomedical measurement;feature extraction;image classification;image matching;image resolution;medical image processing;object detection;principal component analysis;vector quantisation","automatic landmark detection;cephalometry;modified active shape model;subimage matching;feature extraction;learning vector quantization;neural network;image classification;principal component analysis","","1","1","22","","7 Mar 2008","","","IEEE","IEEE Conferences"
"A stochastic learning based approach for automatic medical diagnosis using HMM toolbox in scilab environment","T. Al-Ani; Y. Hamam","Lab. A2SI-ESIEE, Cite Descartes, Noisy-le-Grand; Lab. A2SI-ESIEE, Cite Descartes, Noisy-le-Grand","Proceedings of 2005 IEEE Conference on Control Applications, 2005. CCA 2005.","19 Sep 2005","2005","","","1099","1103","In this work, automatic medical diagnosis system of sleep apnea syndrome is presented. This system is based on hidden Markov models (HMMs) Scilab toolbox. Conventional as well as a new simulated annealing based approaches to train HMMs are incorporated. The inference method of this system translates event state value into common interpretation as a pathophysiological state. The interpretation is extended to sequences of states in time to obtain a pathophysiological state-space trajectory. Some of the measurements of the respiratory activity issued by the technique of polysomnography are considered for offline or online detection of different sleep apnea syndromes. Experimental results using respiratory clinical data and some future perspectives of our work are presented","1085-1992","0-7803-9354-6","10.1109/CCA.2005.1507277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1507277","","Stochastic processes;Medical diagnosis;Hidden Markov models;Sleep apnea;Laboratories;Performance evaluation;Medical treatment;Medical diagnostic imaging;Medical simulation;Simulated annealing","diseases;hidden Markov models;inference mechanisms;learning (artificial intelligence);medical diagnostic computing;medical signal processing;simulated annealing;sleep","stochastic learning;automatic medical diagnosis;HMM toolbox;Scilab;sleep apnea syndrome;hidden Markov models;simulated annealing;inference;pathophysiological state-space trajectory;polysomnography","","","3","23","","19 Sep 2005","","","IEEE","IEEE Conferences"
"Fully automatic 3-D segmentation of knee bone compartments by iterative local branch-and-mincut on MR images from osteoarthritis initiative (OAI)","Sang Hyun Park; Soochahn Lee; H. Shim; Il Dong Yun; Sang Uk Lee; Kyoung Ho Lee; Heung Sik Kang; Joon Koo Han","School of EECS, Seoul Nat'l Univ., Korea; School of EECS, Seoul Nat'l Univ., Korea; School of EECS, Seoul Nat'l Univ., Korea; School of EIE, Hankuk Univ. of F. S., Korea; School of EECS, Seoul Nat'l Univ., Korea; Dept. of Radiology, Seoul Nat'l Univ. Bundang Hosp., Korea; Dept. of Radiology, Seoul Nat'l Univ. Bundang Hosp., Korea; Dept. of Radiology, Seoul Nat'l Univ. Bundang Hosp., Korea","2009 16th IEEE International Conference on Image Processing (ICIP)","17 Feb 2010","2009","","","3381","3384","In this paper, we propose a fully automatic method to segment bone compartments in magnetic resonance (MR) images of knee joints gathered from a public database for research on knee osteoarthritis (OA), the osteoarthritis initiative (OAI). Considering the fixed scanning parameters which include position and flexion of the knee joint, the proposed method efficiently utilizes both shape and intensity priors obtained from pre-segmented data, and iteratively applies branch-and-mincut to a local subset of configurations of shape templates. More specifically, at each iteration, the optimal among a subset of the whole range in translation, rotation, and scale parameters are decomposed and separately computed, and motion is greedily selected by the lowest energy. Experimental results demonstrate the increased accuracy and efficiency compared to when only shape priors are applied and when branch-and-mincut is applied to the whole range of parameters at once, respectively.","2381-8549","978-1-4244-5654-3","10.1109/ICIP.2009.5413874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5413874","Segmentation;Branch-and-mincut;Knee Bone;MR Image;Osteoarthritis","Image segmentation;Knee;Bones;Osteoarthritis;Shape;Joints;Uncertainty;Image storage;Magnetic resonance;Image databases","biomedical MRI;bone;diseases;image segmentation;medical image processing","automatic 3D segmentation;knee bone compartments;iterative local branch-and-mincut;MR images;osteoarthritis initiative;segment bone compartments;magnetic resonance images;knee joints;public database;knee osteoarthritis;scanning parameters;shape templates;shape priors","","1","","10","","17 Feb 2010","","","IEEE","IEEE Conferences"
"Automatic detection of small spherical lesions using multiscale approach in 3D medical images","A. Fazlollahi; F. Meriaudeau; V. L. Villemagne; C. C. Rowe; P. M. Desmond; P. A. Yates; O. Salvado; P. Bourgeat","The Australian e-Health Research Centre-BioMedIA, Royal Brisbane and Women's Hospital, Herston, QLD, Australia, 4029; Le2i, Universite de Bourgone, Le Creusot, France, 71200; Department of Nuclear Medicine and Centre for PET, Austin Hospital, Melbourne, VIC, Australia; Department of Nuclear Medicine and Centre for PET, Austin Hospital, Melbourne, VIC, Australia; Department of Radiology, Royal Melbourne Hospital, Melbourne University, VIC, Parkville, Australia; Department of Nuclear Medicine and Centre for PET, Austin Hospital, Melbourne, VIC, Australia; The Australian e-Health Research Centre-BioMedIA, Royal Brisbane and Women's Hospital, Herston, QLD, Australia, 4029; The Australian e-Health Research Centre-BioMedIA, Royal Brisbane and Women's Hospital, Herston, QLD, Australia, 4029","2013 IEEE International Conference on Image Processing","13 Feb 2014","2013","","","1158","1162","Automated detection of small, low level shapes such as circular/spherical objects in images is a challenging computer vision problem. For many applications, especially microbleed detection in Alzheimer's disease, an automatic pre-screening scheme is required to identify potential seeds with high sensitivity and reasonable specificity. A new method is proposed to detect spherical objects in 3D medical images within the multi-scale Laplacian of Gaussian framework. The major contributions are(1)breaking down 3D sphere detection into 1D line profile detection along each coordinate dimension, (2) identifying center of structures bynormalizing the line response profile and (3) employing eigenvalues of the Hessian matrix at optimum scale for the center points to determine spherical objects. The method is validated both on simulated data and susceptibility weighted MRI images with ground truth provided by a medical expert. Validation results demonstrate that the current approach has higher performance in terms of sensitivity and specificity and is effective in detecting adjacent microbleeds, with invariance to intensity, orientation, translation and object scale.","2381-8549","978-1-4799-2341-0","10.1109/ICIP.2013.6738239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738239","3D sphere detection;Laplacian of Gaussian;cerebral micro bleed;center detection;multi-scale","Three-dimensional displays;Shape;Biomedical imaging;Optimized production technology;Lesions;Eigenvalues and eigenfunctions;Object recognition","biomedical MRI;computer vision;diseases;eigenvalues and eigenfunctions;Gaussian processes;Hessian matrices;Laplace equations;medical image processing;object detection","automatic small spherical lesion detection;multiscale approach;3D medical images;circular objects;spherical objects;computer vision problem;Alzheimer's disease;automatic prescreening scheme;spherical object detection;multiscale Laplacian;Gaussian framework;3D sphere detection;1D line profile detection;line response profile;eigenvalues;Hessian matrix;susceptibility weighted MRI images;adjacent microbleed detection","","4","","12","","13 Feb 2014","","","IEEE","IEEE Conferences"
"Toward Automatic Building Footprint Delineation From Aerial Images Using CNN and Regularization","S. Wei; S. Ji; M. Lu","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Physical Geography, Utrecht University, Utrecht, The Netherlands","IEEE Transactions on Geoscience and Remote Sensing","26 Feb 2020","2020","58","3","2178","2189","This study proposes an automatic building footprint extraction framework that consists of a convolutional neural network (CNN)-based segmentation and an empirical polygon regularization that transforms segmentation maps into structured individual building polygons. The framework attempts to replace part of the manual delineation of building footprints that are involved in surveying and mapping field with algorithms. First, we develop a scale robust fully convolutional network (FCN) by introducing multiple scale aggregation of feature pyramids from convolutional layers. Two postprocessing strategies are introduced to refine the segmentation maps from the FCN. The refined segmentation maps are vectorized and polygonized. Then, we propose a polygon regularization algorithm consisting of a coarse and fine adjustment, to translate the initial polygons into structured footprints. Experiments on a large open building data set including 181 000 buildings showed that our algorithm reached a high automation level where at least 50% of individual buildings in the test area could be delineated to replace manual work. Experiments on different data sets demonstrated that our FCN-based segmentation method outperformed several most recent segmentation methods, and our polygon regularization algorithm is robust in challenging situations with different building styles, image resolutions, and even low-quality segmentation.","1558-0644","","10.1109/TGRS.2019.2954461","National Basic Research Program of China (973 Program); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933116","Building extraction;fully convolutional network (FCN);polygon regularization;segmentation","Image segmentation;Manuals;Remote sensing;Tiles;Feature extraction;Convolution","buildings (structures);cartography;computational geometry;convolutional neural nets;feature extraction;geophysical image processing;image resolution;image segmentation;surveying","aerial images;automatic building footprint extraction framework;convolutional neural network-based segmentation;CNN;empirical polygon regularization;structured individual building polygons;scale robust fully convolutional network;multiple scale aggregation;convolutional layers;refined segmentation maps;polygon regularization algorithm;initial polygons;structured footprints;open building data;FCN-based segmentation method;building styles;low-quality segmentation;automatic building footprint delineation","","3","","32","IEEE","16 Dec 2019","","","IEEE","IEEE Journals"
"FRIP: a region-based image retrieval tool using automatic image segmentation and stepwise Boolean AND matching","B. Ko; H. Byun","Dept. of Comput. Sci., Yonsei Univ., South Korea; Dept. of Comput. Sci., Yonsei Univ., South Korea","IEEE Transactions on Multimedia","24 Jan 2005","2005","7","1","105","113","We present our region-based image retrieval tool, finding region in the picture (FRIP), that is able to accommodate, to the extent possible, region scaling, rotation, and translation. Our goal is to develop an effective retrieval system to overcome a few limitations associated with existing systems. To do this, we propose adaptive circular filters used for semantic image segmentation, which are based on both Bayes' theorem and texture distribution of image. In addition, to decrease the computational complexity without losing the accuracy of the search results, we extract optimal feature vectors from segmented regions and apply them to our stepwise Boolean AND matching scheme. The experimental results using real world images show that our system can indeed improve retrieval performance compared to other global property-based or region-of-interest-based image retrieval methods.","1941-0077","","10.1109/TMM.2004.840603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1386246","Bayes' theorem;Boolean AND matching;content-based image retrieval (CBIR);finding region in the pictures (FRIP)","Image retrieval;Image segmentation;Content based retrieval;Shape;Information retrieval;Image databases;Computer science;Color;Adaptive filters;Computational complexity","image retrieval;visual databases;content-based retrieval;Bayes methods;image segmentation;feature extraction;image texture;adaptive filters;image matching;computational complexity;Boolean algebra;image colour analysis","region-based image retrieval tool;adaptive circular filters;semantic image segmentation;Bayes theorem;image texture distribution;feature vector extraction;stepwise Boolean AND matching scheme;content-based image retrieval","","60","5","28","","24 Jan 2005","","","IEEE","IEEE Journals"
"Vocal tract length invariant features for automatic speech recognition","A. Mertins; J. Rademacher","Inst. of Phys., Oldenburg Univ., Germany; Inst. of Phys., Oldenburg Univ., Germany","IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","308","312","The effects of vocal tract length (VTL) variation are often approximated by linear frequency warping of short-time spectra. Based on this relationship, we present a method for generating vocal tract length invariant features. These new features are computed as translation invariant, correlation-type features in a log-frequency domain. In phoneme classification experiments, their discrimination capabilities turned out to be considerably better than for Mel-frequency cepstral coefficients (MFCCs). The best results are obtained when VTL-invariant (VTLI) features and MFCCs are combined. The superiority of the combined feature set and its resilience to VTL variations is also shown for word recognition, using the TIDIGITS corpus and the HTK recognizer","","0-7803-9478-X","10.1109/ASRU.2005.1566473","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566473","","Automatic speech recognition;Frequency;Wavelet transforms;Cepstral analysis;Hidden Markov models;Testing;Signal resolution;Robustness;Signal processing;Physics","correlation theory;feature extraction;speech recognition","vocal tract length invariant features;automatic speech recognition;linear frequency warping;correlation-type features;log-frequency domain;phoneme classification","","11","","9","","3 Jan 2006","","","IEEE","IEEE Conferences"
"Automatic speech recognition to teleoperate a robot via Web","R. Marin; P. Vila; P. J. Sanz; A. Marzal","Robotics Lab., Jaume I Univ., Castellon, Spain; NA; NA; NA","IEEE/RSJ International Conference on Intelligent Robots and Systems","10 Dec 2002","2002","2","","1278","1283 vol.2","This article shows the way a speech recognition module has been implemented that provides the translation between voice input and the convenient text based commands to be executed on an already existing web robot. The novel contribution is the way the procedure is defined to be run over the Internet, and the interface implemented to connect any kind of external speech recognition program to the robot controller. First section gives an overview to the whole project, from a user point of view. Secondly, the software architecture is presented, giving low level details on the connectivity procedure and the interface defined between the remote controller and the speech recognition and synthesizer program. Moreover, it introduces some of the technologies (e.g. Microsoft Speech SDK) used to implement this capability. Finally, results are shown by means of both, accuracy details of the speech recognition procedure, and computing time.","","0-7803-7398-7","10.1109/IRDS.2002.1043922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1043922","","Automatic speech recognition;Robotics and automation;Speech recognition;Cameras;Robot vision systems;Telerobotics;Human robot interaction;Internet;Space technology;Web sites","telerobotics;Internet;speech recognition;natural language interfaces;software architecture","automatic speech recognition;robot teleoperation;Web;voice input;text based commands;connectivity procedure;speech synthesis;Microsoft Speech SDK","","8","","16","","10 Dec 2002","","","IEEE","IEEE Conferences"
"PlaneCalib: Automatic Camera Calibration by Multiple Observations of Rigid Objects on Plane","V. Bartl; R. Juránek; J. Špaňhel; A. Herout","Graph@FIT Faculty of Information Technology, Brno University of Technology,Brno,Czech Republic; Graph@FIT Faculty of Information Technology, Brno University of Technology,Brno,Czech Republic; Graph@FIT Faculty of Information Technology, Brno University of Technology,Brno,Czech Republic; Graph@FIT Faculty of Information Technology, Brno University of Technology,Brno,Czech Republic","2020 Digital Image Computing: Techniques and Applications (DICTA)","1 Mar 2021","2020","","","1","8","In this work, we propose a novel method for automatic camera calibration, mainly for surveillance cameras. The calibration consists in observing objects on the ground plane of the scene; in our experiments, vehicles were used. However, any arbitrary rigid objects can be used instead, as verified by experiments with synthetic data. The calibration process uses convolutional neural network localisation of landmarks on the observed objects in the scene and the corresponding 3D positions of the localised landmarks - thus fine-grained classification of the detected vehicles in the image plane is done. The observation of the objects (detection, classification and landmark detection) enables to determine all typically used camera calibration parameters (focal length, rotation matrix, and translation vector). The experiments with real data show slightly better results in comparison with state-of-the-art work, however with an extreme speed-up. The calibration error decreased from 3.01% to 2.72% and 1223 × faster computation was achieved.","","978-1-7281-9108-9","10.1109/DICTA51227.2020.9363417","TACR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363417","","Three-dimensional displays;Surveillance;Object detection;Traffic control;Cameras;Calibration;Task analysis","calibration;cameras;convolutional neural nets;image classification;image registration;object detection;road vehicles","localised landmarks;fine-grained classification;detected vehicles;image plane;landmark detection;camera calibration parameters;calibration error;automatic camera calibration;multiple observations;surveillance cameras;observing objects;ground plane;arbitrary rigid objects;calibration process;observed objects","","","","43","","1 Mar 2021","","","IEEE","IEEE Conferences"
"Improving Noise Robust Automatic Speech Recognition with Single-Channel Time-Domain Enhancement Network","K. Kinoshita; T. Ochiai; M. Delcroix; T. Nakatani","NTT Corporation,NTT Communication Science Laboratories,Kyoto,Japan; NTT Corporation,NTT Communication Science Laboratories,Kyoto,Japan; NTT Corporation,NTT Communication Science Laboratories,Kyoto,Japan; NTT Corporation,NTT Communication Science Laboratories,Kyoto,Japan","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","7009","7013","With the advent of deep learning, research on noise-robust automatic speech recognition (ASR) has progressed rapidly. However, ASR performance in noisy conditions of single-channel systems remains unsatisfactory. Indeed, most single-channel speech enhancement (SE) methods (denoising) have brought only limited performance gains over state-of-the-art ASR back-end trained on multi-condition training data. Recently, there has been much research on neural network-based SE methods working in the time-domain showing levels of performance never attained before. However, it has not been established whether the high enhancement performance achieved by such time-domain approaches could be translated into ASR. In this paper, we show that a single-channel time-domain denoising approach can significantly improve ASR performance, providing more than 30 % relative word error reduction over a strong ASR back-end on the real evaluation data of the single-channel track of the CHiME-4 dataset. These positive results demonstrate that single-channel noise reduction can still improve ASR performance, which should open the door to more research in that direction.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053266","Single-channel speech enhancement;time-domain network;robust ASR","Training;Training data;Speech enhancement;Signal processing;Noise robustness;Time-domain analysis;Automatic speech recognition","neural nets;signal denoising;speech enhancement;speech recognition;time-domain analysis","noise robust automatic speech recognition;single-channel time-domain enhancement network;ASR performance;noisy conditions;single-channel systems;single-channel speech enhancement methods;multicondition training data;neural network-based SE methods;high enhancement performance;single-channel time-domain denoising approach;strong ASR;single-channel track;single-channel noise reduction;CHiME-4 dataset;relative word error reduction","","4","","36","","9 Apr 2020","","","IEEE","IEEE Conferences"
"Automatic Inspection System for Quality Evaluation of Fresh Tuna Meat","A. Mateo; F. Soto; J. A. Villarejo; C. Fernandez","Department of Electronic Technology (DTE), Technical University of Cartagena, Campus Muralla del Mar s/n. Cartagena, Spain; Department of Electronic Technology (DTE), Technical University of Cartagena, Campus Muralla del Mar s/n. Cartagena, Spain; Department of Electronic Technology (DTE), Technical University of Cartagena, Campus Muralla del Mar s/n. Cartagena, Spain; Department of Electronic Technology (DTE), Technical University of Cartagena, Campus Muralla del Mar s/n. Cartagena, Spain","2007 IEEE International Symposium on Industrial Electronics","5 Nov 2007","2007","","","1669","1674","This paper describes an automated visual inspection system SIVATUN™ for quality control of tuna meat which automate manual inspection operation. Depending on whether tuna are reared in cages or in the open sea and depending on the method of capture (electroslaughtering, shooting, etc.), differences are observed and changes take place in the organic nature of Blue Fin Tuna (Thunnus Thynnus) meat, affecting appearance and quality. These changes are caused by an increase and accumulation of lactate, which negatively affects the quality. This drop in quality translates into loss of value of the meat in Japanese tuna markets (Tsukiji market and Sashimi market are the main destinations of tuna captured off the southeast coast of Spain). In order to evaluate these changes, Japanese experts are employed to carry out visual inspections, which therefore constitute a subjective assessment. This paper describes the development of an automated visual inspection system that can analyze, model and detect these changes. The ultimate aim is to establish quality indicators and classifiers that will accompany tuna meat from the time of capture and so make it possible to track this product in the main tuna meat export markets. In this way we can establish a connection between variation of tuna meat quality and the feeding and slaughtering methods used, thus providing feedback to the fattening and slaughtering processes so as to improve the global quality of Blue Fin Tuna catches.","2163-5145","978-1-4244-0754-5","10.1109/ISIE.2007.4374855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4374855","","Inspection;Color;Consumer electronics;Aquaculture;Production;Muscles;Quality control;Manuals;Feedback;Temperature","aquaculture;computer vision;inspection;quality control","automatic inspection system;quality evaluation;fresh tuna meat;SIVATUN;quality control;visual inspection;tuna meat export market;feeding method;slaughtering method;blue fin tuna","","1","","18","","5 Nov 2007","","","IEEE","IEEE Conferences"
"A Novel Shape Indexing Method for Automatic Classification of Lepidoptera","O. Starostenko; C. K. Cruz; A. Chavez-Aragon; R. Contreras","Universidad de las Américas Puebla, Mexico; Universidad de las Américas Puebla, Mexico; Universidad de las Américas Puebla, Mexico; Universidad de las Américas Puebla, Mexico","17th International Conference on Electronics, Communications and Computers (CONIELECOMP'07)","12 Mar 2007","2007","","","30","30","This paper presents a novel approach for images retrieval form digital collection based on analysis of their low level characteristics such as color and shape that has been applied for classification of the rare specimens of the Lepidoptera (butterflies) in Mexico. The similarity with user's queries which are the images, manual sketches or textual descriptions is computed by comparison of the feature vectors representing a shape by proposed two segments turning functions that is invariant to translation, rotation, and scale. In order to speed up preprocessing of shapes the proposed convex regions algorithm and discrete curve evolution approach are applied. Another goal of the proposed method is integration of two signatures of shape used in hashing tables for data base of images such as compactness and elongatedness which provide fast and satisfactory image retrieval by accelerating the convergence to expected result. The proposed method has been tested and evaluated using designed search engine for classification of the butterfly's families","","0-7695-2799-X","10.1109/CONIELECOMP.2007.7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127270","","Shape;Indexing;Image retrieval;Image color analysis;Image analysis;Image segmentation;Turning;Information retrieval;Acceleration;Convergence","feature extraction;image classification;image representation;image retrieval;image segmentation;indexing;search engines;visual databases;zoology","shape indexing method;automatic classification;Lepidoptera;images retrieval;Mexico;feature vector;shape representation;segment turning function;convex region algorithm;discrete curve evolution approach;shape signature integration;hashing table;image data base;convergence;search engine;butterfly family","","2","","13","","12 Mar 2007","","","IEEE","IEEE Conferences"
"Empirical Comparison of Automatic Image Annotation Systems","M. M. B. Ismail; H. Frigui; J. Caudill","Multimedia Research Lab, CECS Dept., University of Louisville; Multimedia Research Lab, CECS Dept., University of Louisville; Multimedia Research Lab, CECS Dept., University of Louisville","2008 First Workshops on Image Processing Theory, Tools and Applications","9 Jan 2009","2008","","","1","8","The performance of content-based image retrieval systems has proved to be inherently constrained by the used low-level features, and cannot give satisfactory results when the user's high level concepts cannot be expressed by low level features. In an attempt to bridge this semantic gap, recent approaches started integrating both low level-visual features and high-level textual keywords. Unfortunately, manual image annotation is a tedious process and may not be possible for large image databases. To overcome this limitation, several approaches that can annotate images in a semi-supervised or unsupervised way have emerged. In this paper, we outline and compare four different algorithms. The first one is simple and assumes that image annotation can be viewed as the task of translating from a vocabulary of fixed image regions to a vocabulary of words. The second approach uses a set of annotated images as a training set and learns the joint distribution of regions and words. The third and fourth approaches are based on segmenting the images into homogeneous regions. Both of these approaches rely on a clustering algorithm to learn the association between visual features and keywords. The clustering task is not trivial as it involves clustering a very high-dimensional and sparse feature spaces. To address this, the third approach uses semi-supervised constrained clustering while the fourth approach relies on an algorithm that performs simultaneous clustering and feature discrimination. These four algorithms were implemented and tested on a data set that includes 6000 images using four-fold cross validation.","2154-512X","978-1-4244-3321-6","10.1109/IPTA.2008.4743754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4743754","image annotation;clustering;constrained clustering;content-based image retrieval","Image retrieval;Clustering algorithms;Content based retrieval;Image databases;Image processing;Bridges;Vocabulary;Image segmentation;Information retrieval;Labeling","content-based retrieval;image retrieval;image segmentation;learning (artificial intelligence);visual databases","automatic image annotation systems;content-based image retrieval;low level-visual features;high-level textual keywords;image segmentation;clustering algorithm;semisupervised constrained clustering;feature discrimination","","1","","38","","9 Jan 2009","","","IEEE","IEEE Conferences"
"Automatic Spotting of Vowels, Nasals and Approximants from Speech Signals","S. Salim; G. Deekshitha; A. George; L. Mary","Dept of ECE, Rajiv Gandhi Institute of Technology, Kottayam, Kerala, India; Dept of ECE, Rajiv Gandhi Institute of Technology, Kottayam, Kerala, India; Dept of ECE, Rajiv Gandhi Institute of Technology, Kottayam, Kerala, India; Dept of ECE, Government Engineering College, Idukki, Kerala, India","2018 International CET Conference on Control, Communication, and Computing (IC4)","11 Nov 2018","2018","","","272","277","Automatic speech recognition involves methodologies for translation of spoken language into text. An important problem that needs to be solved for the success of speech recognition is the accurate detection of phonemes. In this paper, a two stage system for spotting the boundaries of vowels, nasals and approximants in Malayalam speech signal is proposed. In the first stage, speech signal is classified into six broad phoneme classes using an Artificial Neural Network based broad phoneme classifier. Classifier with nine features has limited accuracy for detecting vowel, nasal and approximant boundaries. So features like difference of spectral spread, spectral centroid, envelope variance, energy ratio, difference in formant frequencies are added to the classifier. With these additional features, a major improvement in classifier accuracy is achieved. In the second stage, a frequency domain parameter named spectral peak frequency is suggested for accurate verification of nasals. Sonorant and nonsyllabic features are used for verifying approximants and syllabic feature is used for locating vowels.","","978-1-5386-4966-4","10.1109/CETIC4.2018.8531026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531026","Broad Phoneme Classifier;Spectral Spread;Spectral Centroid;Envelope Variance;Energy Ratio;Spectral Peak Frequency;Sonorant","Frequency measurement;Feature extraction;Neural networks;Energy measurement;Speech recognition;Speech processing;Training","natural language processing;neural nets;signal classification;speech recognition","automatic speech recognition;Malayalam speech signal;broad phoneme classifier;nasal boundaries;spectral peak frequency;nonsyllabic features;syllabic feature;artificial neural network;automatic vowel spotting;sonorant features","","","","9","","11 Nov 2018","","","IEEE","IEEE Conferences"
"Assessment of the repeatability in an automatic methodology for hyperemia grading in the bulbar conjunctiva","L. S. Brea; N. B. Rodríguez; A. M. González; K. Evans","Department of Computer Science, University of A Coruna, Spain; Department of Computer Science, University of A Coruna, Spain; Department of Electronics and Computer Science, University of Santiago de Compostela, Spain; School of Optometry and Vision Sciences, Cardiff University, Wales","2017 International Joint Conference on Neural Networks (IJCNN)","3 Jul 2017","2017","","","1673","1680","When the vessels of the bulbar conjunctiva get congested with blood, a characteristic red hue appears in the area. This symptom is known as hyperemia, and can be an early indicator of certain pathologies. Therefore, a prompt diagnosis is desirable in order to minimize both medical and economic repercussions. A fully automatic methodology for hyperemia grading in the bulbar conjunctiva was developed, by means of image processing and machine learning techniques. As there is a wide range of illumination, contrast, and focus issues in the images that specialists use to perform the grading, a repeatability analysis is necessary. Thus, the validation of each step of the methodology was performed, analyzing how variations in the images are translated to the results, and comparing them to the optometrist's measurements. Our results prove the robustness of our methodology to various conditions. Moreover, the differences in the automatic outputs are similar to the optometrist's ones.","2161-4407","978-1-5090-6182-2","10.1109/IJCNN.2017.7966052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966052","","Lenses;Image segmentation;Pathology;Lighting;Image color analysis;Image edge detection","diseases;learning (artificial intelligence);medical image processing","repeatability analysis;image focus;image contrast;illumination;machine learning;image processing;hyperemia grading;economic repercussions;medical repercussions;diagnosis;pathologies;characteristic red hue;blood;bulbar conjunctiva;vessels;repeatability assessment","","","","20","","3 Jul 2017","","","IEEE","IEEE Conferences"
"Gridline: automatic grid alignment DNA microarray scans","P. Bajcsy","Nat. Center for Supercomput. Applic., Champaign, IL, USA","IEEE Transactions on Image Processing","30 Jan 2004","2004","13","1","15","25","We present a new automatic grid alignment algorithm for detecting two-dimensional (2-D) arrays of spots in DNA microarray images. Our motivation for this work is the lack of automation in high-throughput microarray data analysis that leads to a) spatial inaccuracy of located spots and hence inaccuracy of extracted information from a spot and b) inconsistency of extracted features due to manual selection of grid alignment parameters. The proposed grid alignment algorithm is novel in the sense that 1) it can detect irregularly row- and column-spaced spots in a 2-D array, 2) it is independent of spot color and size, 3) it is general to localize a grid of other primitive shapes than the spot shapes, 4) it can perform grid alignment on any number of input channels, 5) it reduces the number of free parameters to minimum by data driven optimization of most algorithmic parameters, and 6) it has a built-in speed versus accuracy tradeoff mechanism to accommodate user's requirements on performance time and accuracy of the results. The developed algorithm also automatically identifies multiple blocks of 2-D arrays, as it is the case in microarray images, and compensates for grid rotations in addition to grid translations.","1941-0042","","10.1109/TIP.2003.819941","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1262009","","DNA;Data mining;Pins;Two dimensional displays;Feature extraction;Shape;Image color analysis;Automation;Data analysis;Quality control","DNA;feature extraction;optimisation;image processing;medical image processing","automatic grid alignment algorithm;DNA microarray images;image analysis;quality control;spot alignment","Algorithms;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Microscopy, Fluorescence;Oligonucleotide Array Sequence Analysis;Pattern Recognition, Automated;Reproducibility of Results;Robotics;Sensitivity and Specificity;Signal Processing, Computer-Assisted;Subtraction Technique","48","18","25","","30 Jan 2004","","","IEEE","IEEE Journals"
"3-D freehand echocardiography for automatic left ventricle reconstruction and analysis based on multiple acoustic windows","Xujiong Ye; J. A. Noble; D. Atkinson","Dept. of Eng. Sci., Univ. of Oxford, UK; Dept. of Eng. Sci., Univ. of Oxford, UK; Dept. of Eng. Sci., Univ. of Oxford, UK","IEEE Transactions on Medical Imaging","22 Jan 2003","2002","21","9","1051","1058","A new method is proposed to reconstruct and analyze the left ventricle (LV) from multiple acoustic window three-dimensional (3-D) ultrasound acquired using a transthoracic 3-D rotational probe. Prior research in this area has been based on one acoustic window acquisition. However, the data suffers from several limitations that degrade the reconstruction and reduce the clinical value of interpretation, such as the presence of shadow due to bone (ribs) and air (in the lungs) and motion of the probe during the acquisition. In this paper, we show how to overcome these limitations by automatically fusing information from multiple acoustic window sparse-view acquisitions and using a position sensor to track the probe in real time. Geometric constraints of the object shape, and spatiotemporal information relating to the image acquisition process, are used in new algorithms for 1) grouping endocardial edge cues from an initial image segmentation and 2) defining a novel reconstruction method that utilizes information from multiple acoustic windows. The new method has been validated on a phantom and three real heart data sets. In the phantom study, one finger of a latex glove was scanned from two acoustic windows and reconstructed using the new method. The volume error was measured to be less than 4%. In the clinical case study, 3-D ultrasound and magnetic resonance imaging (MRI) scanning were performed on the same healthy volunteers. Quantitative ejection fractions (EFs) and volume-time curves over a cardiac cycle were estimated using the new method and compared to cardiac MRI measurements. This showed that the new method agrees better with MRI measurements than the previous approach we have developed based on a single acoustic window. The EF errors of the new method with respect to MRI measurements were less than 6%. A more extensive clinical validation is required to establish whether these promising first results translate to a method suitable for routine clinical use.","1558-254X","","10.1109/TMI.2002.804436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166634","","Echocardiography;Magnetic resonance imaging;Image reconstruction;Acoustic measurements;Probes;Ultrasonic variables measurement;Ultrasonic imaging;Imaging phantoms;Volume measurement;Degradation","echocardiography;image reconstruction;medical image processing;image segmentation","latex glove finger;spatiotemporal information;ribs;endocardial edge cues grouping;volume error;image acquisition process;geometric constraints;cardiac cycle;volume-time curves;quantitative ejection fractions;healthy volunteers;cardiac MRI measurements;routine clinical use","Algorithms;Echocardiography, Three-Dimensional;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Phantoms, Imaging;Ventricular Function, Left","29","2","26","","22 Jan 2003","","","IEEE","IEEE Journals"
"Automatic speech recognition errors detection using supervised learning techniques","R. Errattahi; A. E. Hannani; H. Ouahmane; T. Hain","Laboratory of Information Technologies, National School of Applied Sciences, University of Chouaib Doukkali, El Jadida - Morocco; Laboratory of Information Technologies, National School of Applied Sciences, University of Chouaib Doukkali, El Jadida - Morocco; Laboratory of Information Technologies, National School of Applied Sciences, University of Chouaib Doukkali, El Jadida - Morocco; Speech and Hearing Group, Department of Computer Science, University of Sheffield, UK","2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)","12 Jun 2017","2016","","","1","6","Over the last years, many advances have been made in the field of Automatic Speech Recognition (ASR). However, the persistent presence of ASR errors is limiting the widespread adoption of speech technology in real life applications. This motivates the attempts to find alternative techniques to automatically detect and correct ASR errors, which can be very effective and especially when the user does not have access to tune the features, the models or the decoder of the ASR system or when the transcription serves as input to downstream systems like machine translation, information retrieval, and question answering. In this paper, we present an ASR errors detection system targeted towards substitution and insertion errors. The proposed system is based on supervised learning techniques and uses input features deducted only from the ASR output words and hence should be usable with any ASR system. Applying this system on TV program transcription data leads to identify 40.30% of the recognition errors generated by the ASR system.","2161-5330","978-1-5090-4320-0","10.1109/AICCSA.2016.7945669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7945669","","Feature extraction;Speech;Decoding;Context;Supervised learning;Acoustics;Training","error correction;error detection;learning (artificial intelligence);speech recognition","automatic speech recognition error detection;supervised learning;ASR error detection system;ASR error correction;substitution errors;insertion errors;ASR output words;TV program transcription data","","4","","19","","12 Jun 2017","","","IEEE","IEEE Conferences"
"Context-Based Filtering of Noisy Labels for Automatic Basemap Updating From UAV Data","C. M. Gevaert; C. Persello; S. O. Elberink; G. Vosselman; R. Sliuzas","Faculty of Geo-Information Science and Earth Observation, University of Twente, Enschede, AE, The Netherlands; Faculty of Geo-Information Science and Earth Observation, University of Twente, Enschede, AE, The Netherlands; Faculty of Geo-Information Science and Earth Observation, University of Twente, Enschede, AE, The Netherlands; Faculty of Geo-Information Science and Earth Observation, University of Twente, Enschede, AE, The Netherlands; Faculty of Geo-Information Science and Earth Observation, University of Twente, Enschede, AE, The Netherlands","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","24 Aug 2018","2018","11","8","2731","2741","Unmanned aerial vehicles (UAVs) have the potential to obtain high-resolution aerial imagery at frequent intervals, making them a valuable tool for urban planners who require up-to-date basemaps. Supervised classification methods can be exploited to translate the UAV data into such basemaps. However, these methods require labeled training samples, the collection of which may be complex and time consuming. Existing spatial datasets can be exploited to provide the training labels, but these often contain errors due to differences in the date or resolution of the dataset from which these outdated labels were obtained. In this paper, we propose an approach for updating basemaps using global and local contextual cues to automatically remove unreliable samples from the training set, and thereby, improve the classification accuracy. Using UAV datasets over Kigali, Rwanda, and Dar es Salaam, Tanzania, we demonstrate how the amount of mislabeled training samples can be reduced by 44.1% and 35.5%, respectively, leading to a classification accuracy of 92.1% in Kigali and 91.3% in Dar es Salaam. To achieve the same accuracy in Dar es Salaam, between 50000 and 60000 manually labeled image segments would be needed. This demonstrates that the proposed approach of using outdated spatial data to provide labels and iteratively removing unreliable samples is a viable method for obtaining high classification accuracies while reducing the costly step of acquiring labeled training samples.","2151-1535","","10.1109/JSTARS.2017.2762905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103751","Basemap updating;image classification;informal settlements;label noise;random forests;unmanned aerial vehicles (UAVs);urban planning","Training;Image segmentation;Noise measurement;Remote sensing;Spatial databases;Unmanned aerial vehicles;Spatial resolution","autonomous aerial vehicles;image classification;image resolution;image segmentation;learning (artificial intelligence);town and country planning","outdated spatial data;context-based filtering;automatic basemap updating;unmanned aerial vehicles;high-resolution aerial imagery;urban planners;supervised classification methods;local contextual cues;UAV datasets;Kigali;Dar es Salaam;labeled image segments;Rwanda;Tanzania","","","","40","","10 Nov 2017","","","IEEE","IEEE Journals"
"Weakly Supervised Training of a Sign Language Recognition System Using Multiple Instance Learning Density Matrices","D. Kelly; J. Mc Donald; C. Markham","Computer Science Department, National University of Ireland Maynooth, Maynooth, Ireland; Computer Science Department, National University of Ireland Maynooth, Maynooth, Ireland; Computer Science Department, National University of Ireland Maynooth, Maynooth, Ireland","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","14 Mar 2011","2011","41","2","526","541","A system for automatically training and spotting signs from continuous sign language sentences is presented. We propose a novel multiple instance learning density matrix algorithm which automatically extracts isolated signs from full sentences using the weak and noisy supervision of text translations. The automatically extracted isolated samples are then utilized to train our spatiotemporal gesture and hand posture classifiers. The experiments were carried out to evaluate the performance of the automatic sign extraction, hand posture classification, and spatiotemporal gesture spotting systems. We then carry out a full evaluation of our overall sign spotting system which was automatically trained on 30 different signs.","1941-0492","","10.1109/TSMCB.2010.2065802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582311","HMM;multiple instance learning (MIL);sign language recognition;size function;support vector machine (SVM);weakly supervised learning","Handicapped aids;Hidden Markov models;Training;Feature extraction;Shape;Spatiotemporal phenomena;Videos","gesture recognition;image classification;learning (artificial intelligence);matrix algebra","weakly supervised training;sign language recognition system;multiple instance learning density matrices;continuous sign language sentences;noisy supervision;text translations;spatiotemporal gesture;hand posture classifiers;automatic sign extraction","Algorithms;Artificial Intelligence;Hand;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Photography;Reproducibility of Results;Sensitivity and Specificity;Sign Language","29","","38","","23 Sep 2010","","","IEEE","IEEE Journals"
"Syntactically-informed models for comma prediction","B. Favre; D. Hakkani-Tur; E. Shriberg","International Computer Science Institute, Berkeley, USA; International Computer Science Institute, Berkeley, USA; International Computer Science Institute, Berkeley, USA","2009 IEEE International Conference on Acoustics, Speech and Signal Processing","26 May 2009","2009","","","4697","4700","Providing punctuation in speech transcripts not only improves readability, but it also helps downstream text processing such as information extraction or machine translation. In this paper, we improve by 7% the accuracy of comma prediction in English broadcast news by introducing syntactic features inspired by the role of commas as described in linguistics studies. We conduct an analysis of the impact of those features on other subsets of features (prosody, words...) when combined through CRFs. The syntactic cues can help characterizing large syntactic patterns such as appositions and lists which are not necessarily marked by prosody.","2379-190X","978-1-4244-2353-8","10.1109/ICASSP.2009.4960679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4960679","Speech Processing;Punctuation;Machine Learning","Predictive models;Broadcasting;Speech processing;Data mining;Computer science;Classification tree analysis;Boosting;Neural networks;Decision trees;Testing","linguistics;natural language processing;speech recognition;text analysis","speech transcription;downstream text processing;information extraction;machine translation;English broadcast news;linguistics;automatic speech recognition systems","","3","","11","","26 May 2009","","","IEEE","IEEE Conferences"
"An Undergraduate Curriculum for Deep Learning","G. Tirkeş; C. Ç. Ekin; G. engul; A. Bostan; M. Karakaya","Department of Computer Engineering, Atilim University, Ankara, Turkey; Department of Computer Engineering, Atilim University, Ankara, Turkey; Department of Computer Engineering, Atilim University, Ankara, Turkey; Department of Computer Engineering, Atilim University, Ankara, Turkey; Department of Computer Engineering, Atilim University, Ankara, Turkey","2018 3rd International Conference on Computer Science and Engineering (UBMK)","9 Dec 2018","2018","","","604","609","Deep Learning (DL) is an interesting and rapidly developing field of research which has been currently utilized as a part of industry and in many disciplines to address a wide range of problems, from image classification, computer vision, video games, bioinformatics, and handwriting recognition to machine translation. The starting point of this study is the recognition of a big gap between the sector need of specialists in DL technology and the lack of sufficient education provided by the universities. Higher education institutions are the best environment to provide this expertise to the students. However, currently most universities do not provide specifically designed DL courses to their students. Thus, the main objective of this study is to design a novel curriculum including two courses to facilitate teaching and learning of DL topic. The proposed curriculum will enable students to solve real-world problems by applying DL approaches and gain necessary background to adapt their knowledge to more advanced, industry-specific fields.","","978-1-5386-7893-0","10.1109/UBMK.2018.8566575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8566575","Deep Learning;curriculum design;Machine Learning;Deep Neural Networks;Engineering Education","Handheld computers;Engineering education;Recurrent neural networks;Feature extraction","computer aided instruction;educational courses;educational institutions;further education;image classification;learning (artificial intelligence);teaching","computer vision;video games;handwriting recognition;machine translation;universities;higher education institutions;teaching;undergraduate curriculum;image classification;deep learning;bioinformatics;DL technology;industry-specific fields","","","","25","","9 Dec 2018","","","IEEE","IEEE Conferences"
"Improving Computer-Aided Detection Using Convolutional Neural Networks and Random View Aggregation","H. R. Roth; L. Lu; J. Liu; J. Yao; A. Seff; K. Cherry; L. Kim; R. M. Summers","Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD, USA","IEEE Transactions on Medical Imaging","3 May 2016","2016","35","5","1170","1181","Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities ~ 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.","1558-254X","","10.1109/TMI.2015.2482920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279156","Computer aided diagnosis;computed tomography;medical diagnostic imaging;machine learning;object detection;artificial neural networks;multi-layer neural network;deep learning","Computed tomography;Lymph nodes;Training;Colonic polyps;Feature extraction;Three-dimensional displays","computerised tomography;image classification;learning (artificial intelligence);medical image processing;neural nets;probability","computed tomography;medical imaging;colonic polyp detection;lymph node detection;sclerotic metastasis detection;false positives;classification probability;deep convolutional neural network classifier training;random rotations;random translations;scale transformations;two-tiered coarse-to-fine cascade framework;random view aggregation;computer-aided detection","Adolescent;Adult;Aged;Child;Colonic Polyps;Databases, Factual;Female;Humans;Lymph Nodes;Machine Learning;Male;Middle Aged;Neural Networks (Computer);Radiographic Image Interpretation, Computer-Assisted;Spinal Neoplasms;Tomography, X-Ray Computed;Young Adult","274","7","60","","28 Sep 2015","","","IEEE","IEEE Journals"
"Support Vector Shape: A Classifier-Based Shape Representation","H. Van Nguyen; F. Porikli","University of Maryland, College Park; Mitsubishi Electric Research Labs, Cambridge","IEEE Transactions on Pattern Analysis and Machine Intelligence","15 Feb 2013","2013","35","4","970","982","We introduce a novel implicit representation for 2D and 3D shapes based on Support Vector Machine (SVM) theory. Each shape is represented by an analytic decision function obtained by training SVM, with a Radial Basis Function (RBF) kernel so that the interior shape points are given higher values. This empowers support vector shape (SVS) with multifold advantages. First, the representation uses a sparse subset of feature points determined by the support vectors, which significantly improves the discriminative power against noise, fragmentation, and other artifacts that often come with the data. Second, the use of the RBF kernel provides scale, rotation, and translation invariant features, and allows any shape to be represented accurately regardless of its complexity. Finally, the decision function can be used to select reliable feature points. These features are described using gradients computed from highly consistent decision functions instead from conventional edges. Our experiments demonstrate promising results.","1939-3539","","10.1109/TPAMI.2012.186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6291722","Shape matching;2D and 3D representation;support vector machines","Shape;Support vector machines;Kernel;Training;Noise;Robustness;Vectors","computer graphics;feature extraction;image classification;image matching;image representation;radial basis function networks;shape recognition;support vector machines","support vector shape;classifier-based shape representation;2D shape representation;3D shape representation;support vector machine;analytic decision function;SVM training;radial basis function kernel;interior shape points;sparse feature point subset;discriminative power improvement;RBF kernel;scale invariant features;rotation invariant features;translation invariant features","Humans;Image Processing, Computer-Assisted;Motion;Pattern Recognition, Automated;Support Vector Machines;Video Recording","27","","61","","30 Aug 2012","","","IEEE","IEEE Journals"
"Classification of Atrial Fibrillation and Acute Decompensated Heart Failure Using Smartphone Mechanocardiography: A Multilabel Learning Approach","S. Mehrang; O. Lahdenoja; M. Kaisti; M. J. Tadi; T. Hurnanen; A. Airola; T. Knuutila; J. Jaakkola; S. Jaakkola; T. Vasankari; T. Kiviniemi; J. Airaksinen; T. Koivisto; M. Pänkäälä","Department of Future Technologies, University of Turku, Turku, Finland; Department of Future Technologies, University of Turku, Turku, Finland; Department of Future Technologies, University of Turku, Turku, Finland; Department of Future Technologies, University of Turku, Turku, Finland; Department of Future Technologies, University of Turku, Turku, Finland; Department of Future Technologies, University of Turku, Turku, Finland; Department of Future Technologies, University of Turku, Turku, Finland; Heart Center, Turku University Hospital, Turku, Finland; Heart Center, Turku University Hospital, Turku, Finland; Heart Center, Turku University Hospital, Turku, Finland; Heart Center, Turku University Hospital, Turku, Finland; Heart Center, Turku University Hospital, Turku, Finland; Department of Future Technologies, University of Turku, Turku, Finland; Department of Future Technologies, University of Turku, Turku, Finland","IEEE Sensors Journal","18 Jun 2020","2020","20","14","7957","7968","Timely diagnosis of cardiovascular diseases (CVD) is crucial to prevent morbidity and mortality. Atrial fibrillation (AFib) and heart failure (HF) are two prevalent cardiac disorders that are associated with a high risk of morbidity and mortality, especially if they are concurrently present. Current approaches fail to screen many at-risk individuals who would benefit from preventive treatment; while others receive unnecessary interventions. An effective approach to the detection of CVDs is mechanocardiography (MCG) by which translational and rotational precordial chest movements are monitored. In this study, we collected MCG data from a study sample of 300 hospitalized cardiac patients using multidimensional built-in inertial sensors of a smartphone. Our main objective was to detect concurrent AFib and acute decompensated HF (ADHF) using smartphone MCG (or sMCG). To this end, we adopted a supervised machine learning classification using multi-label and hierarchical classification. Logistic regression, random forest, and extreme gradient boosting were used as candidate classifiers. The results of the analysis showed the area under the receiver operating characteristic curve values of 0.98 and 0.85 for AFib and ADHF, respectively. The highest percentages of positive and negative predictive values for AFib were 91.9 and 100; while for ADHF, they were 56.9 and 88.4 for the multi-label classification and 69.9 and 68.8 for the hierarchical classification, respectively. We conclude that using a single sMCG measurement, AFib can be detected accurately whereas ADHF can be detected with moderate certainty.","1558-1748","","10.1109/JSEN.2020.2981334","Academy of Finland; Finnish Foundation for Technology Promotion; Nokia Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9039560","Acute decompensated heart failure;atrial fibrillation;gyrocardiography;machine learning;seismocardiography;smartphone mechanocardiography","Heart;Feature extraction;Sensor phenomena and characterization;Electrocardiography;Hospitals;Hafnium","cardiovascular system;diseases;medical signal processing;mobile computing;patient diagnosis;patient monitoring;pattern classification;pneumodynamics;regression analysis;smart phones;supervised learning","cardiovascular diseases;cardiac disorders;translational chest movements;rotational precordial chest movements;inertial sensors;ADHF;hierarchical classification;extreme gradient boosting;atrial fibrillation;acute decompensated heart failure;smartphone mechanocardiography;multilabel learning;supervised machine learning classification;logistic regression;random forest","","","","71","CCBY","17 Mar 2020","","","IEEE","IEEE Journals"
"High Dimensional Abnormal Human Activity Recognition Using Histogram Oriented Gradients and Zernike Moments","C. Dhiman; D. K. Vishwakarma","Department of Electronics and Communcation Engineering, Delhi Technological University, Delhi, 110042, India; Department of Electronics and Communcation Engineering, Delhi Technological University, Delhi, 110042, India","2017 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)","8 Nov 2018","2017","","","1","4","In this paper a robust abnormal human activity recognition framework is proposed which goals to recognize any unusual activity to the elderly people and to strengthen the concept of independent and quality living. The framework is structured to construct a robust feature vector by computing integrated feature vector: Histogram of Oriented Gradients (HOG) and Zernike moments on Average Energy Images (AEI). Formation of AEI images provides a compact representation of the video sequences without any Spatio-temporal loss of information. Integration of HOG and Zernike moments augments inter-class separation and translational and rotational invariance. The depth silhouettes are acquired by Microsoft's Kinect sensor which are used to generate clean binary silhouettes by background subtraction making the pre-processing faster and simpler with accuracy. The combined feature vector dimensions are reduced by applying PCA and SVM is applied to classify the activities. The proposed work is validated on publically available UR fall detection and Kinect Activity Recognition Dataset (KARD) 3D dataset. The experiments exhibit impressive results with The average recognition accuracy achieved on these datasets are 94% and 95.22% ARA for UR fall dataset, and KARD dataset, respectively.","2473-943X","978-1-5090-6621-6","10.1109/ICCIC.2017.8524372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8524372","Binary silhouette;Depth images;Average Energy Image(AEI);Zernike Moments;Support Vetcor Machine","Activity recognition;Support vector machines;Histograms;Senior citizens;Shape;Accelerometers;Medical services","feature extraction;geriatrics;image classification;image representation;image sensors;image sequences;object detection;principal component analysis;support vector machines;video signal processing","high dimensional abnormal human Activity Recognition;robust abnormal human activity recognition framework;elderly people;independent quality living;robust feature vector;integrated feature vector;HOG;Average Energy Images;AEI images;inter-class separation;rotational invariance;depth silhouettes;clean binary silhouettes;combined feature vector dimensions;average recognition accuracy;histogram of oriented gradients;Zernike moments;Microsoft Kinect sensor;Kinect activity recognition 3D dataset;video sequences;spatio-temporal information loss;background subtraction;activities classification;KARD 3D dataset;UR fall dataset;translational invariance;compact representation;PCA","","2","","31","","8 Nov 2018","","","IEEE","IEEE Conferences"
"Multi-Cycle-Consistent Adversarial Networks for CT Image Denoising","J. Liu; Y. Ding; J. Xiong; Q. Jia; M. Huang; J. Zhuang; B. Xie; C. Liu; Y. Shi","University of Notre Dame,Department of Computer Science and Engineering,USA; University of Notre Dame,Department of Computer Science and Engineering,USA; IBM Thomas J. Watson Research Center,USA; Guangdong General Hospital,China; Guangdong General Hospital,China; Guangdong General Hospital,China; Kneron Inc.,USA; Kneron Inc.,USA; University of Notre Dame,Department of Computer Science and Engineering,USA","2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)","22 May 2020","2020","","","614","618","CT image denoising can be treated as an image-to-image translation task where the goal is to learn the transform between a source domain X (noisy images) and a target domain Y (clean images). Recently, cycle-consistent adversarial denoising network (CCADN) has achieved state-of-the-art results by enforcing cycle-consistent loss without the need of paired training data. Our detailed analysis of CCADN raises a number of interesting questions. For example, if the noise is large leading to significant difference between domain X and domain Y, can we bridge X and Y with an intermediate domain Z such that both the denoising process between X and Z and that between Z and Y are easier to learn? As such intermediate domains lead to multiple cycles, how do we best enforce cycle-consistency? Driven by these questions, we propose a multi-cycle-consistent adversarial network (MCCAN) that builds intermediate domains and enforces both local and global cycle-consistency. The global cycle-consistency couples all generators together to model the whole denoising process, while the local cycle-consistency imposes effective supervision on the process between adjacent domains. Experiments show that both local and global cycle-consistency are important for the success of MCCAN, which outperforms the state-of-the-art.","1945-8452","978-1-5386-9330-8","10.1109/ISBI45749.2020.9098683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098683","Machine learning;Image enhancement/restoration (noise and artifact reduction);Computed tomography (CT);Multi-cycle-consistency","Computed tomography;Training;Noise reduction;Generators;Noise measurement;Transforms;Image denoising","computerised tomography;image denoising;medical image processing","multicycle-consistent adversarial network;CT image denoising;image-to-image translation task;source domain;noisy images;clean images;cycle-consistent adversarial denoising network;CCADN;cycle-consistent loss;intermediate domain;intermediate domains;multiple cycles;global cycle-consistency couples;local cycle-consistency;adjacent domains;target domain","","1","","12","","22 May 2020","","","IEEE","IEEE Conferences"
"AI in Medical Imaging Informatics: Current Challenges and Future Directions","A. S. Panayides; A. Amini; N. D. Filipovic; A. Sharma; S. A. Tsaftaris; A. Young; D. Foran; N. Do; S. Golemati; T. Kurc; K. Huang; K. S. Nikita; B. P. Veasey; M. Zervakis; J. H. Saltz; C. S. Pattichis","Department of Computer Science, University of Cyprus, Nicosia, Cyprus; Electrical and Computer Engineering Department, University of Louisville, Louisville, KY, USA; University of Kragujevac, Kragujevac, Serbia; Emory University Atlanta, GA, USA; School of Engineering, The University of Edinburgh, U.K.; Department of Anatomy and Medical Imaging, University of Auckland, Auckland, New Zealand; Department of Pathology and Laboratory Medicine, Robert Wood Johnson Medical School, Rutgers, The State University of New Jersey, Piscataway, NJ, USA; U.S. Department of Veterans Affairs Boston Healthcare System, Boston, MA, USA; Medical School, National and Kapodistrian University of Athens, Athens, Greece; Stony Brook University, Stony Brook, NY, USA; School of Medicine, Regenstrief Institute, Indiana University, IN, USA; Biomedical Simulations and Imaging Lab, School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Electrical and Computer Engineering Department, University of Louisville, Louisville, KY, USA; Technical University of Crete, Chania, Crete, Greece; Stony Brook University, Stony Brook, NY, USA; Department of Computer Science of the University of Cyprus, Nicosia, Cyprus","IEEE Journal of Biomedical and Health Informatics","1 Jul 2020","2020","24","7","1837","1857","This paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.","2168-2208","","10.1109/JBHI.2020.2991043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103969","Medical Imaging;Image Analysis;Image Classification;Image Processing;Image Segmentation;Image Visualization;Integrative Analytics;Machine Learning;Deep Learning;Big Data","Biomedical imaging;X-ray imaging;Magnetic resonance imaging;Computed tomography;Informatics;Three-dimensional displays","Big Data;biological organs;biological tissues;data analysis;data visualisation;diagnostic radiography;diseases;health care;image classification;image reconstruction;image segmentation;learning (artificial intelligence);medical image processing;neural nets","big healthcare data analytics;disease classification;clinical benefits;in-silico modelling;visualization applications;medical imaging informatics;clinical translation;clinical practice;medical imaging acquisition technologies;medical data management strategies;organ segmentation;tissue segmentation;deep learning architectures;3D reconstruction;radiology applications;digital pathology applications","Artificial Intelligence;Big Data;Diagnostic Imaging;Humans;Image Interpretation, Computer-Assisted;Image Processing, Computer-Assisted;Medical Informatics;Precision Medicine","11","","268","CCBY","29 May 2020","","","IEEE","IEEE Journals"
"Robust implementation of hand gesture recognition for remote human-machine interaction","J. Dulayatrakul; P. Prasertsakul; T. Kondo; I. Nilkhamhang","School of Information, Computer, and Communication Technology, Sirindhorn International Institute of Technology, Thammasat University; School of Information, Computer, and Communication Technology, Sirindhorn International Institute of Technology, Thammasat University; School of Information, Computer, and Communication Technology, Sirindhorn International Institute of Technology, Thammasat University; School of Information, Computer, and Communication Technology, Sirindhorn International Institute of Technology, Thammasat University","2015 7th International Conference on Information Technology and Electrical Engineering (ICITEE)","18 Feb 2016","2015","","","247","252","A robust hand gesture recognition algorithm for remote human-machine interaction is proposed that has been optimized for implementation on an embedded platform. Hue-saturation-value (HSV) thresholding and unit-gradient vector (UGV) background subtraction methods are employed to overcome common issues related to variations in lighting conditions. Top-hat transformation is used to detect fingers and hand gestures, which are translated to command inputs for remotely controlling a media player. Experimental results demonstrate that the algorithm performs efficiently and accurately on an embedded board with an average computational cost of 143 millisecond per gesture and is robust to changes in illumination.","","978-1-4673-7863-5","10.1109/ICITEED.2015.7408950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408950","","Thumb;Gesture recognition;Lighting;Cameras;Image segmentation;Robustness;Image color analysis","gesture recognition;human computer interaction;image colour analysis;image segmentation;object detection;transforms","robust hand gesture recognition algorithm;remote human-machine interaction;hue-saturation-value thresholding method;unit-gradient vector background subtraction method;HSV;UGV;lighting conditions;top-hat transformation;finger gesture detection;media player remote control","","4","","11","","18 Feb 2016","","","IEEE","IEEE Conferences"
"Real-time decoding algorithm in brain machine interfaces","X. Zheng; Q. Zhang; S. Zhang; Y. Yu; W. Chen; Y. Zhao; S. Lin","Qiushi Academy for Advanced Studies and the Biomedical Engineering Department, Zhejiang University, Hangzhou, 310027 China; Qiushi Academy for Advanced Studies and the Biomedical Engineering Department, Zhejiang University, Hangzhou, 310027 China; Qiushi Academy for Advanced Studies and the Biomedical Engineering Department, Zhejiang University, Hangzhou, 310027 China; Qiushi Academy for Advanced Studies and the Biomedical Engineering Department, Zhejiang University, Hangzhou, 310027 China; Qiushi Academy for Advanced Studies and the College of Computer Science, Zhejiang University, Hangzhou, 310027 China; Qiushi Academy for Advanced Studies and the Biomedical Engineering Department, Zhejiang University, Hangzhou, 310027 China; Qiushi Academy for Advanced Studies and the Biomedical Engineering Department, Zhejiang University, Hangzhou, 310027 China","IEEE/ICME International Conference on Complex Medical Engineering","26 Aug 2010","2010","","","79","84","The technology of invasive Brain-Computer Interfaces (BCIs) has been developed in last decades, for its possibility to restore motor function of the disabilities. The main task of BCI system is to translate the cortical neural activities into commands of direct brain-controlled prosthetic devices. To study how cortical signals simultaneously recorded from primary motor cortex (M1) neurons were used for external devices control, invasive brain-machine interfaces in rat were investigated. In these experiments, rats were trained to control a relay to obtain water by pressing a lever over a pressure threshold. Microwire array was implanted in rat's motor cortex to record neural activities, and pressure was recorded by a pressure sensor. After spike detecting and sorting, totally 22-58 neurons were found in all 15 channels per rat (except the reference electrode in the array). To compute the firing rate of individual unit, the numbers of spikes in a time bin (Δt =100ms) were counted. Meanwhile, the pressure signal was also computed into bin size (Δt =100ms), by averaging pressure value in the bin period. After that some decoders were designed to make mapping between neural activities and pressure, such as Optimum Linear Estimation(OLE), Kalman filter (KF), and Kernel-Based Membership combined with Curve Fitting algorithm (KMCF). These decoders can be easily learned using a few minutes of training data and provides real-time estimates of hand position every 100ms given the firing rates of neurons in motor cortex. The performances of these decoders were evaluated by Correlation Coefficient (CC) between real and predictive pressure value. The results showed that the KMCF decoder performed best in these experiment, which had higher CC than others (CC=0.94). The higher CC indicated that the activity of motor cortex (M1) neurons can be used for detection of the corresponding movement states and estimation of continuous kinematic parameters. The rats could use their neural activities to directly control the relay and successfully get rewards after about one week of training. In further, with the development of the motor-related BCIs technology, BCIs will be possibly used to motor function restoration of paralytics and greatly rehabilitate their capacity of life. Furthermore, the experiment results provide insights into the nature of the neural coding of movement.","","978-1-4244-6843-0","10.1109/ICCME.2010.5558867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5558867","","Real time systems;Sorting","brain;brain-computer interfaces;handicapped aids;Kalman filters;medical signal processing;neurophysiology;pressure sensors","real-time decoding algorithm;cortical signals;primary motor cortex neurons;external device control;invasive brain-machine interfaces;water;pressure threshold;microwire array;neural activity;pressure sensor;firing rate;optimum linear estimation;Kalman filter;kernel-based membership;curve fitting algorithm;correlation coefficient;KMCF decoder;continuous kinematic parameters;motor-related BCI technology;motor function restoration;paralytics;movement neural coding;spike detection;spike sorting","","","","9","","26 Aug 2010","","","IEEE","IEEE Conferences"
"Physiological Artifacts and the Implications for Brain-Machine-Interface Design","M. M. Sorkhabi; M. Benjaber; P. Brown; T. Denison","University of Oxford,MRC Brain Network Dynamics Unit,Oxford,UK; University of Oxford,MRC Brain Network Dynamics Unit,Oxford,UK; University of Oxford,MRC Brain Network Dynamics Unit,Oxford,UK; University of Oxford,MRC Brain Network Dynamics Unit and Department of Engineering Science,Oxford,UK","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","1498","1498","The accurate measurement of brain activity by Brain-Machine-Interfaces (BMI) and closed-loop Deep Brain Stimulators (DBS) is one of the most important steps in communicating between the brain and subsequent processing blocks. In conventional chest-mounted systems, frequently used in DBS, a significant amount of artifact can be induced in the sensing interface, often as a common-mode signal applied between the case and the sensing electrodes. Attenuating this common-mode signal can be a serious challenge in these systems due to finite common-mode-rejection-ratio (CMRR) capability in the interface. Emerging BMI and DBS devices are being developed which can mount on the skull. Mounting the system on the cranial region can potentially suppress these induced physiological signals by limiting the artifact amplitude. In this study, we model the effect of artifacts by focusing on cardiac activity, using a current- source dipole model in a torso-shaped volume conductor. Performing finite element simulation with the different DBS architectures, we estimate the ECG common mode artifacts for several device architectures. Using this model helps define the overall requirements for the total system CMRR to maintain resolution of brain activity. The results of the simulations estimate that the cardiac artifacts for skull-mounted systems will have a significantly lower effect than non-cranial systems that include the pectoral region. It is expected that with a pectoral mounted device, a minimum of 60-80 dB CMRR is required to suppress the ECG artifact, depending on device placement relative to the cardiac dipole, while in cranially mounted devices, a 0 dB CMRR is sufficient, in the worst-case scenario. In addition, the model suggests existing commercial devices could optimize performance with a right-hand side placement. The methods used for estimating cardiac artifacts can be extended to other sources such as motion/muscle sources. The susceptibility of the device to artifacts has significant implications for the practical translation of closed-loop DBS and BMI, including the choice of biomarkers, the system design requirements, and the surgical placement of the device relative to artifact sources.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283328","Royal Academy of Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283328","Deep brain stimulation;Cranial mounted DBS;ECG artifact;Current- source dipole model;Finite element method","Performance evaluation;Solid modeling;Satellite broadcasting;Brain modeling;Physiology;Sensors;Finite element analysis","biomedical electrodes;closed loop systems;electrocardiography;finite element analysis;medical signal processing;neurophysiology","ECG artifact;cardiac dipole;mounted devices;closed-loop DBS;artifact sources;DBS architectures;closed-loop deep brain stimulators;current-source dipole model;pectoral mounted device;pectoral region;noncranial systems;skull-mounted systems;cardiac artifacts;total system CMRR;ECG common mode artifacts;finite element simulation;torso-shaped volume conductor;cardiac activity;artifact amplitude;induced physiological signals;cranial region;DBS devices;common-mode-rejection-ratio capability;sensing electrodes;common-mode signal;sensing interface;conventional chest-mounted systems;subsequent processing blocks;BMI;brain activity;brain-machine-interface design;physiological artifacts","","","","31","","14 Dec 2020","","","IEEE","IEEE Conferences"
"Implicit language identification system based on random forest and support vector machine for speech","M. Gupta; S. S. Bharti; S. Agarwal","Computer Science & Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India; Computer Science & Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India; Computer Science & Engineering Department, Motilal Nehru National Institute of Technology, Allahabad, India","2017 4th International Conference on Power, Control & Embedded Systems (ICPCES)","23 Nov 2017","2017","","","1","6","Speech uttered by the human beings contains the information about speakers, languages and contents. Language of uttered speech can easily be identified by extracting the language specific information from it. Identification of language of speech is known as Language Identification (LID). Identification of language from speech is helpful in its translation, speech recognition and speech activated automatic systems. LID system may also play an important role in speaker recognition as identification of language can be used to reduce search space. In this paper an approach based on Linear Predictive Coding (LPC) and Mel Frequency Cepstral Coefficients (MFCCs) features for language identification is proposed using SVM and Random Forest (RF) classification techniques. Both LPC and MFCC features are vocal tract features. LPC and MFCC features extracted from uttered speech contain language as well as speaker related informations. Identification of language highly depends upon extraction of language specific features. Both these vocal tract parameters of speech contain lot of information about languages spoken compared to other parameters like excitation source parameters and prosodic parameters. Hence combination of these features performs better than individual. Experiments have been performed on the database obtained from IIIT-Hyderabad consisting of 5000 multilingual clean speech signals (Hindi, Bengali, Telugu, Tamil, Marathi and Malayalam). For training the proposed model, 600 speech signals are taken arbitrarily from the above database. Language model are created for each language. Evaluation of the proposed models has been made using other 300 speech signals from same database. Language models are evaluated using individual features as well as combined features. Experiments performed by taking both features at a time give better result as compared to taking individual features one at a time. Using these features, the accuracy of language identification is not more than 80% so far as claimed by other researchers. In the proposed approach, the accuracy of language identification is improved to 92.6% using combination of same features and random forest model.","","978-1-5090-4426-9","10.1109/ICPCES.2017.8117624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117624","MFCCs;SVM;LPC;Random Forest(RF)","Speech;Support vector machines;Mel frequency cepstral coefficient;Feature extraction;Databases;Hidden Markov models;Training","cepstral analysis;encoding;feature extraction;linear predictive coding;natural language processing;signal classification;speaker recognition;speech recognition;support vector machines","language specific information;speech recognition;MFCC;speech signals;language identification system;speech activated automatic systems;linear predictive coding;LPC;mel frequency cepstral coefficients;SVM;random forest classification;RF classification;support vector machine;LID","","1","","14","","23 Nov 2017","","","IEEE","IEEE Conferences"
"Comparing Manual and Machine Annotations of Emotions in Non-acted Speech","G. Deshpande; V. S. Viraraghavan; M. Duggirala; R. R. Vempada; S. Patel","TCS Research & Innovation, India; TCS Research & Innovation, India; TCS Research & Innovation, India; TCS Research & Innovation, India; TCS Research & Innovation, India","2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","28 Oct 2018","2018","","","4241","4244","Psychological well-being at the workplace has increased the demand for detecting emotions with higher accuracies. Speech, one of the most non-obtrusive modes of capturing emotions at the workplace, is still in need of robust emotion annotation mechanisms for non-acted speech corpora. In this paper, we extend our experiments on our non-acted speech database in two ways. First, we report how participants themselves perceive the emotion in their voice after a long gap of about six months, and how a third person, who has not heard the clips earlier, perceives the emotion in the same utterances. Both annotators also rated the intensity of the emotion. They agreed better in neutral (84%) and negative clips (74%) than in positive ones (38%). Second, we restrict our attention to those samples that had agreement and show that the classification accuracy of 80% by machine learning, an improvement of 7% over the state-of-the-art results for speaker-dependent classification. This result suggests that the high-level perception of emotion does translate to the low-level features of speech. Further analysis shows that the silently expressed positive and negative emotions are often misinterpreted as neutral. For the speaker-independent test set, we report an overall accuracy of 61%.","1558-4615","978-1-5386-3646-6","10.1109/EMBC.2018.8513230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513230","Emotion recognition;Non-acted Speech;Selfreported emotions;Perceived emotions;Inter-annotator agreement","Databases;Employment;Psychology;Mel frequency cepstral coefficient;Manuals;Frequency-domain analysis;Correlation","emotion recognition;learning (artificial intelligence);psychology;speech processing;speech recognition","silently expressed positive emotions;speech database;nonacted speech corpora;robust emotion annotation mechanisms","Emotions;Humans;Machine Learning;Speech;Speech Perception;Voice","1","","12","","28 Oct 2018","","","IEEE","IEEE Conferences"
"Implementation of geometric hashing on the Connection Machine","I. Rigoutsos; R. Hummel","New York Univ., NY, USA; New York Univ., NY, USA","[1991 Proceedings] Workshop on Directions in Automated CAD-Based Vision","6 Aug 2002","1991","","","76","84","The authors report on a scalable implementation of geometric hashing on the Connection Machine. Using this implementation, it is possible to recognize models consisting of patterns of points embedded in scenes, independent of translation, rotation, and scale changes. With 1024 models and a scene of 200 points, the implementation yields an execution time of 70 milliseconds for recognition of the one embedded model given a two-point basis set (consisting of a pair of points in the model) on a 64 K-processor CM-2 parallel computer. The algorithm is scalable, yielding an expected execution time that is O(log/sup 2/ M+log S log M) on a Mn/sup 3/-processor hypercube-connected SIMD machine such as the Connection Machine; M is the number of models, n is the number of points per model, S is the number of scene points, and it is assumed that execution proceeds until the first model is discovered.<>","","0-8186-2147-8","10.1109/CADVIS.1991.148760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=148760","","Layout;Solid modeling;Data structures;Pattern recognition;Machine vision;Computational efficiency;Parallel processing;Telephony;Network servers;Digital images","computer vision;computerised pattern recognition;computerised picture processing;file organisation;parallel processing;performance evaluation;solid modelling","hypercube;computer vision;geometric hashing;Connection Machine;CM-2;parallel computer;execution time;SIMD;70 ms","","12","","13","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Decoding Transition between Kinematics Stages for Brain-Machine Interface*","S. Chen; X. Zhang; X. Shen; Y. Huang; Y. Wang","Hong Kong University of Science and Technology,Program of Bioengineering; Hong Kong University of Science and Technology,department of Electronic and Computer Engineering; Hong Kong University of Science and Technology,department of Electronic and Computer Engineering; Hong Kong University of Science and Technology,department of Electronic and Computer Engineering; Hong Kong University of Science and Technology,department of Chemical and Biological Engineering","2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)","28 Nov 2019","2019","","","3592","3597","Brain-machine interfaces (BMIs) translate the neural activity into digital command to control external devices in accomplishing movement task, which could involve multiple stages of behaviors in sequence. Previous work generally discriminates the stage labels using a classifier and uses a combination of sub-decoders designed respectively for each stage. Without considering the time dynamics of neural activity, the classifier often introduces noisy estimation in stage prediction. Brain-controlling neuro-prosthesis requires the decoder to continuously output the kinematics interpretation on brain state for each time instance, including when to start or end each stage within the task smoothly and accurately. We propose to decode the kinematic states within stages and during stage transition in one paradigm. The kinematics state vector is extended, and the time dynamics is modelled in the state-observation framework. We validate our approach on a brain-controlled lever discrimination task. The rats need to adjust the neural activity to press the correct virtual lever and drives the brain state to trigger the start of the next trial. Compared with the existing method whose mean square error is 0.8038, our results show smoother transition prediction and better decoding accuracy with less mean square error which is 0.7922. And the correction rate of holding lever and rest is above 90%. These results help the subjects do the task with even less response time (2.81s) and shorter inter-trial duration (4.5s).","2577-1655","978-1-7281-4569-3","10.1109/SMC.2019.8914285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8914285","","Decoding;Kinematics;Pressing;Neural activity;Task analysis;Rats;Human factors","brain;brain-computer interfaces;medical signal processing;neurophysiology;prosthetics","kinematics stages;brain-machine interface;neural activity;digital command;movement task;stage labels;classifier;subdecoders;time dynamics;stage prediction;brain-controlling neuro-prosthesis;decoder;kinematics interpretation;brain state;time instance;kinematic states;stage transition;kinematics state vector;state-observation framework;brain-controlled lever discrimination task;mean square error;smoother transition prediction;decoding accuracy;response time;time 2.81 s;time 4.5 s","","3","","12","","28 Nov 2019","","","IEEE","IEEE Conferences"
"Decoding the non-stationary neuron spike trains by dual Monte Carlo point process estimation in motor Brain Machine Interfaces","Y. Liao; H. Li; Q. Zhang; G. Fan; Y. Wang; X. Zheng","Qiushi Academy for Advanced Studies and Department of Biomedical Engineering, Zhejiang University, Hangzhou 310027, China; Qiushi Academy for Advanced Studies and Department of Biomedical Engineering, Zhejiang University, Hangzhou 310027, China; Qiushi Academy for Advanced Studies and Department of Biomedical Engineering, Zhejiang University, Hangzhou 310027, China; Qiushi Academy for Advanced Studies and Department of Biomedical Engineering, Zhejiang University, Hangzhou 310027, China; Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou 310027, China; Qiushi Academy for Advanced Studies and Department of Biomedical Engineering, Zhejiang University, Hangzhou 310027, China","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","6 Nov 2014","2014","","","6513","6516","Decoding algorithm in motor Brain Machine Interfaces translates the neural signals to movement parameters. They usually assume the connection between the neural firings and movements to be stationary, which is not true according to the recent studies that observe the time-varying neuron tuning property. This property results from the neural plasticity and motor learning etc., which leads to the degeneration of the decoding performance when the model is fixed. To track the non-stationary neuron tuning during decoding, we propose a dual model approach based on Monte Carlo point process filtering method that enables the estimation also on the dynamic tuning parameters. When applied on both simulated neural signal and in vivo BMI data, the proposed adaptive method performs better than the one with static tuning parameters, which raises a promising way to design a long-term-performing model for Brain Machine Interfaces decoder.","1558-4615","978-1-4244-7929-0","10.1109/EMBC.2014.6945120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945120","","Tuning;Neurons;Decoding;Estimation;Trajectory;Monte Carlo methods;Data models","brain-computer interfaces;decoding;filtering theory;medical signal processing;Monte Carlo methods;neurophysiology;parameter estimation","nonstationary neuron spike trains;dual Monte Carlo point process estimation;motor brain machine interfaces;decoding algorithm;neural signals;movement parameters;neural firings;time-varying neuron tuning property;neural plasticity;motor learning;decoding performance;nonstationary neuron tuning;Monte Carlo point process filtering method;dynamic tuning parameters estimation;BMI data;adaptive method;static tuning parameters;brain machine interfaces decoder","Action Potentials;Algorithms;Animals;Brain-Computer Interfaces;Haplorhini;Humans;Monte Carlo Method;Motor Cortex;Movement;Neurons;Signal Processing, Computer-Assisted","1","","25","","6 Nov 2014","","","IEEE","IEEE Conferences"
"Machine learning model for sign language interpretation using webcam images","K. Dabre; S. Dholay","Department of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India; Department of Computer Engineering, Sardar Patel Institute of Technology, Mumbai, India","2014 International Conference on Circuits, Systems, Communication and Information Technology Applications (CSCITA)","19 Jun 2014","2014","","","317","321","Human beings interact with each other either using a natural language channel such as words, writing, or by body language (gestures) e.g. hand gestures, head gestures, facial expression, lip motion and so on. As understanding natural language is important, understanding sign language is also very important. The sign language is the basic communication method within hearing disable people. People with hearing disabilities face problems in communicating with other hearing people without a translator. For this reason, the implementation of a system that recognize the sign language would have a significant benefit impact on deaf people social live. In this paper, we have proposed a marker-free, visual Indian Sign Language recognition system using image processing, computer vision and neural network methodologies, to identify the characteristics of the hand in images taken from a video trough web camera. This approach will convert video of daily frequently used full sentences gesture into a text and then convert it into audio. Identification of hand shape from continuous frames will be done by using series of image processing operations. Interpretation of signs and corresponding meaning will be identified by using Haar Cascade Classifier. Finally displayed text will be converted into speech using speech synthesizer.","","978-1-4799-2494-3","10.1109/CSCITA.2014.6839279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839279","Indian Sign Language (ISL);Computer Vision(CV)","Assistive technology;Gesture recognition;Training;Feature extraction;Cameras;Shape","computer vision;handicapped aids;image classification;learning (artificial intelligence);neural nets;palmprint recognition;video cameras","machine learning model;sign language interpretation;Webcam images;natural language channel;hearing disability;marker-free visual Indian sign language recognition system;image processing;computer vision;neural network methodology;hand characteristic identification;video camera;hand shape identification;continuous frames;Haar cascade classifier;speech synthesizer","","15","","11","","19 Jun 2014","","","IEEE","IEEE Conferences"
"Novel equilibrium-point control of agonist-antagonist system with pneumatic artificial muscles: II. Application to EMG-based human-machine interface for an elbow-joint system","Y. Ariga; D. Maeda; H. T. T. Pham; M. Uemura; H. Hirai; F. Miyazaki","Department of Mechanical Science and Bioengineering, Graduate School of Engineering Science, Osaka University, Japan; Department of Mechanical Science and Bioengineering, Graduate School of Engineering Science, Osaka University, Japan; Department of Mechanical Science and Bioengineering, Graduate School of Engineering Science, Osaka University, Japan; Department of Mechanical Science and Bioengineering, Graduate School of Engineering Science, Osaka University, Japan; Department of Mechanical Science and Bioengineering, Graduate School of Engineering Science, Osaka University, Japan; Department of Mechanical Science and Bioengineering, Graduate School of Engineering Science, Osaka University, Japan","2012 IEEE/RSJ International Conference on Intelligent Robots and Systems","20 Dec 2012","2012","","","4380","4385","This paper presents an electromyographic-based human-machine interface for the agonist-antagonist system with two pairs of pneumatic artificial muscles (PAMs) that replicates the human elbow-joint system. We introduce the novel concepts of agonist-antagonist muscle-pair ratio (A-A ratio) and agonist-antagonist muscle-pair activity (A-A activity) to link the human muscle system to the PAM system, and we propose a linear control method translating the equilibrium point of human muscle system into that of PAM system. The human-robot experiment demonstrates the validity of the proposed method.","2153-0866","978-1-4673-1736-8","10.1109/IROS.2012.6385790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6385790","","Humans;Muscles;Electromyography;Joints;Robots;Elbow;Mathematical model","electromyography;human-robot interaction;intelligent robots;medical signal processing","novel equilibrium-point control;agonist-antagonist system;pneumatic artificial muscles;EMG-based human-machine interface;electromyographic-based human-machine interface;PAM;human elbow-joint system;A-A ratio;agonist-antagonist muscle-pair activity;(A-A activity;human muscle system;linear control method;human-robot experiment;intelligent robotic system","","6","","11","","20 Dec 2012","","","IEEE","IEEE Conferences"
"The Design and Realization of Machine Vision System in Flip - chip Bonder","J. Li; Z. Zou; F. Wang","School of Mechanical and Electrical Engineering, Central South University, Hunan, Changsha, 410075; School of Mechanical and Electrical Engineering, Central South University, Hunan, Changsha, 410075. Email: zzs5218@163.com, Telephone. 0731-8830293; School of Mechanical and Electrical Engineering, Central South University, Hunan, Changsha, 410075","2006 7th International Conference on Electronic Packaging Technology","15 May 2007","2006","","","1","7","Machine vision scheme and CCD install position are designed to realize the alignment of chip and substrate in the thermosonic flip-chip bonding process. Based on experiments on various color and style LED lights, the single-color LED illumination composed of a ring LED light and some LED lamps are adopted to get a uniform luminance of visual field and clear contours of the object's surface. The vision software is developed based on HexSight, the conference points are added to models of locator to recognize different objects (chip & substrate). The reasonable control scheme of two CCD cameras is adopted. According to the error of translation and rotation estimated by experiments on many real images, the composite position error of the vision system on scale 1 times 1mm chip is calculated, which indicates the high precision and satisfaction of this vision system in microelectronic packaging","","1-4244-0619-6","10.1109/ICEPT.2006.359729","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4198850","","Machine vision;Substrates;Packaging;Bonding processes;Charge coupled devices;Light emitting diodes;Lighting;Charge-coupled image sensors;Microelectronics;Assembly","CCD image sensors;computer vision;flip-chip devices;integrated circuit packaging;lead bonding;LED lamps","machine vision system;CCD install position;thermosonic flip-chip bonding;style LED lights;single-color LED illumination;ring LED light;LED lamps;HexSight;CCD cameras;microelectronic packaging","","5","","18","","15 May 2007","","","IEEE","IEEE Conferences"
"Parkinson’s Disease Diagnosis Using Machine Learning and Voice","T. J. Wroge; Y. Özkanca; C. Demiroglu; D. Si; D. C. Atkins; R. H. Ghomi","Department of Bioengineering, University of Pittsburgh, Pittsburgh, Pennsylvania, USA; Department of Engineering, Özyeğin University, Istanbul, Turkey; Department of Engineering, Özyeğin University, Istanbul, Turkey; Division of Computing and Software Systems, University of Washington, Seattle, Washington, USA; Department of Psychiatry and Behavioral Sciences, University of Washington, Seattle, Washington, USA; Department of Psychiatry and Behavioral Sciences, University of Washington, Seattle, Washington, USA","2018 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)","17 Jan 2019","2018","","","1","7","Biomarkers derived from human voice can offer in-sight into neurological disorders, such as Parkinson's disease (PD), because of their underlying cognitive and neuromuscular function. PD is a progressive neurodegenerative disorder that affects about one million people in the the United States, with approximately sixty thousand new clinical diagnoses made each year [1]. Historically, PD has been difficult to quantity and doctors have tended to focus on some symptoms while ignoring others, relying primarily on subjective rating scales [2]. Due to the decrease in motor control that is the hallmark of the disease, voice can be used as a means to detect and diagnose PD. With advancements in technology and the prevalence of audio collecting devices in daily lives, reliable models that can translate this audio data into a diagnostic tool for healthcare professionals would potentially provide diagnoses that are cheaper and more accurate. We provide evidence to validate this concept here using a voice dataset collected from people with and without PD. This paper explores the effectiveness of using supervised classification algorithms, such as deep neural networks, to accurately diagnose individuals with the disease. Our peak accuracy of 85% provided by the machine learning models exceed the average clinical diagnosis accuracy of non-experts (73.8%) and average accuracy of movement disorder specialists (79.6% without follow-up, 83.9% after follow-up) with pathological post-mortem examination as ground truth [3].","2473-716X","978-1-5386-5916-8","10.1109/SPMB.2018.8615607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8615607","","Feature extraction;Medical diagnostic imaging;Decision trees;Training;Mathematical model;Machine learning;Machine learning algorithms","diseases;learning (artificial intelligence);medical disorders;medical signal processing;neural nets;neurophysiology;patient diagnosis;patient treatment;speech recognition","human voice;neurological disorders;PD;neuromuscular function;progressive neurodegenerative disorder;subjective rating scales;audio collecting devices;voice dataset;machine learning models;average clinical diagnosis accuracy;movement disorder specialists;Parkinsons disease diagnosis","","2","","29","","17 Jan 2019","","","IEEE","IEEE Conferences"
"K-Complex Detection Using a Hybrid-Synergic Machine Learning Method","H. Q. Vu; G. Li; N. S. Sukhorukova; G. Beliakov; S. Liu; C. Philippe; H. Amiel; A. Ugon","School of Information Technology , Deakin University, Melbourne, Australia; School of Information Technology , Deakin University, Melbourne, Australia; Centre for Informatics and Applied Optimisation, University of Ballarat, Ballarat, Australia; School of Information Technology , Deakin University, Melbourne, Australia; School of Information Technology , Deakin University, Melbourne, Australia; Sleep University , Tenon Hospital, France; Sleep University , Tenon Hospital, France; Sleep University , Tenon Hospital, France","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","21 Dec 2012","2012","42","6","1478","1490","Sleep stage identification is the first step in modern sleep disorder diagnostics process. K-complex is an indicator for the sleep stage 2. However, due to the ambiguity of the translation of the medical standards into a computer-based procedure, reliability of automated K-complex detection from the EEG wave is still far from expectation. More specifically, there are some significant barriers to the research of automatic K-complex detection. First, there is no adequate description of K-complex that makes it difficult to develop automatic detection algorithm. Second, human experts only provided the label for whether a whole EEG segment contains K-complex or not, rather than individual labels for each subsegment. These barriers render most pattern recognition algorithms inapplicable in detecting K-complex. In this paper, we attempt to address these two challenges, by designing a new feature extraction method that can transform visual features of the EEG wave with any length into mathematical representation and proposing a hybrid-synergic machine learning method to build a K-complex classifier. The tenfold cross-validation results indicate that both the accuracy and the precision of this proposed model are at least as good as a human expert in K-complex detection.","1558-2442","","10.1109/TSMCC.2012.2191775","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6190762","EEG;K-complex;multi-instance learning (MIL);sleep disorder","Electroencephalography;Sleep;Feature extraction;Visualization;Machine learning;Biomedical monitoring","electroencephalography;learning (artificial intelligence);medical signal processing;patient diagnosis;signal classification","k-complex detection;hybrid-synergic machine learning method;sleep stage identification;sleep disorder diagnostics process;computer-based procedure;medical standards;EEG wave;mathematical representation;k-complex classifier","","16","","46","","26 Apr 2012","","","IEEE","IEEE Journals"
"A multimodal-signals-based gesture recognition method for human machine interaction","X. Zhao; J. Huang; J. Zheng; Y. Ma; H. Tang","Institute of Computer Applied Technology of China Weapon Industry,Beijing,China; Institute of Computer Applied Technology of China Weapon Industry,Beijing,China; Institute of Computer Applied Technology of China Weapon Industry,Beijing,China; Institute of Computer Applied Technology of China Weapon Industry,Beijing,China; The Ohio State University,Columbus,USA","2020 3rd International Conference on Unmanned Systems (ICUS)","7 Dec 2020","2020","","","494","499","As the capacity for machines to extend human capabilities continues to grow, the communication channels used also need to expand. Allowing machines to translate gestural command into robot movements can make the communication between humans and machines more similar to interpersonal communication. Yet for operating requirements, instantaneity and precision of the interaction must reach the application level in realistic scenarios, and the accuracy of results cannot be guaranteed by thejudgement of a single motion sensor. Therefore, exploring a gesture detection technology that integrates multisensor information is very necessary. The presented work takes a step towards real-time gesture detection by fusing multiple physiological signals with wearable motion sensors. An algorithm is presented for processing and extracting motion signal acquired via inertial measurement unit (IMU) and electromyography (EMG) with a high error-tolerant way of wearing, and it is applied to the gesture recognition model established in reasonable threshold and logical judgment. This enables real-time gesture detection of 24 various assembled movements such as rotating palms while bending arms, clenching fist when unwinding upper limb. The result of involving intentional behavior by analyzing electroencephalogram (EEG) is also added to the gesture recognition process for eliminating unconscious actions. Together, these pipelines offer efficient gesture vocabulary suitable for remotely controlling robots. Experiments evaluate classifier performance and interface efficacy. The system successfully detected 95.6% of 360 commands, and the average processing time was 25.4ms among all the 15 trials of the experiment.","","978-1-7281-8025-0","10.1109/ICUS50048.2020.9274853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274853","Gestsure Recognition;EMG;EEG;Inertial Measurement Unit;Muscle-Computer-Interfaces","Electromyography;Real-time systems;Electroencephalography;Robots;Robot sensing systems;Time series analysis;Lenses","electroencephalography;electromyography;feature extraction;gesture recognition;medical signal processing;signal classification","multimodal-signals-based gesture recognition method;human machine interaction;communication channels;gestural command;robot movements;motion sensor;gesture detection technology;real-time gesture detection;physiological signals;wearable motion sensors;extracting motion signal;electromyography;gesture recognition model;gesture recognition process;gesture vocabulary","","","","10","","7 Dec 2020","","","IEEE","IEEE Conferences"
"A time-of-flight-based hand posture database for human-machine interaction","T. Kopinski; A. Gepperth; U. Handmann","ENSTA ParisTech, 858 Blvd des Maréchaux, 91762 Palaiseau, France; ENSTA ParisTech, 858 Blvd des Maréchaux, 91762 Palaiseau, France; Computer Science Institute, Hochschule Ruhr, West Lützowstrasse 5, 46236, Bottrop","2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)","2 Feb 2017","2016","","","1","6","We present a publicly available benchmark database for the problem of hand posture recognition from noisy depth data and fused RGB-D data obtained from low-cost time-of-flight (ToF) sensors. The database is the most extensive database of this kind containing over a million data samples (point clouds) recorded from 35 different individuals for ten different static hand postures. This captures a great amount of variance, due to person-related factors, but also scaling, translation and rotation are explicitly represented. Benchmark results achieved with a standard classification algorithm are computed by cross-validation both over samples and persons, the latter implying training on all persons but one and testing on the remaining one. An important result using this database is that cross-validation performance over samples (which is the standard procedure in machine learning) is systematically higher than cross-validation performance over persons, which is to our mind the true application-relevant measure of generalization performance.","","978-1-5090-3549-6","10.1109/ICARCV.2016.7838613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838613","","Sensors;Databases;Three-dimensional displays;Cameras;Noise measurement;Standards;Benchmark testing","image classification;image colour analysis;image fusion;learning (artificial intelligence);man-machine systems;visual databases","time-of-flight-based hand posture database;human-machine interaction;noisy depth data;hand posture recognition;RGB-D data fusion;ToF sensors;low-cost time-of-flight sensors;person-related factors;standard classification algorithm;cross-validation performance;generalization performance","","6","","17","","2 Feb 2017","","","IEEE","IEEE Conferences"
"Benchmark Meta-Dataset of High-Resolution Remote Sensing Imagery for Training Robust Deep Learning Models in Machine-Assisted Visual Analytics","J. A. Hurt; G. J. Scott; D. T. Anderson; C. H. Davis",NA; NA; NA; NA,"2018 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","9 May 2019","2018","","","1","9","Recent years have seen the publication of various high-resolution remote sensing imagery benchmark datasets. These datasets, while diverse in design, have many co-occurring object classes that are of interest for various application domains of Earth observation. In this research, we present our evaluation of a new meta-benchmark dataset combining object classes from the UC Merced, WHU-RS19, PatternNet, and RESISC-45 benchmark datasets. We provide open-source resources to acquire the individual benchmark datasets and then agglomerate them into a new meta-dataset (MDS). Prior research has shown that contemporary deep convolutional neural networks are able to achieve cross-validation accuracies in the range of 95-100% for the 33 identified object classes. Our analysis shows that the overall accuracy for all object classes from these benchmarks is approximately 98.6%. In this work, we investigate the utility of agglomerating the benchmarks into an MDS to train more generalizable, and therefore translatable from lab to real-world, deep machine learning (DML) models. We evaluate numerous state-of-the-art architectures, as well as our data-driven DML model fusion techniques. Finally, we compare MDS performance with that of the benchmark datasets to evaluate the performance versus cost trade-off of using multiple DML in an ensemble system.","2332-5615","978-1-5386-9306-3","10.1109/AIPR.2018.8707433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8707433","","Benchmark testing;Sports;Remote sensing;Spatial resolution;Rail transportation;Object detection;Bridges","convolutional neural nets;data visualisation;geophysical image processing;image classification;learning (artificial intelligence);remote sensing","WHU-RS19;RESISC-45 benchmark datasets;individual benchmark datasets;contemporary deep convolutional neural networks;deep machine learning models;data-driven DML model fusion techniques;benchmark meta-dataset;robust deep learning models;machine-assisted visual analytics;high-resolution remote sensing imagery benchmark datasets;meta-benchmark dataset;identified object classes","","4","","21","","9 May 2019","","","IEEE","IEEE Conferences"
"Application of Boolean Kernel Function SVM in Face Recognition","K. Cui; Y. Du","Sch. of Comput. Sci. & Technol., North China Electr. Power Univ., Baoding; Sch. of Comput. Sci. & Technol., North China Electr. Power Univ., Baoding","2009 International Conference on Networks Security, Wireless Communications and Trusted Computing","5 May 2009","2009","1","","619","622","SVM based on Boolean kernel function has outstanding performance in classifying, for the problem of face recognition, recognizing strategies based on MDNF and MPDNF Boolean kernel function SVM are Proposed. Firstly, Karhunen-Loeve transform is employed to get the representation basis of face image set, secondly, the extracted characteristics is translated into 0-1 format, thirdly, SVM based Boolean kernel function are used to classify. The face recognition experiments with ORL face databases show that the proposed methods led to significantly better recognition accuracy compared with traditional PCA method and linear SVM, between the proposed methods, the one based on MPDNF Boolean kernel function get better performance.","","978-1-4244-4223-2","10.1109/NSWCTC.2009.172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4908341","face recognition;Karhunen-Loeve transform;support vector machines;Boolean kernel function;multi-classification","Kernel;Support vector machines;Face recognition;Support vector machine classification;Karhunen-Loeve transforms;Principal component analysis;Eigenvalues and eigenfunctions;Computer science;Pattern recognition;Artificial intelligence","Boolean functions;face recognition;image classification;image representation;Karhunen-Loeve transforms;polynomials;support vector machines","MPDNF Boolean kernel function;support vector machine;face recognition;Karhunen-Loeve transform;image representation;image classification;monotone polynomial disjunctive normal form","","","","11","","5 May 2009","","","IEEE","IEEE Conferences"
"Recognising Irish Sign Language Using Electromyography","L. C. Galea; A. F. Smeaton","Dublin City University, Glasnevin, Dublin 9, Ireland; Insight Centre for Data Analytics, Dublin City University, Glasnevin, Dublin 9, Ireland","2019 International Conference on Content-Based Multimedia Indexing (CBMI)","21 Oct 2019","2019","","","1","4","Sign language is the non-verbal communication used by people with hearing and speaking impairments. The automatic recognition of sign languages is usually based on video analysis of the signer though this is difficult when considering different light levels or the surrounding environment. The work in this paper uses electromyography (EMG) and focuses on letters of the Irish Sign Language (ISL) alphabet. EMG is the recording of the electrical activity produced to stimulate movement in the skeletal muscles. We capture muscle signals and inertial movement data using the Thalmic MYO armband and, in real time, recognise the ISL alphabet. Our implementation is based on signal processing, feature extraction and machine learning. The only input required to translate the ISL gestures are EMG and movement data, thus our approach is usable in scenarios where using video for automatic recognition video is not possible.","1949-3991","978-1-7281-4673-7","10.1109/CBMI.2019.8877421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877421","Sign language recognition;EMG;inertial movement;machine learning","Electromyography;Assistive technology;Gesture recognition;Muscles;Feature extraction;Iris recognition;Real-time systems","electromyography;feature extraction;gesture recognition;learning (artificial intelligence);medical signal processing;muscle;sign language recognition","automatic recognition video;electromyography;nonverbal communication;speaking impairments;video analysis;EMG;Irish Sign Language alphabet;muscle signals;inertial movement data;ISL alphabet;Thalmic MYO armband;feature extraction;machine learning","","","","8","","21 Oct 2019","","","IEEE","IEEE Conferences"
"Arabic Sign Language Recognition System on Smartphone","A. M. Zakariya; R. Jindal","Delhi Technological University,Computer Science and Engineering Department,New Delhi,India; Delhi Technological University,Computer Science and Engineering Department,New Delhi,India","2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","30 Dec 2019","2019","","","1","5","Deaf And other verbally challenged people face challenges most of the time communicating with the society, sign language is what they commonly use between them to represent what they want to say to each other for example numbers, words or phrase. To bridge this communication Barrier between them and the society an automated system to stand as a translator between them and the society is needed, recently many researches are performed, but most of the developed Systems are only executable on computers, which are difficult and impractical to take around. We are proposing the use of smartphone as a platform, a client server system is implemented, sign image background is detected and removed under HSV color space, features extracted from the frame are the binary pixels, SVM is used to classify the features, we are able to classify 10 Arabic Sign Language with an experimental accuracy result of 92.5%.","","978-1-5386-5906-9","10.1109/ICCCNT45670.2019.8944518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944518","Computer Vision;Gesture Recognition;Image Processing;Machine Learning;Sign Language","Assistive technology;Gesture recognition;Support vector machines;Feature extraction;Servers;Smart phones;Image recognition","client-server systems;feature extraction;handicapped aids;image classification;image colour analysis;natural language processing;sign language recognition;support vector machines","communication Barrier;automated system;smartphone;client server system;Arabic Sign Language recognition system;deaf;verbally challenged people","","1","","11","","30 Dec 2019","","","IEEE","IEEE Conferences"
"Peruvian sign language recognition using low resolution cameras","B. Berrú-Novoa; R. González-Valenzuela; P. Shiguihara-Juárez","Department of Computer Science, Universidad Peruana de Ciencias Aplicadas, Lima, Perú; Department of Computer Science, Universidad Peruana de Ciencias Aplicadas, Lima, Perú; Department of Computer Science, Universidad Peruana de Ciencias Aplicadas, Lima, Perú","2018 IEEE XXV International Conference on Electronics, Electrical Engineering and Computing (INTERCON)","8 Nov 2018","2018","","","1","4","The recognition of sign language gesture through image processing and Machine Learning has been widely studied in recent years. This article presents a dataset consisting of 2400 images of the static gestures of the Peruvian sign language alphabet, in addition to applying it to a hand gesture recognition system using low resolution cameras. For the gesture recognition, the Histogram Oriented Gradient feature descriptor was used, along with 4 classification algorithms. The results showed that Histogram Oriented Gradient, along with Support Vector Machine, got the best result with a 89.46% accuracy and the system was able to recognize the gestures with variations of translation, rotation and scale.","","978-1-5386-5491-0","10.1109/INTERCON.2018.8526408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8526408","Image Processing;Machine Learning;Histogram Oriented Gradient;Sign Language Recognition","Gesture recognition;Assistive technology;Histograms;Support vector machines;Cameras;Microprocessors;Computer architecture","feature extraction;image classification;learning (artificial intelligence);sign language recognition;support vector machines","Peruvian sign language recognition;low resolution cameras;image processing;static gestures;Peruvian sign language alphabet;hand gesture recognition system;sign language gesture recognition;machine learning;classification algorithms;support vector machine;histogram oriented gradient feature descriptor","","3","","13","","8 Nov 2018","","","IEEE","IEEE Conferences"
"Brain-computer interface design using relative wavelet energy","Zhao Haibin; Wang Xu; Wang Hong","School of Information Science and Engineering, Northeastem University, Shenyang, Liaoning Province, China; School of Information Science and Engineering, Northeastem University, Shenyang, Liaoning Province, China; School of Mechanical Engineering and Automation, Northeastem University, Shenyang, Liaoning Province, China","2008 Chinese Control and Decision Conference","12 Aug 2008","2008","","","3558","3561","A brain-computer interface (BCI) is a communication system that translates brain-activity into commands for a computer or other devices. In this paper, we used a new method: relative wavelet energy (RWE) for feature selection in BCI design. Linear discriminant analysis (LDA) and support vector machine (SVM) were utilized to classify the pattern of left and right hand movement imagery. Its performance was evaluated by mutual information (MI) using the data set III of BCI competition 2003. From the results of the experiment, we can get that RWE is a very good method for feature selection in BCI research and there is not much difference between LDA and SVM.","1948-9447","978-1-4244-1733-9","10.1109/CCDC.2008.4597992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4597992","brain-computer interface;relative wavelet energy;support vector machine;linear discriminant analysis","Wavelet transforms;Transforms;Mutual information;Support vector machines;Discrete wavelet transforms;Electroencephalography;Signal resolution","man-machine systems;medical signal processing;neurophysiology;support vector machines;user interfaces","brain-computer interface design;relative wavelet energy;communication system;linear discriminant analysis;support vector machine;mutual information","","","","20","","12 Aug 2008","","","IEEE","IEEE Conferences"
"Study and implementation of color-based object tracking in monocular image sequences","M. O. Mehmood","Department of Electronic Engineering, NED University of Engineering & Technology, Karachi, Pakistan","2009 IEEE Student Conference on Research and Development (SCOReD)","5 Apr 2010","2009","","","109","111","Kernel tracking of density-based appearance models is implemented in this paper for real-time object tracking applications. First a ROI, i.e., the region of interest is selected in real-time to create a model. Then the matching and locating of the search object is achieved by using mean-shift algorithm. Experimental results show that this method can find perform object tracking with adaptation to scale and translation, robustness to noise and handling of occlusion.","","978-1-4244-5186-9","10.1109/SCORED.2009.5443273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5443273","Machine vision;Image matching;Image motion analysis;Image Processing;Pattern Recognition","Image sequences;Histograms;Kernel;Noise robustness;Tracking;Colored noise;Feeds;Probability;Gravity;Research and development","computer vision;image colour analysis;image matching;image motion analysis;image sequences;object detection","color-based object tracking;monocular image sequences;kernel tracking;density-based appearance models;real-time object tracking;search object matching;mean-shift algorithm;machine vision;image matching;Image motion analysis","","3","","8","","5 Apr 2010","","","IEEE","IEEE Conferences"
"Robust Semi-synchronous BCI Controller for Brain-Actuated Exoskeleton System","J. Choi; K. Kim; J. Lee; S. J. Lee; H. Kim","University of Science and Technology, KIST School,Division of Bio-Medical Science & Technology,Seoul,KOREA Rep.; Center for Bionics, Korea Institute of Science and Technology,Seoul,KOREA Rep.; Center for Bionics, Korea Institute of Science and Technology,Seoul,KOREA Rep.; Center for Bionics, Korea Institute of Science and Technology,Seoul,KOREA Rep.; Center for Bionics, Korea Institute of Science and Technology,Seoul,KOREA Rep.","2020 8th International Winter Conference on Brain-Computer Interface (BCI)","9 Apr 2020","2020","","","1","3","In this study, we propose a real-time brain-computer interface (BCI)-based control system for lower limb exoskeleton. Voluntarily induced electroencephalogram (EEG) signal during gait and sit motor imagery (MI) was decoded and translate into the exoskeleton as control commands in real-time. While it is hard to tell when the user engages in the conventional BCI control system, the EEG signal via the user's consecutive eyeblink was used as an initiation of MI in the proposed semi-synchronous control system; hence the EEG decoder is able to denote the transition point. This semi-synchronous BCI control system combines the advantages of asynchronous and synchronous BCI systems by understanding the user's MI environment for robust exception handling. The classifier's flow diagram of the presented BCI control system was described as a finite state machine.","2572-7672","978-1-7281-4707-9","10.1109/BCI48061.2020.9061658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9061658","EEG;Brain-Computer Interface;Lower-Limb Exoskeleton;Gait;Finite State Machine","Exoskeletons;Electroencephalography;Control systems;Brain-computer interfaces;Real-time systems;Decoding;Automata","brain-computer interfaces;electroencephalography;finite state machines;medical signal processing;orthotics","robust semisynchronous BCI controller;brain-actuated exoskeleton system;motor imagery;finite state machine;EEG decoder;EEG signal;asynchronous BCI systems;BCI control system;synchronous BCI systems;semisynchronous BCI control system;voluntarily induced electroencephalogram signal;lower limb exoskeleton;real-time brain-computer interface-based control system","","","","5","","9 Apr 2020","","","IEEE","IEEE Conferences"
"Attention-Based Vanishing Point Detection","F. Stentiford","University College London, Adastral Park Campus, Martlesham Heath, Ipswich, UK. f.stentiford@adastral.ucl.ac.uk","2006 International Conference on Image Processing","20 Feb 2007","2006","","","417","420","Perspective is a fundamental structure that is found to some extent in most images that reflect 3D structure. It is thought to be an important factor in the human visual system for obtaining understanding and extracting semantics from visual material. This paper describes a method of detecting vanishing points in images that does not require prior assumptions about the image being analysed. It enables 3D information to be inferred from 2D images. The approach is derived from earlier work on visual attention that identifies salient regions and translational symmetries.","2381-8549","1-4244-0480-0","10.1109/ICIP.2006.312482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4106555","Machine vision","Image edge detection;Data mining;Layout;Image segmentation;Cameras;Humans;Educational institutions;Visual system;Image analysis;Machine vision","computer vision;feature extraction;image recognition;visual perception","vanishing point detection;perspective structure;human visual system;semantics extraction;3D information","","8","1","13","","20 Feb 2007","","","IEEE","IEEE Conferences"
"Extraction and classification of Electroencephalogram signals","R. Upadhyay; P. K. Kankar; P. K. Padhy; V. K. Gupta","PDPM Indian Institute of Information Technology, Design and Manufacturing/Mechatronics Department, Jabalpur, India; PDPM Indian Institute of Information Technology, Design and Manufacturing/Mechatronics Department, Jabalpur, India; PDPM Indian Institute of Information Technology, Design and Manufacturing/Mechatronics Department, Jabalpur, India; PDPM Indian Institute of Information Technology, Design and Manufacturing/Mechatronics Department, Jabalpur, India","2012 IEEE International Conference on Computational Intelligence and Computing Research","2 May 2013","2012","","","1","4","Brain Computer Interface creates a communication path way between brain and outside world. Brain signals are recorded and processed to translate Electroencephalogram activity to an external command. Brain signals recorded from the scalp or from inside the brain, enable users to control a variety of applications. This capability can be very useful for the patient, suffering from severe motor disorder. Efficient working of a Brain Computer Interface widely depends upon the signal processing methodology applied for feature extraction and classification of Electroencephalograms. The efficiency of a versatile signal processing framework proposed in this work has been determined for Electroencephalogram signals in terms of feature extraction and classification. The power spectral density of rhythmic components of the Electroencephalogram signals extracted using IEEE standard 1057 four-parameter sine wave fit algorithm, is calculated using Welch method and classification of the Electroencephalogram signals is done using Non-linear Support Vector Machine model.","","978-1-4673-1344-5","10.1109/ICCIC.2012.6510216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510216","Non-linear Support Vector Machine Electroencephalogram (EEG);Brain Computer Interface (BCI);Gaussian Kernel;IEEE standards 1057","","brain-computer interfaces;electroencephalography;feature extraction;medical signal processing;signal classification;support vector machines","nonlinear support vector machine model;Welch method;signal processing framework;feature extraction;signal processing methodology;motor disorder;brain signal recording;communication path way;brain computer interface;electroencephalogram signal classification;electroencephalogram signal extraction","","1","","19","","2 May 2013","","","IEEE","IEEE Conferences"
"Braille Image Recognition for Beginners","S. Ibrahim; N. A. Tarmizi; N. Sabri; N. F. Mohd Johari; A. F. Ahmad Fadzil","Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (UiTM) Melaka, Kampus Jasin, Merlimau, Melaka, 77300, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (UiTM) Melaka, Kampus Jasin, Merlimau, Melaka, 77300, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (UiTM) Melaka, Kampus Jasin, Merlimau, Melaka, 77300, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (UiTM) Melaka, Kampus Jasin, Merlimau, Melaka, 77300, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (UiTM) Melaka, Kampus Jasin, Merlimau, Melaka, 77300, Malaysia","2018 9th IEEE Control and System Graduate Research Colloquium (ICSGRC)","4 Mar 2019","2018","","","81","84","Braille is a tactile that consists of dots which is used by visually impaired people in reading. Braille pattern of alphabet consists of its own pattern in which some of it does not even relate to the alphabet. Thus, it is difficult for normal people to detect and recognize the braille pattern. This paper presents a study of braille image recognition for beginners. The outcome of this study is expected to translate a braille image patterns into a readable alphabet text. A technique of Bag of Features (BOF) is proposed for the recognition of the braille image. On the other hand, the image classification is done using a Support Vector Machine (SVM) technique. Seventy-eight of braille images is tested. From the testing performed, it is found that 97.44% of correct recognition accuracy is achieved which revealed that the proposed techniques are applicable for braille image recognition.","","978-1-5386-6321-9","10.1109/ICSGRC.2018.8657490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8657490","Braille;image recognition;Bag of Features (BOF);Support Vector Machine (SVM)","","handicapped aids;image classification;image recognition;support vector machines","braille image recognition;visually impaired people;braille image patterns;image classification","","","","11","","4 Mar 2019","","","IEEE","IEEE Conferences"
"Content Based Color Image Classification using SVM","S. Agrawal; N. K. Verma; P. Tamrakar; P. Sircar","Dept. of Electr. Eng., Indian Inst. of Technol. Kanpur, Kanpur, India; Dept. of Electr. Eng., Indian Inst. of Technol. Kanpur, Kanpur, India; Dept. of Electr. Eng., Indian Inst. of Technol. Kanpur, Kanpur, India; Dept. of Electr. Eng., Indian Inst. of Technol. Kanpur, Kanpur, India","2011 Eighth International Conference on Information Technology: New Generations","11 Jul 2011","2011","","","1090","1094","We propose a novel approach for content based color image classification using Support Vector Machine (SVM). Traditional classification approaches deal poorly on content based image classification tasks being one of the reasons of high dimensionality of the feature space. In this paper, color image classification is done on features extracted from histograms of color components. The benefit of using color image histograms are better efficiency, and insensitivity to small changes in camera view-point i.e. translation and rotation. As a case study for validation purpose, experimental trials were done on a database of about 500 images divided into four different classes has been reported and compared on histogram features for RGB, CMYK, Lab, YUV, YCBCR, HSV, HVC and YIQ color spaces. Results based on the proposed approach are found encouraging in terms of color image classification accuracy.","","978-1-61284-427-5","10.1109/ITNG.2011.202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945183","Support Vector Machine;color image histogram;image classification","Image color analysis;Histograms;Support vector machines;Training;Accuracy;Color;Feature extraction","feature extraction;image classification;image colour analysis;support vector machines","content based color image classification;support vector machine;extracted features;color component histograms;color image histograms;RGB color spaces;CMYK color spaces;Lab color spaces;YUV color spaces;YCBCR color spaces;HSV color spaces;HVC color spaces;YIQ color spaces","","20","","10","","11 Jul 2011","","","IEEE","IEEE Conferences"
"The research of image segmentation based on color characteristic","G. Jiang; C. Zhao; J. Qi","School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China; School of Resources and Environment Engineering, Henan Polytechnic University, Jiaozuo 454000, China; School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454000, China","2011 International Conference on Machine Learning and Cybernetics","12 Sep 2011","2011","4","","1851","1855","Agricultural image segmentation is the basic work of computer vision technology in agriculture. This paper presents an image process method based on color characteristic. The algorithm separates the crop area from soil background according to the color feature, which is got by analyzing the color difference between crop and background. In order to translate the color images to gray ones effectively, fourth different transformation methods in RGB color space and three components of HSI were compared. Test results show that the excess green 2G-R-B is the best index for segmentation. This index has a characteristic of consuming little time, good quality of segmentation and not being influenced by sunshine intensity and can meet the real-time navigation in the target extraction.","2160-1348","978-1-4577-0308-9","10.1109/ICMLC.2011.6016969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016969","Machine vision;Image segmentation;Color characteristic","Image color analysis;Image segmentation;Agriculture;Object segmentation;Machine learning;Cybernetics","agriculture;crops;image colour analysis;image segmentation","image segmentation;color characteristic;agricultural image segmentation;computer vision technology;crop area;soil background;RGB color space","","","","8","","12 Sep 2011","","","IEEE","IEEE Conferences"
"GA-SVM based feature selection and parameters optimization for BCI research","L. Wang; G. Xu; J. Wang; S. Yang; L. Guo; W. Yan","The Province-Ministry Joint Key Laboratory of Electromagnetic Field and Electrical Apparatus Reliability, Hebei University of Technology, Tianjin, China; The Province-Ministry Joint Key Laboratory of Electromagnetic Field and Electrical Apparatus Reliability, Hebei University of Technology, Tianjin, China; The Province-Ministry Joint Key Laboratory of Electromagnetic Field and Electrical Apparatus Reliability, Hebei University of Technology, Tianjin, China; The Province-Ministry Joint Key Laboratory of Electromagnetic Field and Electrical Apparatus Reliability, Hebei University of Technology, Tianjin, China; The Province-Ministry Joint Key Laboratory of Electromagnetic Field and Electrical Apparatus Reliability, Hebei University of Technology, Tianjin, China; The Province-Ministry Joint Key Laboratory of Electromagnetic Field and Electrical Apparatus Reliability, Hebei University of Technology, Tianjin, China","2011 Seventh International Conference on Natural Computation","19 Sep 2011","2011","1","","580","583","Brain Computer Interface (BCI) can translate the mind of the patients who suffered from locked- in syndrome into control commands or meaning symbols. Using this technology, the patients can communicate with the world. The core parts of a typical BCI system is feature extraction and pattern recognition. Too many irrelevant and redundant features will increase the time of classification and decrease the prediction accuracy. The kernel parameters setting for support vector machine (SVM) also impact on the classification accuracy. In this paper, after the features extracted though the algorithm called Sample Entropy, GA-SVM hybrid algorithm was used with two purposes: Selecting of the optimal feature subset and deciding the parameters for SVM classifier. Compared with GA-based feature selection and GA-based parameters optimization for SVM, the GA-SVM hybrid algorithm has fewer input features and gain much higher classification accuracy.","2157-9563","978-1-4244-9953-3","10.1109/ICNC.2011.6022083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6022083","Brain Computer Interface;Sample Entropy;Genetic Algorithm;Support Vector Machine","Support vector machines;Accuracy;Genetic algorithms;Feature extraction;Electroencephalography;Entropy;Optimization","brain-computer interfaces;electroencephalography;feature extraction;genetic algorithms;handicapped aids;medical signal processing;pattern recognition;support vector machines","GA-SVM based feature selection;parameter optimization;BCI research;brain computer interface;locked- in syndrome;feature extraction;pattern recognition;support vector machine;genetic algorithm;EEG;electrocephalogram","","2","","6","","19 Sep 2011","","","IEEE","IEEE Conferences"
"Regressing force-myographic signals collected by an armband to estimate torque exerted by the wrist: A preliminary investigation","M. Sakr; C. Menon","MENRVA Group, School of Engineering Science, Simon Fraser University, Burnaby, BC V5A 1S6 Canada; MENRVA Group, School of Engineering Science, Simon Fraser University, Burnaby, BC V5A 1S6 Canada","2016 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)","3 Nov 2016","2016","","","1","4","Human-machine-interfaces (HMI) have a key role for translating human intention into control commands to external devices. Different wearable techniques, including surface electromyography (sEMG), have been proposed for acquiring bio-signals that reveal the human intention. In this paper, we explore an easy-to-use wearable sensor device that can be used to measure force-myography (FMG) signals. We assess if FMG signals can be used to estimate isometric torque of hand pronation-supination, wrist flexion-extension or wrist radial-ulnar using a regression model. Results of our investigation report an average accuracy over 90%. The related standard deviation of 0.02 is showing consistency of data among different data collecting sessions. The proposed FMG-based device shows therefore promising performance for different future applications, which may include: monitoring the progress of patients during exercising in arm rehabilitation programs; proportional control of robotic hand prosthesis; and control of robot movements.","","978-1-4673-8721-7","10.1109/CCECE.2016.7726852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726852","Force Myography;Human Machine Interaction Regression;Rehabilitation Robotics;Wearable Sensors","Torque;Wrist;Data models;Training;Training data;Robot sensing systems;Force","electromyography;medical robotics;medical signal processing;prosthetics;regression analysis","force-myographic signals;armband;human-machine-interfaces;HMI;wearable techniques;control commands;external devices;surface electromyography;sEMG;FMG signals;wearable sensor device;isometric torque estimation;regression model;hand pronation-supination;wrist flexion-extension;wrist radial-ulnar;data collecting sessions;arm rehabilitation programs;robotic hand prosthesis;robot movements","","2","","20","","3 Nov 2016","","","IEEE","IEEE Conferences"
"Bidirectional Attention-Recognition Model for Fine-Grained Object Classification","C. Liu; H. Xie; Z. Zha; L. Yu; Z. Chen; Y. Zhang","School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China","IEEE Transactions on Multimedia","2 Jul 2020","2020","22","7","1785","1795","Fine-grained object classification (FGOC) is a challenging research topic in multimedia computing with machine learning, which faces two pivotal conundrums: focusing attention on the discriminate part regions, and then processing recognition with the part-based features. Existing approaches generally adopt a unidirectional two-step structure, that first locate the discriminate parts and then recognize the part-based features. However, they neglect the truth that part localization and feature recognition can be reinforced in a bidirectional process. In this paper, we propose a novel bidirectional attention-recognition model (BARM) to actualize the bidirectional reinforcement for FGOC. The proposed BARM consists of one attention agent for discriminate part regions proposing and one recognition agent for feature extraction and recognition. Meanwhile, a feedback flow is creatively established to optimize the attention agent directly by recognition agent. Therefore, in BARM the attention agent and the recognition agent can reinforce each other in a bidirectional way and the overall framework can be trained end-to-end without neither object nor parts annotations. Moreover, a novel Multiple Random Erasing data augmentation is proposed, and it exhibits impressive pertinency and superiority for FGOC. Conducted on several extensive FGOC benchmarks, BARM outperforms the present state-of-the-art methods in classification accuracy. Furthermore, BARM exhibits a clear interpretability and keeps consistent with the human perception in visualization experiments.","1941-0077","","10.1109/TMM.2019.2954747","National Key Research and Development Program of China Stem Cell and Translational Research; National Natural Science Foundation of China; Youth Innovation Promotion Association of the Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8907499","Fine-grained object classification;interpretable machine learning;visual attention;pattern recognition;data augmentation","Feature extraction;Proposals;Annotations;Visualization;Task analysis;Training;Computational modeling","feature extraction;image classification;learning (artificial intelligence);multimedia computing;neural nets;object recognition","feature extraction;bidirectional reinforcement;novel bidirectional attention-recognition model;bidirectional process;feature recognition;part localization;discriminate parts;part-based features;discriminate part regions;fine-grained object classification;FGOC;object nor parts annotations;attention agent;BARM;recognition agent","","2","","68","IEEE","20 Nov 2019","","","IEEE","IEEE Journals"
"Atlas-Based Multiorgan Segmentation for Dynamic Abdominal PET","S. Ren; P. Laub; Y. Lu; M. Naganawa; R. E. Carson","Department of Biomedical Engineering, Yale University, New Haven, CT, USA; Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA; Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA; Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT, USA; Department of Biomedical Engineering, Yale University, New Haven, CT, USA","IEEE Transactions on Radiation and Plasma Medical Sciences","1 Jan 2020","2020","4","1","50","62","Region of interest (ROI) delineation is required to extract tissue time-activity curves (TACs) in dynamic PET studies, to analyze functional changes or estimate physiological parameters. In this paper, we present an automatic framework for atlas-based multiorgan segmentation in abdominal dynamic PET images with three different methods (4D-pair, 4D-PCA, and 3-D), incorporating probabilistic atlas information into the segmentation as a spatial prior using maximum a posteriori (MAP) estimation. Due to different tracer kinetics in each organ, PET images from different time periods post injection (p.i.) have great intensity differences. Thus, when dynamic images are available, to use this temporal information, two strategies can be employed. First, tissue activities from two frames with highly different activity distributions were selected, namely, an early 8-10 min p.i. and a late 55-60 min p.i. frame, and modeled as a bivariate Gaussian distribution. Theoretically, this method can be applied if more than one frame of data is available. Second, principal component analysis (PCA) was applied to the full series of dynamic images to extract two images corresponding to the first two components. When dynamic image series are not available, the segmentation framework can be scaled down to 3-D, by building a univariate Gaussian distribution based on one 3-D image. The final segmentation results for all three methods were determined by optimizing the MAP-based energy function with two hyperparameters (λl, η) by multilabel graph cuts. We performed hyperparameter optimization and evaluated the proposed segmentation methods of 4D-pair and 4D-PCA by leave-one-out cross-validation using 30 sets of 4-D abdominal 18F-FP-(+)-DTBZ PET images. To evaluate segmentation results, the pancreas and spleen TACs were extracted, and the percentage error between the area under curve (AUC) of the TACs extracted by manual and automated segmentations was determined. The 4D-pair method with the hyperparameter combination of (λ_l = 0.1, η = 1) yielded the best performance. TAC AUC %error results with PCA-based methods showed slightly higher %error than 4D-pair. The 3-D method showed much larger %error than the other two methods. The 4D-pair results agreed well with the manual segmentation, with mean pancreas and spleen TAC AUC %errors of 0.3±3.3% and -0.4±8.1%, respectively. In addition, the distribution volume (VT) values of pancreas and spleen were determined by kinetic modeling using TACs from either manual or automated segmentations. There were no significant differences between manual- and auto-VT values (p values of 0.14 and 0.74 for pancreas and spleen, respectively). Thus, the proposed automated segmentation method can provide robust and reliable ROIs of the pancreas and spleen for kinetic modeling.","2469-7303","","10.1109/TRPMS.2019.2926889","National Institutes of Health; National Center for Advancing Translational Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8755506","Image segmentation;machine learning;maximum a posteriori estimation;positron emission tomography","Image segmentation;Probabilistic logic;Pancreas;Kinetic theory;Manuals;Computed tomography","biological organs;biological tissues;feature extraction;Gaussian distribution;image segmentation;medical image processing;optimisation;positron emission tomography;principal component analysis","4D-pair method;manual segmentation;automated segmentation method;atlas-based multiorgan segmentation;tissue time-activity curves;dynamic PET studies;physiological parameters;abdominal dynamic PET images;4D-PCA;probabilistic atlas information;maximum a posteriori estimation;tracer kinetics;tissue activities;bivariate Gaussian distribution;dynamic image series;univariate Gaussian distribution;MAP-based energy function;region-of-interest delineation;MAP estimation;principal component analysis;hyperparameter optimization;time 8.0 min to 10.0 min;time 55.0 min to 60.0 min","","","","33","IEEE","4 Jul 2019","","","IEEE","IEEE Journals"
"EEG Classification by Factoring in Sensor Spatial Configuration","L. S. Mokatren; R. Ansari; A. E. Cetin; A. D. Leow; O. A. Ajilore; H. Klumpp; F. T. Yarman Vural","Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA; Department of Psychiatry, University of Illinois at Chicago, Chicago, IL, USA; Department of Psychiatry, University of Illinois at Chicago, Chicago, IL, USA; Department of Psychiatry, University of Illinois at Chicago, Chicago, IL, USA; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey","IEEE Access","17 Feb 2021","2021","9","","19053","19065","Electroencephalography (EEG) serves as an effective diagnostic tool for mental disorders and neurological abnormalities. Enhanced analysis and classification of EEG signals can help improve performance in classifying the disorders and abnormalities. A new approach is examined here for enhancing EEG classification performance using a novel model of data representation that leverages knowledge of spatial layout of EEG sensors. An investigation of the performance of the proposed data representation model provides evidence of consistently higher classification accuracy of the proposed model compared with a model that ignores the sensor layout. The performance is assessed for models that represent the information content of the EEG signals in two different ways: a one-dimensional concatenation of the channels of the frequency bands and a proposed image-like two-dimensional representation of the EEG channel locations. The models are used in conjunction with different machine learning techniques. Performance of these models is examined on two tasks: social anxiety disorder classification, and emotion recognition using a dataset, DEAP, for emotion analysis using physiological signals. We hypothesize that the proposed two-dimensional model will significantly outperform the one-dimensional model and this is validated in our results as this model consistently yields 5-8% higher accuracy in all machine learning algorithms investigated. Among the algorithms investigated, Convolutional Neural Networks provide the best performance, far exceeding that of Support Vector Machine and k-Nearest Neighbors algorithms.","2169-3536","","10.1109/ACCESS.2021.3054670","NIMH K23 MH093679 (HK) and the Center for Clinical and Translational Research (CCTS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336002","Machine learning;EEG;CNN;spatio-temporal features;emotion recognition;SAD","Electroencephalography;Brain modeling;Data models;Task analysis;Emotion recognition;Machine learning;Machine learning algorithms","electroencephalography;emotion recognition;feature extraction;learning (artificial intelligence);medical signal processing;neural nets;neurophysiology;signal classification;support vector machines","EEG signals;one-dimensional concatenation;frequency bands;image-like two-dimensional representation;EEG channel locations;social anxiety disorder classification;emotion recognition;emotion analysis;physiological signals;two-dimensional model;one-dimensional model;sensor spatial configuration;electroencephalography;effective diagnostic tool;mental disorders;neurological abnormalities;EEG classification performance;leverages knowledge;spatial layout;EEG sensors;data representation model;consistently higher classification accuracy;sensor layout;information content","","1","","46","CCBY","26 Jan 2021","","","IEEE","IEEE Journals"
"Detecting Suspected Pump Thrombosis in Left Ventricular Assist Devices via Acoustic Analysis","B. Semiz; S. Hersek; M. B. Pouyan; C. Partida; L. Blazquez-Arroyo; V. Selby; G. Wieselthaler; J. M. Rehg; L. Klein; O. T. Inan","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; University of California, San Francisco, CA, USA; University of California, San Francisco, CA, USA; University of California, San Francisco, CA, USA; University of California, San Francisco, CA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; University of California, San Francisco, CA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Journal of Biomedical and Health Informatics","1 Jul 2020","2020","24","7","1899","1906","Objective: Left ventricular assist devices (LVADs) fail in up to 10% of patients due to the development of pump thrombosis. Remote monitoring of patients with LVADs can enable early detection and, subsequently, treatment and prevention of pump thrombosis. We assessed whether acoustical signals measured on the chest of patients with LVADs, combined with machine learning algorithms, can be used for detecting pump thrombosis. Methods: 13 centrifugal pump (HVAD) recipients were enrolled in the study. When hospitalized for suspected pump thrombosis, clinical data and acoustical recordings were obtained at admission, prior to and after administration of thrombolytic therapy, and every 24 hours until laboratory and pump parameters normalized. First, we selected the most important features among our feature set using LDH-based correlation analysis. Then using these features, we trained a logistic regression model and determined our decision threshold to differentiate between thrombosis and non-thrombosis episodes. Results: Accuracy, sensitivity and precision were calculated to be 88.9%, 90.9% and 83.3%, respectively. When tested on the post-thrombolysis data, our algorithm suggested possible pump abnormalities that were not identified by the reference pump power or biomarker abnormalities. Significance: We showed that the acoustical signatures of LVADs can be an index of mechanical deterioration and, when combined with machine learning algorithms, provide clinical decision support regarding the presence of pump thrombosis.","2168-2208","","10.1109/JBHI.2020.2966178","National Institutes of Health; National Center for Advancing Translational Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957505","Left ventricular assist device;pump thrombosis;heart failure;machine learning","Thrombosis;Feature extraction;Harmonic analysis;Pumps;Blood;Acoustics;Correlation","bioacoustics;blood vessels;cardiology;diseases;haemodynamics;learning (artificial intelligence);medical signal processing;patient diagnosis;patient monitoring;patient treatment;regression analysis","pump thrombosis;left ventricular assist devices;LVADs;machine learning;nonthrombosis episodes;reference pump power;pump abnormalities;centrifugal pump recipients;acoustic analysis;remote monitoring;thrombolytic therapy;LDH-based correlation analysis;logistic regression model;biomarker abnormalities;acoustical signatures","Acoustics;Aged;Algorithms;Female;Heart Sounds;Heart-Assist Devices;Humans;Male;Middle Aged;Signal Processing, Computer-Assisted;Sound Spectrography;Stethoscopes;Thrombosis","","","45","IEEE","13 Jan 2020","","","IEEE","IEEE Journals"
"Neuromuscular Password-Based User Authentication","X. Jiang; K. Xu; X. Liu; C. Dai; D. A. Clifton; E. A. Clancy; M. Akay; W. Chen","School of Information Science and Technology, Center for Intelligent Medical Electronics, Fudan University, Shanghai, China; School of Information Science and Technology, Center for Intelligent Medical Electronics, Fudan University, Shanghai, China; School of Art Design and Media, East China University of Science and Technology, Shanghai, China; School of Information Science and Technology, Center for Intelligent Medical Electronics, Fudan University, Shanghai, China; Department of Engineering Science, Institute of Biomedical Engineering, University of Oxford, Oxford, U.K.; Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Department of Biomedical Engineering, University of Houston, Houston, TX, USA; School of Information Science and Technology, Center for Intelligent Medical Electronics, Fudan University, Shanghai, China","IEEE Transactions on Industrial Informatics","11 Jan 2021","2021","17","4","2641","2652","In this article, we propose a novel neuromuscular password-based user authentication method. The method consists of two parts: surface electromyogram (sEMG) based finger muscle isometric contraction password (FMICP) and neuromuscular biometrics. FMICP can be entered through isometric contraction of different finger muscles in a prescribed order without actual finger movement, which makes it difficult for observers to obtain the password. In our study, the isometric contraction patterns of different finger muscles were recognized through high-density sEMG signals acquired from the right dorsal hand. Moreover, both time-frequency-space domain features at macroscopic level (interference-pattern EMG) and motor neuron firing rate features at microscopic level (via decomposition) were extracted to represent neuromuscular biometrics, serving as a second defense. The FMICP and macro-micro neuromuscular biometrics together form a neuromuscular password. The proposed neuromuscular password achieved an equal error rate (EER) of 0.0128 when impostors entered a wrong FMICP. Even when impostors entered the correct FMICP, the neuromuscular biometrics, as the second defense, inhibited impostors with an EER of 0.1496. To the best of our knowledge, this is the first study to use individually unique neuromuscular information during unobservable muscle isometric contractions for user authentication, with training and testing data acquired on different days.","1941-0050","","10.1109/TII.2020.3001612","National Key Research and Development Program of China Stem Cell and Translational Research; Shanghai Pujiang Program; Shanghai Municipal Science and Technology Major Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115220","Biometrics;high-density surface electromyogram (sEMG);machine learning;neuromuscular password;user authentication","Neuromuscular;Password;Biometrics (access control);Authentication;Feature extraction;Task analysis","biomechanics;biometrics (access control);electromyography;feature extraction;medical signal processing;neurophysiology;time-frequency analysis","high-density sEMG signals;macroscopic level;neuromuscular information;surface electromyogram based finger muscle isometric contraction password;neuromuscular password-based user authentication method;right dorsal hand;time-frequency-space domain features;interference-pattern EMG;microneuromuscular biometrics;macroneuromuscular biometrics;equal error rate;motor neuron firing rate","","1","","26","IEEE","11 Jun 2020","","","IEEE","IEEE Journals"
"VINet: A Visually Interpretable Image Diagnosis Network","D. Gu; Y. Li; F. Jiang; Z. Wen; S. Liu; W. Shi; G. Lu; C. Zhou","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Department of Medical Imaging, Nanjing Jinling Hospital, Nanjing, China; Department of Medical Imaging, Nanjing Jinling Hospital, Nanjing, China","IEEE Transactions on Multimedia","2 Jul 2020","2020","22","7","1720","1729","Recently, due to the black box characteristics of deep learning techniques, the deep network-based computer-aided diagnosis (CADx) systems have encountered many difficulties in practical applications. The crux of the problem is that these models should be explainable the model should give doctors rationales that can explain the diagnosis. In this paper, we propose a visually interpretable network (VINet) which can generate diagnostic visual interpretations while making accurate diagnoses. VINet is an end-to-end model consisting of an importance estimation network and a classification network. The former produces a diagnostic visual interpretation for each case, and the classifier diagnoses the case. In the classifier, by exploring the information in the diagnostic visual interpretation, the irrelevant information in the feature maps is eliminated by our proposed feature destruction process. This allows the classification network to concentrate on the important features and use them as the primary references for classification. Through a joint optimization of higher classification accuracy and eliminating as many irrelevant features as possible, a precise, fine-grained diagnostic visual interpretation, along with an accurate diagnosis, can be produced by our proposed network simultaneously. Based on a computed tomography image dataset (LUNA16) on pulmonary nodule, extensive experiments have been conducted, demonstrating that the proposed VINet can produce state-of-the-art diagnostic visual interpretations compared with all baseline methods.","1941-0077","","10.1109/TMM.2020.2971170","National Key Research and Development Program of China Stem Cell and Translational Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8979157","Machine learning;neural network;image classification;medical diagnostic imaging","Visualization;Medical services;Biomedical imaging;Estimation;Computational modeling;Solid modeling;Task analysis","computerised tomography;fault diagnosis;feature extraction;image classification;learning (artificial intelligence);medical image processing;pattern classification;support vector machines","VINet;visually interpretable image diagnosis network;deep learning techniques;deep network-based computer-aided diagnosis systems;visually interpretable network;end-to-end model;importance estimation network;classification network;fine-grained diagnostic visual interpretation;state-of-the-art diagnostic visual interpretations","","","","48","IEEE","3 Feb 2020","","","IEEE","IEEE Journals"
"A Deformation Robust ISAR Image Satellite Target Recognition Method Based on PT-CCNN","W. Lu; Y. Zhang; C. Yin; C. Lin; C. Xu; X. Zhang","Graduate School, Space Engineering University, Beijing, China; Department of Aerospace Institute, Space Engineering University, Beijing, China; Department of Aerospace Institute, Space Engineering University, Beijing, China; Department of Aerospace Institute, Space Engineering University, Beijing, China; Department of Aerospace Institute, Space Engineering University, Beijing, China; China Xi’an Satellite Control Center, Xi’an, China","IEEE Access","9 Feb 2021","2021","9","","23432","23453","To tackle the inherent unknown deformation (e.g., translation, rotation and scaling) of the inverse synthetic aperture radar (ISAR) images, a deep polar transformer-circular convolutional neural network, i.e., PT-CCNN, is proposed to achieve deformation robust ISAR image automatic target recognition (ATR) in this article. Inspired by human visual system and canonical coordinate of Lie-groups, we adopt a polar transformer module to transform the deformation ISAR images to the log-polar representations, before which a conventional convolutional neural network (CNN) is utilized to predict the origin of log-polar transformation. The above techniques make the proposed network invariant to translation, and equivariant to rotation and scaling. On this basis, for the log-polar representations with wrap-around structure, a circular convolutional neural network (CCNN) is further applied to extract more effective and highly discriminative features and improve recognition accuracy. The proposed network is end-to-end trainable with a classification loss, and could extract deformation-robust and essential features automatically. For multiple practical ISAR image datasets of six satellites, the performance testing and comparison experiments demonstrate that the techniques utilized in PT-CCNN are effective, and our proposed network achieve higher recognition accuracy than those previous common methods based on deep learning. For instance, our proposed PT-CCNN beats traditional CNN on rotation, scaling and practical deformation datasets by 24.5-49.3%, 9.0-40.8% and 22.3-26.7%. And it also outperforms the polar CNN without using the above techniques on rotation, scaling and practical deformation datasets by 9.2-53.7%, 5.2-54.6% and 9.0-49.9%. Additionally, the presented visualization results show the abilities and advantages of our method in terms of handling image deformation and extracting effective features.","2169-3536","","10.1109/ACCESS.2021.3056671","National Natural Science Foundation of China; National Outstanding Youth Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345674","Automatic target recognition (ATR);inverse synthetic aperture radar (ISAR);deep convolutional neural network (DCNN);image deformation;log-polar transformation","Feature extraction;Radar imaging;Target recognition;Strain;Radar;Spaceborne radar;Satellites","feature extraction;geophysical image processing;image classification;image recognition;learning (artificial intelligence);neural nets;object detection;radar imaging;radar target recognition;remote sensing by radar;synthetic aperture radar","deformation robust ISAR image satellite target recognition method;inherent unknown deformation;inverse synthetic aperture radar images;deep polar transformer-circular;deformation robust ISAR image automatic target recognition;human visual system;polar transformer module;deformation ISAR images;log-polar representations;conventional convolutional neural network;log-polar transformation;network invariant;circular convolutional neural network;highly discriminative features;deformation-robust;multiple practical ISAR image datasets;PT-CCNN beats traditional CNN;practical deformation datasets;polar CNN;image deformation;extracting effective features","","","","55","CCBYNCND","3 Feb 2021","","","IEEE","IEEE Journals"
"Optical Character Recognition using Convolutional Neural Network","S. Shreya; Y. Upadhyay; M. Manchanda; R. Vohra; G. D. Singh","Bharati Vidyapeeth’s College of Engineering,Electronics and Communication Computer Science Engineering,New Delhi,India; Bharati Vidyapeeth’s College of Engineering,Computer Science Engineering,New Delhi,India; Bharati Vidyapeeth’s College of Engineering,Electronics and Communication Computer Science Engineering,New Delhi,India; Bharati Vidyapeeth’s College of Engineering,Electronics and Communication Computer Science Engineering,New Delhi,India; Bharati Vidyapeeth’s College of Engineering,Electronics and Communication Computer Science Engineering,New Delhi,India","2019 6th International Conference on Computing for Sustainable Global Development (INDIACom)","13 Feb 2020","2019","","","55","59","Optical Character Recognition is the process of translating images of handwritten, typewritten, or printed text into a format understood by machines. The purposes of Optical Character Recognition are editing, indexing/searching, and reduction in storage size. This is achieved by first scanning the photo of the text character-by-character, then it is followed by analysis of the scanned image, and finally the translation of the character image into character codes, such as ASCII. In this paper, we have used segmentation algorithm to divide the image into lines, words and then characters. The characters are recognized using Convolutional Neural Network The results obtained by Convolutional Neural Networks seems to be promising in recognizing the characters in comparison to results obtained through other machine learning algorithms such as Support Vector Machines and Artificial Neural Networks.","","978-9-3805-4434-2","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8991268","Optical Character Recognition;Convolutional Neural Network;Artificial Neural Network;Support Vector Machines;Segmentation;Image Characters","","convolutional neural nets;handwritten character recognition;image segmentation;learning (artificial intelligence);optical character recognition;support vector machines","machine learning algorithms;segmentation algorithm;character codes;character image;scanned image;text character-by-character;optical character recognition;convolutional neural network","","","","15","","13 Feb 2020","","","IEEE","IEEE Conferences"
"EEG-Based Adaptive Driver-Vehicle Interface Using Variational Autoencoder and PI-TSVM","L. Bi; J. Zhang; J. Lian","School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Neural Systems and Rehabilitation Engineering","17 Oct 2019","2019","27","10","2025","2033","Event-related potential (ERP)-based driver-vehicle interfaces (DVIs) have been developed to provide a communication channel for people with disabilities to drive a vehicle. However, they require a tedious and time-consuming training procedure to build the decoding model, which can translate EEG signals into commands. In this paper, to address this problem, we propose an adaptive DVI by using a new semi-supervised algorithm. The decoding model of the proposed DVI is first built with a small labeled training set, and then gradually improved by updating the proposed semi-supervised decoding model with new collected unlabeled EEG signals. In our semi-supervised algorithm, independent component analysis (ICA) and Kalman smoother are first used to improve the signal-to-noise ratio (SNR). After that, variational autoencoder is applied to provide a robust feature representation of EEG signals. Finally, a prior information-based transductive support vector machine (PI-TSVM) classifier is developed to translate these features into commands. Experimental results show that the proposed DVI can significantly reduce the training effort. After a short updating, its performance can be close to that of the supervised DVI requiring a lengthy training procedure. This work is vital for advancing the application of these DVIs.","1558-0210","","10.1109/TNSRE.2019.2940046","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827606","Brain-controlled vehicle;EEG;semi-supervised learning;variational autoencoder;transductive support vector machine","Electroencephalography;Decoding;Brain modeling;Training;Adaptation models;Kalman filters;Integrated circuits","electroencephalography;feature extraction;independent component analysis;medical signal processing;signal classification;signal representation;support vector machines","prior information-based transductive support vector machine classifier;robust feature representation;SNR;Kalman smoother;ICA;independent component analysis;unlabeled EEG signals;communication channel;EEG-based adaptive driver-vehicle interface;supervised DVI;signal-to-noise ratio;semisupervised decoding model;labeled training set;semisupervised algorithm;adaptive DVI;EEG signals;tedious time-consuming training procedure;event-related potential-based driver-vehicle interfaces;PI-TSVM;variational autoencoder","Adult;Algorithms;Automobile Driving;Brain-Computer Interfaces;Electroencephalography;Female;Healthy Volunteers;Humans;Male;Pattern Recognition, Automated;Principal Component Analysis;Psychomotor Performance;Signal-To-Noise Ratio;Support Vector Machine;Young Adult","4","","34","","9 Sep 2019","","","IEEE","IEEE Journals"
"Assessment of Speech Intelligibility in Parkinson’s Disease Using a Speech-To-Text System","G. Dimauro; V. Di Nicola; V. Bevilacqua; D. Caivano; F. Girardi","Dipartimento di Informatica, Università degli Studi di Bari ‘Aldo Moro’, Bari, Italy; Dipartimento di Scienze Mediche di Base, Neuroscienze e Organi di Senso, Università degli Studi di Bari ‘Aldo Moro’, Bari, Italy; Dipartimento di Ingegneria Elettrica e dell’Informazione, Politecnico di Bari, Bari, Italy; Dipartimento di Informatica, Università degli Studi di Bari ‘Aldo Moro’, Bari, Italy; ASL Bari, Bari, Italy","IEEE Access","7 Nov 2017","2017","5","","22199","22208","Patients with Parkinson's disease (PD) may have difficulties in speaking because of reduced coordination of the muscles that control breathing, phonation, articulation, and prosody. Symptoms that may occur are weakening of the volume of the voice, voice monotony, changes in the quality of the voice, the speed of speech, uncontrolled repetition of words, and difficult speech intelligibility. To date, evaluation of the speech intelligibility is performed based on the unified PD rating scale. Specifically, section 3.1 (eloquence) of the cited scale provides the specialist with some tips to evaluate the patient's speech ability. With the aim of evaluating the speech intelligibility by measuring the variation in parameters in an objective manner, we show that a speech-to-text (STT) system could help specialists to obtain an accurate and objective measure of speech, phrase, and word intelligibility in PD. STT systems are based on methodologies and technologies that enable the recognition and translation of spoken language into text by computers and computerized devices. We decided to base our study on Google STT conversion. We expand Voxtester, a software system for digital assessment of voice and speech changes in PD, in order to perform this study. No previous studies have been presented to address the mentioned challenges based on STT. The experiments here presented are related with detection/classification between pathological speech from patients with PD and regular speech from healthy control group. The results are very interesting and are an important step toward assessing the intelligibility of the speech of PD patients.","2169-3536","","10.1109/ACCESS.2017.2762475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070308","Parkinson’s disease;speech analysis;automatic speech recognition;human voice;speech to text","Speech;Parkinson's disease;Production;Pathology;Acoustics","diseases;medical disorders;medical signal detection;medical signal processing;speech intelligibility;speech processing;text analysis","unified PD rating scale;voice monotony;speech-to-text system;PD patients;regular speech;pathological speech;STT systems;Voxtester software system;Google STT conversion;speech intelligibility;Parkinson disease","","5","","41","","17 Oct 2017","","","IEEE","IEEE Journals"
"Liver segmentation using structured sparse representations","V. Singh; D. Wang; A. H. Tewfik; B. J. Erickson","University of Texas, Austin, USA; University of Texas, Austin, USA; University of Texas, Austin, USA; Mayo Clinic, Rochester, Minnesota, USA","2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","30 Aug 2012","2012","","","565","568","Segmentation of liver from volumetric images forms the basis for surgical planning required for living donor transplantations and tumor resections surgeries. This paper introduces a novel idea of using sparse representations of liver shapes in a learned structured dictionary to produce an accurate preliminary segmentation, which is further evolved using a joint image and shape based level-set framework to obtain the final segmented volume. Structured dictionary for liver shapes can be learned from an available training dataset. The proposed approach requires only 3 orthogonal segmented masks as user-input, which is less than half the number required by current state-of-the-art interaction-based methods. The increased accuracy of the preliminary segmentation translates into faster convergence of the evolution step and highly accurate final segmentations with mean average symmetric surface distances (ASSD) [1] of (1.03±0.3)mm when tested on a challenging dataset containing 62 volumes. Our approach segments a volume on an average of 5 mins and, is 25% (approx.) faster than comparably performing techniques.","2379-190X","978-1-4673-0046-9","10.1109/ICASSP.2012.6287942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6287942","Structured Sparsity;Sparse Representations;Level-set Evolution;Semi-Automatic Segmentation;Subspace Clustering","Liver;Shape;Image segmentation;Training;Dictionaries;Measurement;Computed tomography","computerised tomography;image segmentation;liver;medical image processing;planning;surgery;tumours","liver segmentation;structured sparse representations;volumetric images;surgical planning;donor transplantations;tumor resection surgery;learned structured dictionary;joint image;shape based level-set framework;orthogonal segmented masks;state-of-the-art interaction-based methods;convergence;mean average symmetric surface distances;computerised tomography","","1","","11","","30 Aug 2012","","","IEEE","IEEE Conferences"
"Progressive Transfer Learning and Adversarial Domain Adaptation for Cross-Domain Skin Disease Classification","Y. Gu; Z. Ge; C. P. Bonnington; J. Zhou","Griffith University, Nathan, Australia; Monash University, Clayton, Australia; Monash University, Clayton, Australia; Griffith University, Nathan, Australia","IEEE Journal of Biomedical and Health Informatics","5 May 2020","2020","24","5","1379","1393","Deep learning has been used to analyze and diagnose various skin diseases through medical imaging. However, recent researches show that a well-trained deep learning model may not generalize well to data from different cohorts due to domain shift. Simple data fusion techniques such as combining disease samples from different data sources are not effective to solve this problem. In this paper, we present two methods for a novel task of cross-domain skin disease recognition. Starting from a fully supervised deep convolutional neural network classifier pre-trained on ImageNet, we explore a two-step progressive transfer learning technique by fine-tuning the network on two skin disease datasets. We then propose to adopt adversarial learning as a domain adaptation technique to perform invariant attribute translation from source to target domain in order to improve the recognition performance. In order to evaluate these two methods, we analyze generalization capability of the trained model on melanoma detection, cancer detection, and cross-modality learning tasks on two skin image datasets collected from different clinical settings and cohorts with different disease distributions. The experiments prove the effectiveness of our method in solving the domain shift problem.","2168-2208","","10.1109/JBHI.2019.2942429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846038","Automatic melanoma detection;dermoscopy image;cycle-GAN;deep learning;transfer learning;domain adaptation","Skin;Adaptation models;Melanoma;Deep learning;Data models","cancer;convolutional neural nets;diseases;image classification;image fusion;learning (artificial intelligence);medical image processing;skin","two-step progressive transfer learning;supervised deep convolutional neural network classifier;deep learning model;medical imaging;cross-domain skin disease classification;adversarial domain adaptation;disease distributions;clinical settings;skin image datasets;cross-modality learning tasks;adversarial learning;skin disease datasets;cross-domain skin disease recognition;data fusion","Databases, Factual;Deep Learning;Humans;Image Interpretation, Computer-Assisted;Melanoma;Neural Networks, Computer;Skin;Skin Diseases","3","","44","IEEE","23 Sep 2019","","","IEEE","IEEE Journals"
"Transfer Learning of a Temporal Bone Performance Model via Anatomical Feature Registration","Y. Zhou; I. Ioannou; S. Wijewickrema; J. Bailey; P. Piromchai; G. Kennedy; S. OLeary","Dept. of Comput. & Inf. Syst., Univ. of Melbourne, Melbourne, VIC, Australia; Dept. of Otolaryngology, Univ. of Melbourne, Melbourne, VIC, Australia; Dept. of Otolaryngology, Univ. of Melbourne, Melbourne, VIC, Australia; Dept. of Comput. & Inf. Syst., Univ. of Melbourne, Melbourne, VIC, Australia; Dept. of Otolaryngology, Univ. of Melbourne, Melbourne, VIC, Australia; Centre for the Study of Higher Educ., Univ. of Melbourne, Melbourne, VIC, Australia; NA","2014 22nd International Conference on Pattern Recognition","6 Dec 2014","2014","","","1916","1921","Evaluation of the outcome (end-product) of surgical procedures carried out in virtual reality environments is an essential part of simulation-based surgical training. Automated end-product assessment can be carried out by performance classifiers built from a set of expert performances. When applied to temporal bone surgery simulation, these classifiers can evaluate performance on the bone specimen they were trained on, but they cannot be extended to new specimens. Thus, new expert performances need to be recorded for each new specimen, requiring considerable time commitment from time-poor expert surgeons. To eliminate this need, we propose a transfer learning framework to adapt a classifier built on a single temporal bone specimen to multiple specimens. Once a classifier is trained, we translate each new specimens' features to the original feature space, which allows us to carry out performance evaluation on different specimens using the same classifier. In our experiment, we built a surgical end-product performance classifier from 16 expert trials on a simulated temporal bone specimen. We applied the transfer learning approach to 8 new specimens to obtain machine generated end-products. We also collected end-products for these 8 specimens drilled by a single expert. We then compared the machine generated end-products to those drilled by the expert. The drilled regions generated by transfer learning were similar to those drilled by the expert.","1051-4651","978-1-4799-5209-0","10.1109/ICPR.2014.335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977047","transfer learning;anatomy registration;automatic evaluation","Surgery;Bones;Anatomical structure;Solid modeling;Adaptation models;Decision trees;Training","bone;feature extraction;image classification;image registration;learning (artificial intelligence);medical image processing;surgery;virtual reality","temporal bone performance model;anatomical feature registration;surgical procedures;virtual reality environments;simulation-based surgical training;automated end-product assessment;performance classifiers;temporal bone surgery simulation;bone specimen;time-poor expert surgeons;transfer learning framework;surgical end-product performance classifier;transfer learning approach;machine generated end-products","","1","","17","","6 Dec 2014","","","IEEE","IEEE Conferences"
"Robust ISAR Target Recognition Based on IC-STNs","X. Zhou; X. Bai","National Lab of Radar Signal Processing, Xidian University,Xi'an,China; National Lab of Radar Signal Processing, Xidian University,Xi'an,China","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","30 Mar 2020","2019","","","1","5","Although the deep convolutional neural network (DCNN) have been successfully applied to automatic target recognition (ATR) of ground vehicles based on synthetic aperture radar (SAR), most of the available techniques are not suitable for inverse synthetic aperture radar (ISAR) because they cannot tackle the inherent unknown deformation (e.g. translation and scaling etc.) among the training and test samples. To achieve robust ISAR ATR, this paper designs an effective deep structure based on the Inverse Compositional Spatial Transformer Networks, i.e., IC-STNs. In this structure, the geometric predictor is adopted to adjust the image deformation of ISAR images. Then, robust feature extraction is performed by DCNN, and finally the recognition result is output by the softmax classifier. The proposed network is end-to-end trainable, and could learn the optimal deformation parameters automatically from the training samples. For the ISAR image database generated from electromagnetic (EM) echoes of four satellites, the proposed structure achieves higher recognition accuracy than traditional DCNN and DCNN combining STN. Additionally, it has shown robustness to image deformation like scaling.","2474-2333","978-1-7281-2912-9","10.1109/APSAR46974.2019.9048576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9048576","Inverse synthetic aperture radar (ISAR);automatic target recognition (ATR);deep convolutional neural network (DCNN);image deformation","","convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);radar computing;radar imaging;radar target recognition;synthetic aperture radar;visual databases","effective deep structure;inverse compositional spatial transformer networks;IC-STN;geometric predictor;image deformation;ISAR images;robust feature extraction;recognition result;optimal deformation parameters;training samples;ISAR image database;robust ISAR target recognition;deep convolutional neural network;automatic target recognition;ground vehicles;inverse synthetic aperture radar;inherent unknown deformation;robust ISAR ATR;DCNN","","","","28","","30 Mar 2020","","","IEEE","IEEE Conferences"
"A Real-Time ATC Safety Monitoring Framework Using a Deep Learning Approach","Y. Lin; L. Deng; Z. Chen; X. Wu; J. Zhang; B. Yang","National Key Laboratory of Fundamental Science on Synthetic Vision, Sichuan University, Chengdu, China; College of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory of Fundamental Science on Synthetic Vision, Sichuan University, Chengdu, China; National Key Laboratory of Fundamental Science on Synthetic Vision, Sichuan University, Chengdu, China; National Key Laboratory of Fundamental Science on Synthetic Vision, Sichuan University, Chengdu, China; National Key Laboratory of Fundamental Science on Synthetic Vision, Sichuan University, Chengdu, China","IEEE Transactions on Intelligent Transportation Systems","30 Oct 2020","2020","21","11","4572","4581","A deep learning-based safety monitoring framework for air traffic control (ATC) systems is proposed in this paper to reduce human errors and relieve the controllers' workload by regulating the controlling procedure, eliminating communication misunderstanding, monitoring flight conformance, and detecting potential conflicts. The framework comprises automatic speech recognition (ASR), controlling intent inference (CII), and control safety monitoring (CSM) subsystems. The pipeline of the proposed framework can be described as follows: the ASR subsystem translates the pilot-controller voice communications (PCVCs) into texts, which are then converted to the predefined data structure by the CII subsystem. Three types of air traffic safety measures, including repetition check, flight conformance verification, and potential conflict detection, are finally validated by the CSM subsystem. An improved end-to-end ASR model with convolutional, bidirectional long short-term memory (BLSTM) and fully connected (FC) layers is trained using the connectionist temporal classification loss function. The BLSTM and FC combined CII model is designed to infer the controlling intent and slot filling. A language model is also trained in this subsystem to improve the overall performance of the framework. After converting the PCVCs to ATC data, the CSM subsystem checks the given safety monitoring tasks and sends warnings to the current system. The experimental results show that the proposed ASR model obtains a better performance than that of other approaches, and the tasks in the CII subsystem are fulfilled with a high classification precision. The CSM subsystem is also tested to confirm its safety monitoring function by playing back the data and several simulated instructions. To the best of our knowledge, this is pioneering work in the safety monitoring of flight control by recognizing the PCVCs with deep learning-based methods.","1558-0016","","10.1109/TITS.2019.2940992","National Natural Science Foundation of China; National Key Scientific Instrument and Equipment Development Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846596","Automatic speech recognition;pilot-controller voice communications;controlling intent inference;slot filling;control safety monitoring","Safety;Monitoring;Hidden Markov models;Real-time systems;Task analysis;Neural networks;Deep learning","aerospace safety;air traffic control;data structures;learning (artificial intelligence);neural nets;speech recognition;traffic engineering computing","controlling intent inference;flight control;ASR model;ATC data;language model;connectionist temporal classification loss function;CSM subsystem;flight conformance verification;air traffic safety;CII subsystem;data structure;PCVC;pilot-controller voice communications;ASR subsystem;control safety monitoring subsystems;automatic speech recognition;air traffic control systems;deep learning;real-time ATC safety monitoring framework","","","","49","IEEE","23 Sep 2019","","","IEEE","IEEE Journals"
"Robust Pol-ISAR Target Recognition Based on ST-MC-DCNN","X. Bai; X. Zhou; F. Zhang; L. Wang; R. Xue; F. Zhou","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","22 Nov 2019","2019","57","12","9912","9927","Although the deep convolutional neural network (DCNN) has been successfully applied to automatic target recognition (ATR) of ground vehicles based on synthetic aperture radar (SAR), most of the available techniques are not suitable for inverse synthetic aperture radar (ISAR) because they cannot tackle the inherent unknown deformation (e.g., translation, scaling, and rotation) among the training and test samples. To achieve robust polarimetric-ISAR (Pol-ISAR) ATR, this paper proposes the spatial transformer-multi-channel-deep convolutional neural network, i.e., ST-MC-DCNN. In this structure, we adopt the double-layer spatial transformer network (STN) module to adjust the image deformation of each polarimetric channel and then perform a robust hierarchical feature extraction by MC-DCNN. Finally, we carry out feature fusion in the concatenation layer and output the recognition result by the softmax classifier. The proposed network is end-to-end trainable and could learn the optimal deformation parameters automatically from training samples. For the fully Pol-ISAR image database generated from electromagnetic (EM) echoes of four satellites, the proposed structure achieves higher recognition accuracy than traditional DCNN and MC-DCNN. Additionally, it has shown robustness to image scaling, rotation, and combined deformation.","1558-0644","","10.1109/TGRS.2019.2930112","National Natural Science Foundation of China; Foundation for the Author of National Excellent Doctoral Dissertation of the People's Republic of China; Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804365","Automatic target recognition (ATR);deep convolutional neural network (DCNN);image deformation;inverse synthetic aperture radar (ISAR)","Feature extraction;Scattering;Strain;Target recognition;Shape;Azimuth;Image recognition","convolutional neural nets;feature extraction;image classification;image fusion;image recognition;radar imaging;radar polarimetry;radar target recognition;synthetic aperture radar;telecommunication computing","robust Pol-ISAR target recognition;ST-MC-DCNN;automatic target recognition;inverse synthetic aperture radar;inherent unknown deformation;polarimetric-ISAR ATR;double-layer spatial transformer network module;image deformation;polarimetric channel;robust hierarchical feature extraction;concatenation layer;recognition result;optimal deformation parameters;training samples;Pol-ISAR image database;image scaling;electromagnetic echoes;feature fusion;spatial transformer-multichannel-deep convolutional neural network","","4","","62","IEEE","16 Aug 2019","","","IEEE","IEEE Journals"
"The Receptive Field as a Regularizer in Deep Convolutional Neural Networks for Acoustic Scene Classification","K. Koutini; H. Eghbal-zadeh; M. Dorfer; G. Widmer","Johannes Kepler University,Linz,Austria; Johannes Kepler University,Linz,Austria; Johannes Kepler University,Linz,Austria; Johannes Kepler University,Linz,Austria","2019 27th European Signal Processing Conference (EUSIPCO)","18 Nov 2019","2019","","","1","5","Convolutional Neural Networks (CNNs) have had great success in many machine vision as well as machine audition tasks. Many image recognition network architectures have consequently been adapted for audio processing tasks. However, despite some successes, the performance of many of these did not translate from the image to the audio domain. For example, very deep architectures such as ResNet [1] and DenseNet [2], which significantly outperform VGG [3] in image recognition, do not perform better in audio processing tasks such as Acoustic Scene Classification (ASC). In this paper, we investigate the reasons why such powerful architectures perform worse in ASC compared to simpler models (e.g., VGG). To this end, we analyse the receptive field (RF) of these CNNs and demonstrate the importance of the RF to the generalization capability of the models. Using our receptive field analysis, we adapt both ResNet and DenseNet, achieving state-of-the-art performance and eventually outperforming the VGG-based models. We introduce systematic ways of adapting the RF in CNNs, and present results on three data sets that show how changing the RF over the time and frequency dimensions affects a model's performance. Our experimental results show that very small or very large RFs can cause performance degradation, but deep models can be made to generalize well by carefully choosing an appropriate RF size within a certain range.","2076-1465","978-9-0827-9703-9","10.23919/EUSIPCO.2019.8902732","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902732","CNN;acoustic scene classification;deep learning;machine learning","Radio frequency;Computer architecture;Task analysis;Training;Acoustics;Neurons;Convolution","audio signal processing;computer vision;feature extraction;image classification;learning (artificial intelligence);neural nets","machine vision;machine audition tasks;image recognition network architectures;audio processing tasks;audio domain;deep architectures;ResNet;acoustic scene classification;ASC;powerful architectures;CNNs;receptive field analysis;VGG-based models;performance degradation;deep models;appropriate RF size;deep convolutional Neural Networks","","7","","19","","18 Nov 2019","","","IEEE","IEEE Conferences"
"A Novel Positive Transfer Learning Approach for Telemonitoring of Parkinson’s Disease","H. Yoon; J. Li","Industrial Engineering Program, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA; Industrial Engineering Program, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA","IEEE Transactions on Automation Science and Engineering","4 Jan 2019","2019","16","1","180","191","Telemonitoring is the use of electronic devices to remotely monitor patients. Taking the Parkinson's disease (PD) as an example, the use of at-home testing device (AHTD) enables remote, internet-based measurement of PD vocal symptoms. Translating AHTD measurement into a unified PD rating scale (UPDRS) through predictive analytics enables cost-effective, convenient, and close tracking of PD progression. Building a predictive model between AHTD measurement and UPDRS is not straightforward because PD patients are highly heterogeneous, which requires patient-specific models. Learning a patient-specific model faces the challenge of limited data. Transfer learning (TL) tackles this challenge by leveraging other patients' information to make up the data shortage when modeling a target patient. Among different TL methods, the category of parameter transfer methods is more appropriate for the telemonitoring application because it transfers patient-specific model parameters but not patients' data. However, existing parameter transfer methods fall short because not every other patient's information is helpful and blind transfer causes the problem of negative transfer. To tackle this limitation, we propose a positive TL (PTL) method. We provide an in-depth theoretical study on the risk and condition for negative transfer to happen, which further drive the development of novel PTL algorithms that are robust to negative transfer. We apply PTL to predict UPDRS of 42 PD patients using their AHTD vocal measurement. PTL achieves significantly better accuracy compared with single learning and one-model-fits-all approaches.","1558-3783","","10.1109/TASE.2018.2874233","Division of Civil, Mechanical and Manufacturing Innovation; Division of Civil, Mechanical and Manufacturing Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520887","Machine learning;negative transfer;telemonitoring;transfer learning (TL)","Data models;Predictive models;Diseases;Machine learning;Monitoring;Prediction algorithms","biomedical measurement;diseases;learning (artificial intelligence);medical signal processing;patient monitoring;telemedicine","Parkinson's disease;electronic devices;at-home testing device;internet-based measurement;PD vocal symptoms;unified PD rating scale;UPDRS;data shortage;telemonitoring application;patient-specific model parameters;blind transfer;negative transfer;positive TL method;AHTD vocal measurement;parameter transfer method;positive transfer learning approach","","2","","33","","2 Nov 2018","","","IEEE","IEEE Journals"
"Radar target recognition using time-frequency analysis and polar transformation","J. Cexus; A. Toumi","ENSTA Bretagne - Lab-STICC, UMR CNRS 6285, 2, Rue François Verny, 29806 Brest Cedex 9, France; ENSTA Bretagne - Lab-STICC, UMR CNRS 6285, 2, Rue François Verny, 29806 Brest Cedex 9, France","2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)","24 May 2018","2018","","","1","6","A new method for Automatic Radar Targets Recognition is presented based on Inverse Synthetic Aperture Radar (ISAR). In this work, the first step is to construct ISAR images via a Non uniformly Sampled Bivariate Empirical Mode Decomposition Time-Frequency Distribution (NSBEMD-TFD) method. Indeed, this Time-Frequency representation is well suited for non-stationary signals analysis and provides high resolution with good accuracy. The obtained ISAR images is used to provide the evolution of two-dimensional spatial distribution of a moving target and, therefore, its are suitable to be used for radar target recognition tasks. In second step, a feature vectors are extracted from each ISAR images in order to describe the discriminative informations about a target. In the features extraction step, we computed several rings of polar space applied on ISAR image. Then, these rings is projected on 1-D vector. To ensure translation invariance of the obtained projected 1-D vector, a Fourier Descriptors are computed. In third step of this work, the recognition task is achieved using k-Nearest Neighbors (K-NN), Fuzzy k-NN, Neural network and Bayesian classifiers. To validate our approach, simulation results are presented on a set of several targets constituted by ideal point scatterers models.","","978-1-5386-5239-8","10.1109/ATSIP.2018.8364500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8364500","Target recognition;Inverse Synthetic Aperture Radar;Machine learning;Time-Frequency Analysis;Empirical Mode Decomposition","Radar imaging;Feature extraction;Target recognition;Time-frequency analysis;Empirical mode decomposition;Imaging","Bayes methods;feature extraction;image classification;image representation;neural nets;radar computing;radar imaging;radar target recognition;synthetic aperture radar;time-frequency analysis","recognition task;time-frequency analysis;polar transformation;ISAR image;two-dimensional spatial distribution;moving target;automatic radar target recognition;inverse synthetic aperture radar;time-frequency representation;nonstationary signal analysis;feature extraction step;1D vector;radar target recognition task;nonuniformly sampled bivariate empirical mode decomposition time-frequency distribution method;NSBEMD-TFD method","","1","","41","","24 May 2018","","","IEEE","IEEE Conferences"
"Towards extending real-time EMG-based gesture recognition system","C. Andronache; M. Negru; A. Neacsu; G. Cioroiu; A. Radoi; C. Burileanu","University Politehnica of Bucharest,Center for Advanced Research on New Materials, Products and Innovative Processes; University Politehnica of Bucharest,Center for Advanced Research on New Materials, Products and Innovative Processes; University Politehnica of Bucharest,Center for Advanced Research on New Materials, Products and Innovative Processes; University Politehnica of Bucharest,Center for Advanced Research on New Materials, Products and Innovative Processes; University Politehnica of Bucharest,Center for Advanced Research on New Materials, Products and Innovative Processes; University Politehnica of Bucharest,Center for Advanced Research on New Materials, Products and Innovative Processes","2020 43rd International Conference on Telecommunications and Signal Processing (TSP)","11 Aug 2020","2020","","","301","304","In biomedical applications, the Electromyographic (EMG) signal is used to record the electrical activity of the muscles during their contraction. EMG signal classification stands at the core of real time systems that aim to discriminate between user's movements without relying on other environmental conditions (as it is the case with gesture classification based on video). The classification of EMG signals is usually used to translate the sign language or to design computer interfaces based on gesture recognition. In this paper, we propose a continuation of our previous work, on a real-time automatic hand gesture recognition using fully connected neural networks and temporal features extracted from the EMG signals. This approach leads to a recognition of an increased number of gestures, starting from a system trained for discriminating between a small number of classes of recognized gestures. More precisely, using an innovative transfer learning method, we preserved the performance of the system (99.31% overall accuracy compared to previous 99.78%), while doubling the number of recognized gestures.","","978-1-7281-6376-5","10.1109/TSP49548.2020.9163481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163481","classification;EMG;machine learning;neural networks;transfer learning","Electromyography;Gesture recognition;Muscles;Real-time systems;Training;Wrist;Learning systems","electromyography;feature extraction;gesture recognition;learning (artificial intelligence);medical signal processing;neural nets;signal classification","real-time automatic hand gesture recognition;fully connected neural networks;EMG signals;recognized gestures;real-time EMG-based gesture recognition system;biomedical applications;Electromyographic signal;electrical activity;EMG signal classification;time systems;gesture classification","","","","16","","11 Aug 2020","","","IEEE","IEEE Conferences"
"Assessment of Shift-Invariant CNN Gaze Mappings for PS-OG Eye Movement Sensors","H. K. Griffith; D. Katrychuk; O. V. Komogortsev","Texas State University, USA; Texas State University, USA; Texas State University, USA","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3651","3659","Photosensor oculography (PS-OG) eye movement sensors offer desirable performance characteristics for integration within wireless head mounted devices (HMDs), including low power consumption and high sampling rates. To address the known performance degradation of these sensors due to HMD shifts, various machine learning techniques have been proposed for mapping sensor outputs to gaze location. This paper advances the understanding of a recently introduced convolutional neural network designed to provide shift invariant gaze mapping within a specified range of sensor translations. Performance is assessed for shift training examples which better reflect the distribution of values that would be generated through manual repositioning of the HMD during a dedicated collection of training data. The network is shown to exhibit comparable accuracy for this realistic shift distribution versus a previously considered rectangular grid, thereby enhancing the feasibility of in-field set-up. In addition, this work further demonstrates the practical viability of the proposed initialization process by demonstrating robust mapping performance versus training data scale. The ability to maintain reasonable accuracy for shifts extending beyond those introduced during training is also demonstrated.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022315","eye tracking, virtual reality, photo sensor oculography, machine learning","Training data;Training;Sensor arrays;Resists;Graphical models;Distribution functions","computer vision;convolutional neural nets;gaze tracking;helmet mounted displays;learning (artificial intelligence)","wireless head mounted devices;photosensor oculography eye movement sensors;PS-OG eye movement sensors;shift-invariant CNN gaze mappings;realistic shift distribution;convolutional neural network;machine learning techniques;HMD shifts;performance degradation;low power consumption","","1","","10","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Encoding Invariances in Remote Sensing Image Classification With SVM","E. Izquierdo-Verdiguier; V. Laparra; L. Gómez-Chova; G. Camps-Valls","Image Processing Laboratory, University of Valencia, Paterna, Spain; Image Processing Laboratory, University of Valencia, Paterna, Spain; Image Processing Laboratory, University of Valencia , Paterna, Spain; Image Processing Laboratory, University of Valencia, Paterna, Spain","IEEE Geoscience and Remote Sensing Letters","13 Jun 2013","2013","10","5","981","985","This letter introduces a simple method for including invariances in support-vector-machine (SVM) remote sensing image classification. We design explicit invariant SVMs to deal with the particular characteristics of remote sensing images. The problem of including data invariances can be viewed as a problem of encoding prior knowledge, which translates into incorporating informative support vectors (SVs) that better describe the classification problem. The proposed method essentially generates new (synthetic) SVs from the obtained by training a standard SVM with the available labeled samples. Then, original and transformed SVs are used for training the virtual SVM introduced in this letter. We first incorporate invariances to rotations and reflections of image patches for improving contextual classification. Then, we include an invariance to object scale in patch-based classification. Finally, we focus on the challenging problem of including illumination invariances to deal with shadows in the images. Very good results are obtained when few labeled samples are available for classification. The obtained classifiers reveal enhanced sparsity and robustness. Interestingly, the methodology can be applied to any maximum-margin method, thus constituting a new research opportunity.","1558-0571","","10.1109/LGRS.2012.2227297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392856","Image classification;invariance;support vector machine (SVM)","Support vector machines;Remote sensing;Training;Standards;Image coding;Kernel;Encoding","geophysical image processing;geophysical techniques;image classification;remote sensing;support vector machines","invariance encoding;SVM remote sensing image classification;support-vector-machine;data invariances;informative support vectors;classification problem;virtual SVM;contextual classification;patch-based classification;object scale;maximum-margin method","","33","","19","","24 Dec 2012","","","IEEE","IEEE Journals"
"Classification of motor imagery tasks with LS-SVM in EEG-based self-paced BCI","M. E. A. Abdel-Hadi; R. A. El-Khoribi; M. I. Shoman; M. M. Refaey","Cairo University Giza, Egypt; Cairo University Giza, Egypt; Cairo University Giza, Egypt; Cairo University Giza, Egypt","2015 Fifth International Conference on Digital Information Processing and Communications (ICDIPC)","12 Nov 2015","2015","","","244","249","Motivated by the need to deal with critical disorders that involve death of neurons, such as Amyotrophic Lateral Sclerosis (ALS) and brainstem stroke, interpretation of the brain's Motor Imagery (MI) activities is highly needed. Brain signals can be translated into control commands. Electroencephalography (EEG) is considered in this work, EEG is a low-cost non-invasive technique. A big challenge is faced due to the poor signal-to-noise ratio of EEG signals. The dataset used in this work is based on asynchronous or self-paced motor imagery problem. The used self-paced Brain Computer Interface (BCI) problem poses a considerable challenge by introducing an additional class, a relax class, or non-intentional control periods that are not included in the training set and should be classified. In this work, a number of subject dependent parameters and their values are determined. These parameters are: the best frequency range, the best Common Spatial Pattern (CSP) channels, and the number of these CSP channels. System parameters are determined dynamically in the offline training phase. Energy based features are extracted afterwards from the best selected signals. The Least-Squares Support Vector Machine (LS-SVM) classifier is used as a classification back end. Results of the proposed system show superiority over the previously introduced systems in terms of the Mean Square Error (MSE) when tested on the Berlin BCI (BBCI) competition IV dataset 1.","","978-1-4673-6832-2","10.1109/ICDIPC.2015.7323036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7323036","self-paced;BCI competition IV dataset 1;support vector machine;motor imagery;EEG classification","Band-pass filters;Electroencephalography;Feature extraction;Information filters;Training","brain-computer interfaces;electroencephalography;least squares approximations;medical signal processing;signal classification;support vector machines","motor imagery task classification;MI;least-squares support vector machine;LS-SVM;electroencephalography;EEG;brain computer interface;BCI;common spatial pattern;CSP channel","","6","","20","","12 Nov 2015","","","IEEE","IEEE Conferences"
"A Multimodal Adaptive Wireless Control Interface for People With Upper-Body Disabilities","C. L. Fall; F. Quevillon; M. Blouin; S. Latour; A. Campeau-Lecours; C. Gosselin; B. Gosselin","Department of Electrical Engineering, Laval University, Quebec City, QC, Canada; Department of Electrical Engineering, Laval University, Quebec City, QC, Canada; Kinova Robotics, QC, Canada; Kinova Robotics, QC, Canada; Department of Mechanical Engineering, Laval University, Quebec City, QC, Canada; Department of Mechanical Engineering, Laval University, Quebec City, QC, Canada; Department of Electrical Engineering, Laval University, Quebec City, QC, Canada","IEEE Transactions on Biomedical Circuits and Systems","5 Jun 2018","2018","12","3","564","575","This paper describes a multimodal body-machine interface (BoMI) to help individuals with upper-limb disabilities using advanced assistive technologies, such as robotic arms. The proposed system uses a wearable and wireless body sensor network (WBSN) supporting up to six sensor nodes to measure the natural upper-body gesture of the users and translate it into control commands. Natural gesture of the head and upper-body parts, as well as muscular activity, are measured using inertial measurement units (IMUs) and surface electromyography (sEMG) using custom-designed multimodal wireless sensor nodes. An IMU sensing node is attached to a headset worn by the user. It has a size of 2.9 cm × 2.9 cm, a maximum power consumption of 31 mW, and provides angular precision of 1°. Multimodal patch sensor nodes, including both IMU and sEMG sensing modalities are placed over the user able-body parts to measure the motion and muscular activity. These nodes have a size of 2.5 cm × 4.0 cm and a maximum power consumption of 11 mW. The proposed BoMI runs on a Raspberry Pi. It can adapt to several types of users through different control scenarios using the head and shoulder motion, as well as muscular activity, and provides a power autonomy of up to 24 h. JACO, a 6-DoF assistive robotic arm, is used as a testbed to evaluate the performance of the proposed BoMI. Ten able-bodied subjects performed ADLs while operating the AT device, using the Test d' Evaluation des Membres Superieurs de Personnes Agees to evaluate and compare the proposed BoMI with the conventional joystick controller. It is shown that the users can perform all tasks with the proposed BoMI, almost as fast as with the joystick controller, with only 30% time overhead on average, while being potentially more accessible to the upper-body disabled who cannot use the conventional joystick controller. Tests show that control performance with the proposed BoMI improved by up to 17% on average, after three trials.","1940-9990","","10.1109/TBCAS.2018.2810256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8331092","Assistive technologies;body-machine interface;electromyography;inertial sensor;low-power;motion control;multimodal;wireless body sensor network","Manipulators;Robot sensing systems;Mobile robots;Control systems;Wireless sensor networks;Wireless communication","biomechanics;body sensor networks;electromyography;gesture recognition;handicapped aids;human-robot interaction;interactive devices;manipulators;medical robotics;medical signal processing;wireless sensor networks","multimodal adaptive wireless control interface;upper-body disabilities;multimodal body-machine interface;BoMI;upper-limb disabilities;advanced assistive technologies;robotic arms;wearable body sensor network;wireless body sensor network;natural upper-body gesture;control commands;natural gesture;upper-body parts;muscular activity;sEMG;multimodal wireless sensor nodes;IMU sensing node;maximum power consumption;multimodal patch sensor nodes;able-body parts;shoulder motion;power autonomy;6-DoF assistive robotic arm;able-bodied subjects;Test d Evaluation des Membres Superieurs de Personnes Agees;control performance;size 2.5 cm;size 4.0 cm;time 24.0 hour;size 2.9 cm","Brain-Computer Interfaces;Disabled Persons;Electromyography;Female;Humans;Male;Self-Help Devices;Wireless Technology","4","","43","","4 Apr 2018","","","IEEE","IEEE Journals"
"Facial Expression Recognition from 3D Facial Landmarks Reconstructed from Images","L. Kalapala; H. Yadav; H. Kharwar; S. Susan","Delhi Technological University,Shabad Daulatpur, Bawana Road,Delhi,India,110042; Delhi Technological University,Shabad Daulatpur, Bawana Road,Delhi,India,110042; Delhi Technological University,Shabad Daulatpur, Bawana Road,Delhi,India,110042; Delhi Technological University,Shabad Daulatpur, Bawana Road,Delhi,India,110042","2020 IEEE International Symposium on Sustainable Energy, Signal Processing and Cyber Security (iSSSC)","25 Feb 2021","2020","","","1","5","Direct classification of normalized and flattened 3D facial landmarks reconstructed from 2D images is proposed in this paper for recognizing eight types of facial expressions depicting the emotions of- sadness, anger, contempt, disgust, fear, happiness, neutral and surprised. The first stage is the 3D projection of 2D facial landmarks. The pre-trained convolutional Face Alignment Network (FAN) proposed recently for 2D/3D face alignment is used for the purpose. The 3D cartesian coordinates are translated to the spherical coordinate system prior to the classification stage. A variety of classifiers are tried and tested for classifying the 68 facial landmarks, for both the coordinate systems. The benchmark CK+ video dataset is used for the experimentation; the last frame of each video that depicts the peak of each emotion is used as the input image. The experimental results indicate that direct classification of normalized and flattened 3D facial landmarks in the spherical coordinate system yields the highest accuracy for the support vector machine classifier with grid search for determining optimal parameters.","","978-1-7281-8880-5","10.1109/iSSSC50941.2020.9358815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358815","Facial expression recognition;Face alignment network;Support Vector Machine;3D facial landmarks;Spherical coordinate system","Fans;Three-dimensional displays;Image recognition;Face recognition;Two dimensional displays;Support vector machine classification;Image reconstruction","emotion recognition;face recognition;image classification;image reconstruction;support vector machines","pre-trained convolutional Face Alignment Network;3D cartesian coordinates;spherical coordinate system;classification stage;direct classification;facial expression recognition;3D facial landmarks;2D facial landmarks;support vector machine classifier;facial landmarks","","","","35","","25 Feb 2021","","","IEEE","IEEE Conferences"
"The Hardware and Algorithm Co-Design for Energy-Efficient DNN Processor on Edge/Mobile Devices","J. Lee; S. Kang; J. Lee; D. Shin; D. Han; H. -J. Yoo","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Circuits and Systems I: Regular Papers","28 Sep 2020","2020","67","10","3458","3470","Deep neural network (DNN) has been widely studied due to its high performance and usability for various applications such as image classification, detection, segmentation, translation, and action recognition. Thanks to the universal applications and high performance of DNN algorithm, DNN is adopted for various AI platforms, including edge/mobile devices as well as cloud servers. However, high-performance DNN requires a large amount of computation and memory access, making it challenging to implement DNN operation on edge/mobile. There have been several ways to solve these problems, including algorithms as well as hardware for DNN. Algorithms that help accelerate DNN in hardware enable much more efficient operation of high-performance AI. This article aims to provide an overview of the recent hardware and algorithm co-design schemes enabling efficient processing of DNNs. Specifically, it will provide algorithm optimization methods for DNN structure, neurons, synapses, and data types. This paper also introduces optimization methods for hardware architectures, PE array, data-path control, and microarchitecture of PE. And we will also show examples of DNN algorithm and hardware co-designed ASICs.","1558-0806","","10.1109/TCSI.2020.3021397","Samsung Research Funding & Incubation Center for Future Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9189797","Application specific integrated circuit (ASIC);co-design;deep learning (DL);deep neural network (DNN);energy efficient hardware;hardware friendly algorithm;machine learning (ML)","Hardware;Optimization;Computer architecture;Neurons;Artificial intelligence;Performance evaluation;Synapses","application specific integrated circuits;image classification;image recognition;image segmentation;mobile computing;neural nets;optimisation","energy-efficient DNN processor;deep neural network;universal applications;high-performance AI;algorithm co-design schemes;algorithm optimization methods;DNN structure;hardware architectures;hardware co-designed ASICs;hardware co-design schemes;data-path control;PE array;PE microarchitecture;edge/mobile devices;image classification;image detection;image segmentation;action recognition","","1","","60","IEEE","9 Sep 2020","","","IEEE","IEEE Journals"
"Human movement intentions based on EEG using brain computer interfaces","M. K. Ishak; M. Dyson","School of Electrical and Electronic Engineering, Engineering Campus, Universiti Sains Malaysia, 14300 Nibong Tebal, Penang, Malaysia; Brain Computer Interface (BCI) Group, School of Computer Science and Electronic Engineering, University of Essex, Colchester, UK, United Kingdom","2015 International Conference on Control, Electronics, Renewable Energy and Communications (ICCEREC)","30 Nov 2015","2015","","","58","62","This paper proposes classifying the signal of movement intention and identifying feature selection and translation algorithms. Furthermore, this paper will select the most appropriate algorithms for the feature classification of the signal of movement intentions. The study uses signals previously recorded in the BCI lab. Feature selection and classification were based on the Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA). The results of classification show that LDA classifier recorded the highest accuracy in 3 and 4-class of movement in comparison to the SVM. LDA classified the 4-class of movements at central channel and single channel with the average accuracy of 43.75% and 42%. Overall, LDA performed better result in 3-class of movement, with an average accuracy 62%.","","978-1-4799-8975-1","10.1109/ICCEREC.2015.7337054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7337054","EEG;Linear Discriminant Analysis (LDA);Support Vector Machine (SVM) and movement intention","Support vector machines;Extraterrestrial measurements;Logic gates;Electrodes","brain-computer interfaces;electroencephalography;medical signal processing;support vector machines","human movement intention;EEG;brain computer interface;feature selection;feature classification;support vector machine;SVM;linear discriminant analysis;LDA classifier","","","","24","","30 Nov 2015","","","IEEE","IEEE Conferences"
"Learning Discriminative Spatiospectral Features of ERPs for Accurate Brain–Computer Interfaces","B. Abibullaev; A. Zollanvari","Department of Robotics and Mechatronics, Nazarbayev University, Astana, Kazakhstan; Department of Electrical and Computer Engineering, Nazarbayev University, Astana, Kazakhstan","IEEE Journal of Biomedical and Health Informatics","4 Sep 2019","2019","23","5","2009","2020","Constructing accurate predictive models is at the heart of brain-computer interfaces (BCIs) because these models can ultimately translate brain activities into communication and control commands. The majority of the previous work in BCI use spatial, temporal, or spatiotemporal features of event-related potentials (ERPs). In this study, we examined the discriminatory effect of their spatiospectral features to capture the most relevant set of neural activities from electroencephalographic recordings that represent users' mental intent. In this regard, we model ERP waveforms using a sum of sinusoids with unknown amplitudes, frequencies, and phases. The effect of this signal modeling step is to represent high-dimensional ERP waveforms in a substantially lower dimensionality space, which includes their dominant power spectral contents. We found that the most discriminative frequencies for accurate decoding of visual attention modulated ERPs lie in a spectral range less than 6.4 Hz. This was empirically verified by treating dominant frequency contents of ERP waveforms as feature vectors in the state-of-the-art machine learning techniques used herein. The constructed predictive models achieved remarkable performance, which for some subjects was as high as 94% as measured by the area under curve. Using these spectral contents, we further studied the discriminatory effect of each channel and proposed an efficient strategy to choose subject-specific subsets of channels that generally led to classifiers with comparable performance.","2168-2208","","10.1109/JBHI.2018.2883458","Nazarbayev University Faculty Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613780","Brain-computer interfaces;ERPs;P300;EEG;signal processing;machine learning","Feature extraction;Electroencephalography;Electrodes;Visualization;Predictive models;Brain modeling;Machine learning","bioelectric potentials;brain;brain-computer interfaces;electroencephalography;feature extraction;learning (artificial intelligence);medical signal processing;neurophysiology","brain activities;control commands;BCI use spatial;spatiotemporal features;event-related potentials;neural activities;electroencephalographic recordings;high-dimensional ERP waveforms;discriminative frequencies;feature vectors;discriminative spatiospectral features;machine learning techniques;visual attention modulated ERP;ERP waveforms;brain-computer interfaces;frequency 6.4 Hz","Adult;Brain-Computer Interfaces;Electroencephalography;Evoked Potentials;Humans;Machine Learning;Signal Processing, Computer-Assisted;Young Adult","3","","93","OAPA","16 Jan 2019","","","IEEE","IEEE Journals"
"FPGA-based object detection processor with HOG feature and SVM classifier","F. An; P. Xu; Z. Xiao; C. Wang",Southern University of Science and Technology; Southern University of Science and Technology; Southern University of Science and Technology; Huazhong University of Science and Technology,"2019 32nd IEEE International System-on-Chip Conference (SOCC)","7 May 2020","2019","","","187","190","Computer vision is an important sensing technique to translate the information to decisions. In robotic applications, object detection is a critical skill to perform tasks for robots in complex environments. The deep-learning framework, e.g. You Only Look Once (YOLO), attracts much more attention recently. Moreover, it is not an optimal solution for a mobile robot since it requires a large scale of hardware resources, on-chip SRAMs, and power consumption. In this work, we report an object detection processor synchronizing the image sensor in FPGA with a cell-based histogram of oriented gradient (HOG) feature descriptor and support vector machine (SVM) classifier by parallel sliding window mechanism. The HOG feature extraction circuitry with pixel-based pipelined architecture constructs the cell-based feature vectors for parallelizing partial SVM-based classification in multiple sliding windows. The SVM classification produces the final result after accumulating the vector components in one sliding window. This framework can be used to both localize and recognize multiple objects in video footage. The proposed object processor, in which the SVM classifier is trained by INRIA datasets, is implemented and verified on Intel Stratix IV FPGA for the pedestrian.","2164-1706","978-1-7281-3483-3","10.1109/SOCC46988.2019.1570558044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9087991","Computer vision;object detection processor;histogram of oriented gradient (HOG);support vector machine (SVM)","","feature extraction;field programmable gate arrays;image classification;image sensors;learning (artificial intelligence);mobile robots;object recognition;robot vision;support vector machines","FPGA-based object detection processor;SVM classifier;computer vision;important sensing technique;robotic applications;critical skill;complex environments;deep-learning framework;optimal solution;mobile robot;hardware resources;on-chip SRAM;power consumption;image sensor;cell-based histogram;window mechanism;HOG feature extraction circuitry;pixel-based pipelined architecture;cell-based feature vectors;partial SVM-based classification;multiple sliding windows;SVM classification;vector components;sliding window;multiple objects;object processor;Intel Stratix IV FPGA","","","","8","","7 May 2020","","","IEEE","IEEE Conferences"
"A Learning Scheme for EMG Based Decoding of Dexterous, In-Hand Manipulation Motions","A. Dwivedi; Y. Kwon; A. J. McDaid; M. Liarokapis","Department of Mechanical Engineering, New Dexterity Research Group, The University of Auckland, Auckland, New Zealand; Department of Mechanical Engineering, New Dexterity Research Group, The University of Auckland, Auckland, New Zealand; Department of Mechanical Engineering, The University of Auckland, Auckland, New Zealand; Department of Mechanical Engineering, New Dexterity Research Group, The University of Auckland, Auckland, New Zealand","IEEE Transactions on Neural Systems and Rehabilitation Engineering","14 Oct 2019","2019","27","10","2205","2215","Electromyography (EMG) based interfaces are the most common solutions for the control of robotic, orthotic, prosthetic, assistive, and rehabilitation devices, translating myoelectric activations into meaningful actions. Over the last years, a lot of emphasis has been put into the EMG based decoding of human intention, but very few studies have been carried out focusing on the continuous decoding of human motion. In this work, we present a learning scheme for the EMG based decoding of object motions in dexterous, in-hand manipulation tasks. We also study the contribution of different muscles while performing these tasks and the effect of the gender and hand size in the overall decoding accuracy. To do that, we use EMG signals derived from 16 muscle sites (8 on the hand and 8 on the forearm) from 11 different subjects and an optical motion capture system that records the object motion. The object motion decoding is formulated as a regression problem using the Random Forests methodology. Regarding feature selection, we use the following time-domain features: root mean square, waveform length and zero crossings. A 10-fold cross validation procedure is used for model assessment purposes and the feature variable importance values are calculated for each feature. This study shows that subject specific, hand specific, and object specific decoding models offer better decoding accuracy that the generic models.","1558-0210","","10.1109/TNSRE.2019.2936622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809258","Electromyography;human-robot interaction;learning systems;machine learning;prosthetics","Muscles;Decoding;Electromyography;Robots;Task analysis;Feature extraction;Brain modeling","biomechanics;electromyography;encoding;medical signal processing;muscle;orthotics;patient rehabilitation;prosthetics;random forests;regression analysis","EMG signals;optical motion capture system;object motion decoding;decoding accuracy;learning scheme;in-hand manipulation motions;electromyography-based interfaces;robotic rehabilitation devices;orthotic rehabilitation devices;prosthetic rehabilitation devices;assistive rehabilitation devices;continuous decoding;human motion;in-hand manipulation tasks;EMG-based decoding;rehabilitation devices;myoelectric activations;random forest methodology;root mean square method;time-domain features","Adult;Algorithms;Biomechanical Phenomena;Electromyography;Female;Forearm;Hand;Healthy Volunteers;Humans;Machine Learning;Male;Movement;Muscle, Skeletal;Prostheses and Implants;Prosthesis Design;Reproducibility of Results;Robotics;Young Adult","5","","49","","21 Aug 2019","","","IEEE","IEEE Journals"
"Wireless and Wearable EEG System for Evaluating Driver Vigilance","C. Lin; C. Chuang; C. Huang; S. Tsai; S. Lu; Y. Chen; L. Ko","Institute of Electrical Control Engineering and the Brain Research Center, National Chiao Tung University, Hsinchu, Taiwan; Institute of Electrical Control Engineering and the Brain Research Center, National Chiao Tung University, Hsinchu, Taiwan; Institute of Electrical Control Engineering and the Brain Research Center, National Chiao Tung University, Hsinchu, Taiwan; Institute of Computer Science and Engineering and the Brain Research Center, National Chiao Tung University, Hsinchu, Taiwan; Institute of Electrical Control Engineering and the Brain Research Center, National Chiao Tung University, Hsinchu, Taiwan; Institute of Electrical Control Engineering and the Brain Research Center, National Chiao Tung University, Hsinchu, Taiwan; Department of Biological Science and Technology and the Brain Research Center, National Chiao Tung University, Hsinchu, Taiwan","IEEE Transactions on Biomedical Circuits and Systems","23 May 2014","2014","8","2","165","176","Brain activity associated with attention sustained on the task of safe driving has received considerable attention recently in many neurophysiological studies. Those investigations have also accurately estimated shifts in drivers' levels of arousal, fatigue, and vigilance, as evidenced by variations in their task performance, by evaluating electroencephalographic (EEG) changes. However, monitoring the neurophysiological activities of automobile drivers poses a major measurement challenge when using a laboratory-oriented biosensor technology. This work presents a novel dry EEG sensor based mobile wireless EEG system (referred to herein as Mindo) to monitor in real time a driver's vigilance status in order to link the fluctuation of driving performance with changes in brain activities. The proposed Mindo system incorporates the use of a wireless and wearable EEG device to record EEG signals from hairy regions of the driver conveniently. Additionally, the proposed system can process EEG recordings and translate them into the vigilance level. The study compares the system performance between different regression models. Moreover, the proposed system is implemented using JAVA programming language as a mobile application for online analysis. A case study involving 15 study participants assigned a 90 min sustained-attention driving task in an immersive virtual driving environment demonstrates the reliability of the proposed system. Consistent with previous studies, power spectral analysis results confirm that the EEG activities correlate well with the variations in vigilance. Furthermore, the proposed system demonstrated the feasibility of predicting the driver's vigilance in real time.","1940-9990","","10.1109/TBCAS.2014.2316224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818437","Brain computer interface;dry electroencephalographic (EEG) system;machine learning;vigilance monitoring","Electroencephalography;Wireless communication;Wireless sensor networks;Biomedical monitoring;Monitoring;Brain models","biomedical equipment;biosensors;electroencephalography;Java;medical signal processing;neurophysiology;programming languages;regression analysis;virtual reality;wireless sensor networks","wearable EEG system;driver vigilance status;brain activity;safe driving task;arousal;fatigue;task performance;electroencephalographic changes;neurophysiological activities;automobile drivers;laboratory-oriented biosensor technology;novel dry EEG sensor;mobile wireless EEG system;driving performance;Mindo system;EEG signal recording;hairy regions;regression models;JAVA programming language;online analysis;sustained-attention driving task;immersive virtual driving environment","Attention;Automobile Driving;Brain-Computer Interfaces;Clothing;Electroencephalography;Humans;Signal Processing, Computer-Assisted;Sleep Stages;Wireless Technology","92","","60","","19 May 2014","","","IEEE","IEEE Journals"
"A Generative Adversarial Neural Network for Beamforming Ultrasound Images : Invited Presentation","A. A. Nair; T. D. Tran; A. Reiter; M. A. L. Bell","Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD","2019 53rd Annual Conference on Information Sciences and Systems (CISS)","18 Apr 2019","2019","","","1","6","Plane wave ultrasound imaging is an ideal approach to achieve maximum real-time frame rates. However, multiple plane wave insonifications at different angles are often combined to improve image quality, reducing the throughput of the system. We are exploring deep learning-based ultrasound image formation methods as an alternative to this beamforming process by extracting critical information directly from raw radio-frequency channel data from a single plane wave insonification prior to the application of receive time delays. In this paper, we investigate a Generative Adversarial Network (GAN) architecture for the proposed task. This network was trained with over 50,000 FieldII simulations, each containing a single cyst in tissue insonified by a single plane wave. The GAN is trained to produce two outputs - a Deep Neural Network (DNN) B-mode image trained to match a Delay-and-Sum (DAS) beamformed B-mode image and a DNN segmentation trained to match the true segmentation of the cyst from surrounding tissue. We systematically investigate the benefits of feature sharing and discriminative loss during GAN training. Our overall best performing network architecture (with feature sharing and discriminative loss) obtained a PSNR score of 29.38 dB with the simulated test set and 14.86 dB with a tissue-mimicking phantom. The DSC scores were 0.908 and 0.79 for the simulated and phantom data, respectively. The successful translation of the feature representations learned by the GAN to phantom data demonstrates the promise that deep learning holds as an alternative to the traditional ultrasound information extraction pipeline.","","978-1-7281-1151-3","10.1109/CISS.2019.8692835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8692835","Deep Learning;Generative Adversarial Network;Ultrasound Image Formation;Beamforming;Image Segmentation;Machine Learning","Image segmentation;Radio frequency;Ultrasonic imaging;Gallium nitride;Generators;Array signal processing;Generative adversarial networks","biological tissues;biomedical ultrasonics;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets;phantoms","generative adversarial neural network;plane wave ultrasound imaging;multiple plane wave insonifications;image quality;deep learning-based ultrasound image formation methods;beamforming process;raw radio-frequency channel data;single plane wave insonification;Deep Neural Network B-mode image;DNN segmentation;GAN training;tissue-mimicking phantom;cyst;ultrasound information extraction;FieldII simulations;Delay-and-Sum beamformed B-mode image;PSNR score","","13","","26","","18 Apr 2019","","","IEEE","IEEE Conferences"
"Nondestructive Detection of Targeted Microbubbles Using Dual-Mode Data and Deep Learning for Real-Time Ultrasound Molecular Imaging","D. Hyun; L. Abou-Elkacem; R. Bam; L. L. Brickson; C. D. Herickhoff; J. J. Dahl","Department of Radiology, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Electrical Engineering, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA","IEEE Transactions on Medical Imaging","30 Sep 2020","2020","39","10","3079","3088","Ultrasound molecular imaging (UMI) is enabled by targeted microbubbles (MBs), which are highly reflective ultrasound contrast agents that bind to specific biomarkers. Distinguishing between adherent MBs and background signals can be challenging in vivo. The preferred preclinical technique is differential targeted enhancement (DTE), wherein a strong acoustic pulse is used to destroy MBs to verify their locations. However, DTE intrinsically cannot be used for real-time imaging and may cause undesirable bioeffects. In this work, we propose a simple 4-layer convolutional neural network to nondestructively detect adherent MB signatures. We investigated several types of input data to the network: “anatomy-mode” (fundamental frequency), “contrast-mode” (pulse-inversion harmonic frequency), or both, i.e., “dual-mode”, using IQ channel signals, the channel sum, or the channel sum magnitude. Training and evaluation were performed on in vivo mouse tumor data and microvessel phantoms. The dual-mode channel signals yielded optimal performance, achieving a soft Dice coefficient of 0.45 and AUC of 0.91 in two test images. In a volumetric acquisition, the network best detected a breast cancer tumor, resulting in a generalized contrast-to-noise ratio (GCNR) of 0.93 and Kolmogorov-Smirnov statistic (KSS) of 0.86, outperforming both regular contrast mode imaging (GCNR = 0.76, KSS = 0.53) and DTE imaging (GCNR = 0.81, KSS = 0.62). Further development of the methodology is necessary to distinguish free from adherent MBs. These results demonstrate that neural networks can be trained to detect targeted MBs with DTE-like quality using nondestructive dual-mode data, and can be used to facilitate the safe and real-time translation of UMI to clinical applications.","1558-254X","","10.1109/TMI.2020.2986762","National Cancer Institute; Seed Grant from the Stanford Cancer Institute, a NCI-designated Comprehensive Cancer Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9062573","Image enhancement/restoration (noise and artifact reduction);machine learning;molecular and cellular imaging;neural network;ultrasound","Imaging;Ultrasonic imaging;Neural networks;Mice;Machine learning;Harmonic analysis;Tumors","biomedical ultrasonics;bubbles;cancer;convolutional neural nets;learning (artificial intelligence);medical image processing;phantoms;tumours","Kolmogorov-Smirnov statistic;volumetric acquisition;breast cancer tumor;real-time ultrasound molecular imaging;in vivo mouse tumor data;contrast mode imaging;neural networks;DTE imaging;dual-mode channel signals;IQ channel signals;pulse-inversion harmonic frequency;4-layer convolutional neural network;acoustic pulse;differential targeted enhancement;specific biomarkers;ultrasound contrast agents;UMI;deep learning;dual-mode data;targeted microbubbles","","3","","52","IEEE","9 Apr 2020","","","IEEE","IEEE Journals"
"Evolving complex yet interpretable representations: application to Alzheimer’s diagnosis and prognosis","J. -P. Kröll; S. B. Eickhoff; F. Hoffstaedter; K. R. Patil","Inst. of Neurosci. and Medicine, INM-7, Forschungszentrum Jülich, and Inst. of Systems Neurosci.,HHU Düsseldorf,Germany; Inst. of Neurosci. and Medicine, INM-7, Forschungszentrum Jülich, and Inst. of Systems Neurosci.,HHU Düsseldorf,Germany; Inst. of Neurosci. and Medicine, INM-7, Forschungszentrum Jülich, and Inst. of Systems Neurosci.,HHU Düsseldorf,Germany; Inst. of Neurosci. and Medicine, INM-7, Forschungszentrum Jülich, and Inst. of Systems Neurosci.,HHU Düsseldorf,Germany","2020 IEEE Congress on Evolutionary Computation (CEC)","3 Sep 2020","2020","","","1","8","With increasing accuracy and availability of more data, the potential of using machine learning (ML) methods in medical and clinical applications has gained considerable interest. However, the main hurdle in translational use of ML methods is the lack of explainability, especially when non-linear methods are used. Explainable (i.e. human-interpretable) methods can provide insights into disease mechanisms but can equally importantly promote clinician-patient trust, in turn helping wider social acceptance of ML methods. Here, we empirically test a method to engineer complex, yet interpretable, representations of base features via evolution of context-free grammar (CFG). We show that together with a simple ML algorithm evolved features provide higher accuracy on several benchmark datasets and then apply it to a real word problem of diagnosing Alzheimer's disease (AD) based on magnetic resonance imaging (MRI) data. We further demonstrate high performance on a hold-out dataset for the prognosis of AD.","","978-1-7281-6929-3","10.1109/CEC48606.2020.9185843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185843","grammar evolution;feature representation;interpretability;Alzheimer’s disease;machine learning","Biological cells;Feature extraction;Production;Dementia;Grammar;Sociology","biomedical MRI;context-free grammars;diseases;learning (artificial intelligence);medical image processing","machine learning;disease mechanisms;Alzheimer's disease diagnosis;magnetic resonance imaging data;Alzheimer's disease prognosis;context-free grammar;MRI","","","","39","","3 Sep 2020","","","IEEE","IEEE Conferences"
"A combined feature extraction method for left-right hand motor imagery in BCI","J. Hong; X. Qin; J. Bai; P. Zhang; Y. Cheng","School of mechanical Engineering, Northwestern Polytechnical University, 544 mailbox, No. 127, West Youyi Road, Beilin, 710072, Xi'an City, Shaanxi Province, China; School of mechanical Engineering, Northwestern Polytechnical University, 544 mailbox, No. 127, West Youyi Road, Beilin, 710072, Xi'an City, Shaanxi Province, China; School of mechanical Engineering, Northwestern Polytechnical University, 544 mailbox, No. 127, West Youyi Road, Beilin, 710072, Xi'an City, Shaanxi Province, China; School of mechanical Engineering, Northwestern Polytechnical University, 544 mailbox, No. 127, West Youyi Road, Beilin, 710072, Xi'an City, Shaanxi Province, China; School of mechanical Engineering, Northwestern Polytechnical University, 544 mailbox, No. 127, West Youyi Road, Beilin, 710072, Xi'an City, Shaanxi Province, China","2015 IEEE International Conference on Mechatronics and Automation (ICMA)","3 Sep 2015","2015","","","2621","2625","The aim of BCI is to translate brain activity into a command for a computer. For this purpose, the EEG signal processing plays an important role, especially in feature extraction. In this paper, a combined feature extraction method is proposed for left-right hand motor imagery in BCI. The power in the sensorimotor rhythm band and the statistical features of wavelet coefficients are used for extracting features and support vector machine is adopted for pattern recognition of left-right hand motor imagery. The performance is tested by the EEG signals of subject b and subject g from the datasets1 BCI Competition IV. The results have shown the availability of this method. It provides a novel way to EEG feature extraction in BCI.","2152-744X","978-1-4799-7098-8","10.1109/ICMA.2015.7237900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237900","BCI (Brain-computer interface);Motor imagery;ERD/ERS;Wavelet decomposition;SVM (Support vector machine )","Feature extraction;Electroencephalography;Support vector machines;Wavelet coefficients;Rhythm;Discrete wavelet transforms;Pattern recognition","brain-computer interfaces;electroencephalography;feature extraction;medical signal processing;statistical analysis;support vector machines;wavelet transforms","feature extraction method;left-right hand motor imagery;BCI;brain-computer interface;EEG signal processing;electroencephalography;sensorimotor rhythm band;statistical features;wavelet coefficients;support vector machine;pattern recognition","","3","","20","","3 Sep 2015","","","IEEE","IEEE Conferences"
"Optimization of sitting posture classification based on user identification","B. Ribeiro; H. Pereira; R. Almeida; A. Ferreira; L. Martins; C. Quaresma; P. Vieira","Departamento de Física, Faculdade de Ciências e Tecnologias, Universidade Nova de Lisboa, Quinta da Torre, 2829-516, Caparica, Portugal; Departamento de Física, Faculdade de Ciências e Tecnologias, Universidade Nova de Lisboa, Quinta da Torre, 2829-516, Caparica, Portugal; Departamento de Física, Faculdade de Ciências e Tecnologias, Universidade Nova de Lisboa, Quinta da Torre, 2829-516, Caparica, Portugal; Departamento de Física, Faculdade de Ciências e Tecnologias, Universidade Nova de Lisboa, Quinta da Torre, 2829-516, Caparica, Portugal; CA3, UNINOVA, Institute for the Development of New Technologies, Quinta da Torre, 2829-516, Caparica, Portugal; LIBPhys-UNL, Departamento de Física, Faculdade de Ciências e Tecnologias, Universidade Nova de Lisboa, 2829-516 Caparica, Portugal; LIBPhys-UNL, Departamento de Física, Faculdade de Ciências e Tecnologias, Universidade Nova de Lisboa, 2829-516 Caparica, Portugal","2015 IEEE 4th Portuguese Meeting on Bioengineering (ENBENG)","20 Apr 2015","2015","","","1","6","In a precursory work, an intelligent sensing chair prototype was developed to classify 12 standardized sitting postures using 8 pneumatic bladders (4 in the chair's seat and 4 in the backrest) connected to piezoelectric sensors to measure inner pressure. A Classification of around 80% was obtained using Neural Networks. This work aims to demonstrate how algorithmic optimization can be applied to a newly developed prototype to improve posture classification performance. The aforementioned optimization is based on the split of users by sex and use two different previously trained Neural Networks (one for Male and the other for Female). Results showed that the best neural network parameters had an overall classification 89.0% (from the 92.1% for Female Classification and 85.8% for Male, which translates into an overall optimization of around 8%). Automatic separation of these sets was achieved with Decision Trees with an overall classification optimization of 87.1%.","","978-1-4799-8269-1","10.1109/ENBENG.2015.7088853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7088853","Sensory Intelligent Chair;Sitting Posture Classification;Machine Learning","Classification algorithms;Optimization;Sensors;Prototypes;Bladder;Biological neural networks;Neurons","decision trees;gait analysis;intelligent sensors;medical signal processing;neural nets;optimisation;piezoelectric devices;pressure measurement","sitting posture classification optimization;user identification;intelligent sensing chair prototype;standardized sitting postures;pneumatic bladders;chair seat;backrest;piezoelectric sensors;inner pressure;algorithmic optimization;posture classification performance;neural network parameters;decision trees;overall classification optimization","","3","","34","","20 Apr 2015","","","IEEE","IEEE Conferences"
"Toward Universal Neural Interfaces for Daily Use: Decoding the Neural Drive to Muscles Generalises Highly Accurate Finger Task Identification Across Humans","M. Stachaczyk; S. F. Atashzar; S. Dupan; I. Vujaklija; D. Farina","Department of Bioengineering, Imperial College London, London, U.K.; Tandon School of Engineering, New York University, New York, NY, USA; School of Engineering, Newcastle University, Newcastle upon Tyne, U.K.; Department of Electrical Engineering and Automation, Aalto University, Espoo, Finland; Department of Bioengineering, Imperial College London, London, U.K.","IEEE Access","19 Aug 2020","2020","8","","149025","149035","Peripheral neural signals can be used to estimate movement-specific muscle activation patterns for the purpose of human-machine interfacing (HMI). The available HMI solutions, however, provide limited movement decoding accuracy that often results in inadequate device control, especially in the dynamic tasks context, and require extensive algorithm training that is highly subject-specific. Here, we show that dexterous movements can be identified with high accuracy using a physiology-derived and information-theoretically optimised feature space that targets the spatio-temporal properties of the spiking activity of spinal motor neurons (neural features), decomposed from the interference myoelectric signal. Moreover, we show that the movement decoding accuracy based on these neural features is not influenced by the muscle activation level, reaching overall >98% in the full range of forces investigated and from processing intervals as short as 30-ms. Finally, we show that the high accuracy in individual finger movement recognition can be achieved without user-specific models. These results are the first to show a highly accurate discrimination of dexterous movement tasks in a wide range of muscle activation levels from near-real time processing intervals, with minimal subject-specific training, and thus are promising for the translation of HMI to daily use.","2169-3536","","10.1109/ACCESS.2020.3015761","European Research Council Synergy project Natural BionicS; U.K. Engineering and Physical Sciences Research Council (EPSRC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164963","Dexterous movement classification;human-machine interfaces;information theory;neural drive;universality of neural control","Muscles;Fingers;Task analysis;Neurons;Electromyography;Decoding;Force","biomechanics;electromyography;human computer interaction;medical signal processing;neurophysiology","human-machine interfacing;movement decoding accuracy;dexterous movements;spatio-temporal properties;spiking activity;spinal motor neurons;neural features;interference myoelectric signal;muscle activation level;finger movement recognition;user-specific models;minimal subject-specific training;neural drive;peripheral neural signals;movement-specific muscle activation patterns;universal neural interfaces;finger task identification;dynamic task context;information-theoretically optimised feature space","","1","","57","CCBY","11 Aug 2020","","","IEEE","IEEE Journals"
"Early Experiences in Using Blood Cells Biomembranes as Markers for Diabetes Diagnosis","E. Cordelli; G. Pani; D. Pitocco; G. Maulucci; P. Soda","Dept. of Eng., Univ. Campus Bio-Medico di Roma, Rome, Italy; Ist. di Patologia Gen., Univ. Cattolica del Sacro Cuore, Cattolica, Italy; Ist. di Med. Interna e Geriatria, Univ. Cattolica del Sacro Cuore, Cattolica, Italy; Ist. di Fis., Univ. Cattolica del Sacro Cuore, Cattolica, Italy; Dept. of Eng., Univ. Campus Bio-Medico di Roma, Rome, Italy","2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)","18 Aug 2016","2016","","","197","202","Investigation of membrane fluidity by two photon fluorescence microscopy opens up a new and important area of translational research, being a useful and sensitive method for disease monitoring and treatment. In this paper we investigate if biomembranes in human red blood cells (RBC) and peripheral mononuclear cells (PMC) could be used as markers for type 1 diabetes mellitus (T1DM) diagnosis, leading to the development of a method for monitoring T1DM progression that nowadays is lacking, as clinical exams cannot pursue this task with enough reliability. To this aim, we present a set of features computed from PMC and RBC images that are given to a multi-experts system leveraging on multi-spectral information for positive/negative classifications. The experiments are carried out on a dataset of 800 blood cell images belonging to 18 subjects adopting the leave-one-person-out approach.","2372-9198","978-1-4673-9036-1","10.1109/CBMS.2016.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545984","Type 1 Diabetes;Two photon microscopy;Image processing;Feature extraction;Machine learning","Biomembranes;Cells (biology);Monitoring;Blood;Diseases;Training;Principal component analysis","biomembranes;diseases;medical image processing;patient diagnosis","blood cells biomembrane;diabetes diagnosis;membrane fluidity;photon fluorescence microscopy;disease monitoring;disease treatment;human red blood cells;RBC image;peripheral mononuclear cells;PMC image;type 1 diabetes mellitus;T1DM progression;clinical exam;multiexperts system;multispectral information;leave-one-person-out approach;positive-negative classification","","1","","33","","18 Aug 2016","","","IEEE","IEEE Conferences"
"Optimized Backpropagation Neural Network Model For Brain Computer Interface System","M. Krishnaveni; S. N. Geethalakshmi; P. Subashini; T. T. Dhivyaprabha; S. Lakshmi","Avinashilingam Institute for Home Science and Higher Education for Women,Department of Computer Science,Coimbatore,India; Avinashilingam Institute for Home Science and Higher Education for Women,Department of Computer Science,Coimbatore,India; Avinashilingam Institute for Home Science and Higher Education for Women,Department of Computer Science,Coimbatore,India; Avinashilingam Institute for Home Science and Higher Education for Women,Department of Computer Science,Coimbatore,India; Avinashilingam Institute for Home Science and Higher Education for Women,Department of Computer Science,Coimbatore,India","2019 IEEE International Smart Cities Conference (ISC2)","20 Apr 2020","2019","","","420","425","People with several disabilities face many challenges in personal communication with the external real-world environment. Brain Computer Interface (BCI) system offers promising solution for both communication and rehabilitation therapies which overcomes the difficulties faced by disabled people, especially fully non-speaking people. Noninvasive Electroencephalogram (EEG) based BCI system acts as a communication medium that translates brain-activity into commands for computer systems or other devices. EEG signals recorded from the scalp of aforementioned subjects are utilized to extract the meaningful patterns in the EEG-based BCI system. In this paper, an optimized Backpropagation Neural Network (BPN) based on Synergistic Fibroblast optimization (SFO) algorithm is proposed for the classification of EEG patterns in the design of robust BCI system. EEG signals collected from open repository database are employed to evaluate the efficiency of SFO based BPN classifier which is compared with Support Vector Machine (SVM) and conventional BPN method. Investigation of the results show that SFO based BPN method achieves highest classification accuracy compared to other conventional classifiers.","2687-8860","978-1-7281-0846-9","10.1109/ISC246665.2019.9071784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9071784","Backpropagation Neural Network (BPN);Brain Computer Interface (BCI);Electroencephalogram (EEG);Support Vector Machine (SVM);Synergistic Fibroblast Optimization (SFO)","Electroencephalography;Feature extraction;Support vector machines;Classification algorithms;Brain modeling;Training;Scalp","backpropagation;brain-computer interfaces;electroencephalography;handicapped aids;medical signal processing;neural nets;optimisation;support vector machines","Synergistic Fibroblast optimization algorithm;computer systems;brain-activity;communication medium;Noninvasive Electroencephalogram;disabled people;rehabilitation therapies;Brain Computer Interface system;personal communication;brain Computer Interface system;optimized Backpropagation Neural Network model;SFO based BPN classifier;EEG signals;robust BCI system","","","","24","","20 Apr 2020","","","IEEE","IEEE Conferences"
"SVM-Based System for Prediction of Epileptic Seizures From iEEG Signal","H. -T. Shiao; V. Cherkassky; J. Lee; B. Veber; E. E. Patterson; B. H. Brinkmann; G. A. Worrell","Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN, USA; Department of Electrical and Computer EngineeringUniversity of Minnesota; Department of Electrical and Computer EngineeringUniversity of Minnesota; Department of Electrical and Computer EngineeringUniversity of Minnesota; Department of Veterinary Clinical Sciences University of Minnesota; Mayo Systems Electrophysiology Laboratory, Mayo Clinic; Mayo Systems Electrophysiology Laboratory, Mayo Clinic","IEEE Transactions on Biomedical Engineering","18 Apr 2017","2017","64","5","1011","1022","Objective: This paper describes a data-analytic modeling approach for the prediction of epileptic seizures from intracranial electroencephalogram (iEEG) recording of brain activity. Even though it is widely accepted that statistical characteristics of iEEG signal change prior to seizures, robust seizure prediction remains a challenging problem due to subject-specific nature of data-analytic modeling. Methods: Our work emphasizes the understanding of clinical considerations important for iEEG-based seizure prediction, and proper translation of these clinical considerations into data-analytic modeling assumptions. Several design choices during preprocessing and postprocessing are considered and investigated for their effect on seizure prediction accuracy. Results: Our empirical results show that the proposed support vector machine-based seizure prediction system can achieve robust prediction of preictal and interictal iEEG segments from dogs with epilepsy. The sensitivity is about 90-100%, and the false-positive rate is about 0-0.3 times per day. The results also suggest that good prediction is subject specific (dog or human), in agreement with earlier studies. Conclusion : Good prediction performance is possible only if the training data contain sufficiently many seizure episodes, i.e., at least 5-7 seizures. Significance: The proposed system uses subject-specific modeling and unbalanced training data. This system also utilizes three different time scales during training and testing stages.","1558-2531","","10.1109/TBME.2016.2586475","National Institutes of Health; NeuroVista Inc.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501827","Data-analytic modeling;epilepsy;feature representation;intracranial electroencephalogram (iEEG);postprocessing;seizure prediction;subject-specific modeling;support vector machine (SVM);unbalanced classification","Predictive models;Training data;Data models;Brain models;Support vector machines;Testing","electroencephalography;medical disorders;medical signal processing;support vector machines","SVM based system;epileptic seizures prediction;iEEG signal;data analytic modeling approach;intracranial electroencephalogram recording;brain activity;seizure prediction accuracy;support vector machine;dogs;epilepsy","Algorithms;Animals;Diagnosis, Computer-Assisted;Dogs;Electrocorticography;Epilepsy;Humans;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Support Vector Machine","37","","25","","29 Jun 2016","","","IEEE","IEEE Journals"
"Classification of mental tasks using S-transform based fractal features","S. Sethi; R. Upadhyay","Electronics and Communication Engineering, Thapar University, Patiala, India; Electronics and Communication Engineering, Thapar University, Patiala, India","2017 International Conference on Computer, Communications and Electronics (Comptelix)","10 Aug 2017","2017","","","38","43","Brain Computer Interface is a reliable communication interface between human brain and external world. It translates human brain electrical activity to useful command by extracting meaningful features from Electroencephalogram signals. In present work, feature extraction techniques and classification methods are proposed for implementation of Brain Computer Interface system. Proposed methodology is carried out in four methodological steps. At first step, segmentation and windowing of Electroencephalogram signals are performed. The S-transform of segmented Electroencephalogram signals is evaluated in second step. At third step, mean and maximum values of Katz's Fractal Dimension are calculated from S-transform coefficients as features. Classification of extracted features is carried out in the fourth step using three machine learning techniques viz. Random Forest, Artificial Neural Network and Support Vector Machine. Classification results reflect the efficiency of S-transform based feature extraction technique in Brain Computer Interface implementation.","","978-1-5090-4708-6","10.1109/COMPTELIX.2017.8003934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003934","Electroencephalogram;windowing;feature extraction;S-Transform;machine learning techniques","Feature extraction;Electroencephalography;Support vector machines;Fractals;Time-frequency analysis;Radio frequency;Artificial neural networks","brain-computer interfaces;electroencephalography;feature extraction;fractals;learning (artificial intelligence);medical signal processing;neural nets;signal classification;support vector machines;wavelet transforms","mental task classification;S-transform based fractal features;communication interface;human brain electrical activity;electroencephalogram signals;feature extraction techniques;brain computer interface system;electroencephalogram signal windowing;electroencephalogram signal segmentation;mean values;maximum values;fractal dimension;machine learning techniques;random forest;artificial neural network;support vector machine","","2","","30","","10 Aug 2017","","","IEEE","IEEE Conferences"
"aCortex: An Energy-Efficient Multipurpose Mixed-Signal Inference Accelerator","M. Bavandpour; M. R. Mahmoodi; D. B. Strukov","Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, USA","IEEE Journal on Exploratory Solid-State Computational Devices and Circuits","3 Jul 2020","2020","6","1","98","106","We introduce “aCortex,” an extremely energy-efficient, fast, compact, and versatile neuromorphic processor architecture suitable for the acceleration of a wide range of neural network inference models. The most important feature of our processor is a configurable mixed-signal computing array of vector-by-matrix multiplier (VMM) blocks utilizing embedded nonvolatile memory arrays for storing weight matrices. Analog peripheral circuitry for data conversion and high-voltage programming are shared among a large array of VMM blocks to facilitate compact and energy-efficient analog-domain VMM operation of different types of neural network layers. Other unique features of aCortex include configurable chain of buffers and data buses, simple and efficient instruction set architecture and its corresponding multiagent controller, programmable quantization range, and a customized refresh-free embedded dynamic random access memory. The energy-optimal aCortex with 4-bit analog computing precision was designed in a 55-nm process with embedded NOR flash memory. Its physical performance was evaluated using experimental data from testing individual circuit elements and physical layout of key components for several common benchmarks, namely, Inception-v1 and ResNet-152, two state-of-the-art deep feedforward networks for image classification, and GNTM, Google's deep recurrent network for language translation. The system-level simulation results for these benchmarks show the energy efficiency of 97, 106, and 336 TOp/J, respectively, combined with up to 15 TOp/s computing throughput and 0.27-MB/mm2 storage efficiency. Such estimated performance results compare favorably with those of previously reported mixed-signal accelerators based on much less mature aggressively scaled resistive switching memories.","2329-9231","","10.1109/JXCDC.2020.2999581","Directorate for Computer and Information Science and Engineering; Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9107115","Artificial neural networks;floating-gate memory;machine learning;mixed-signal circuits;neuromorphic inference accelerator;nonvolatile memory (NVM)","Virtual machine monitors;Neurons;Arrays;Nonvolatile memory;Memory management;Data transfer","AI chips;feedforward neural nets;image classification;integrated circuit layout;low-power electronics;matrix multiplication;mixed analogue-digital integrated circuits;natural language processing;neural chips;neuromorphic engineering;random-access storage;recurrent neural nets","high-voltage programming;embedded nonvolatile memory arrays;mixed-signal computing array;versatile neuromorphic processor architecture;multipurpose mixed-signal inference accelerator;Google deep recurrent network;neural network layers;energy-efficient analog-domain;analog peripheral circuitry;vector-by-matrix multiplier blocks;configurable mixed-signal computing array;neural network inference models;resistive switching memories;mixed-signal accelerators;state-of-the-art deep feedforward networks;embedded NOR flash memory;energy-optimal aCortex;refresh-free embedded dynamic random access memory;programmable quantization range;multiagent controller;efficient instruction set architecture;simple instruction set architecture;data buses","","","","40","CCBY","3 Jun 2020","","","IEEE","IEEE Journals"
"A novel brain computer interface based on Principle Component Analysis and Fuzzy Logic","S. S. Labib","Faculty of Computer Science, October University for Modern Science and Arts, Giza, Egypt","2016 Sixth International Conference on Digital Information Processing and Communications (ICDIPC)","19 May 2016","2016","","","31","36","Brain computer interface (BCI) systems measure brain signal and translate it into control commands in an attempt to mimic specific human thinking activities. In recent years, many researchers have shown their interests in BCI systems, which has resulted in many experiments and applications. The main issue to build applicable Brain-Computer Interfaces is the capability to classify the Electroencephalograms (EEG). The purpose behind this research is to improve a model for brain signals analysis. We have used high pass filter to remove artifacts, discrete wavelet transform algorithms for feature extraction and statistical features like Mean Absolute Value, Root Mean Square, and Simple Square Integral are used, also we have used principle component analysis to reduce the size of feature vector and we used fuzzy Gaussian membership function to optimize the classification phase. It has been depicted from results that the proposed integrated techniques outperform a better performance than methods mentioned in literature.","","978-1-4673-7504-7","10.1109/ICDIPC.2016.7470787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470787","Brain Computer Interface;Principle Component Analysis;Support Vector Machine;Wavelet Transform;EEG","Electroencephalography;Brain;Wavelet transforms;Wavelet analysis;Feature extraction;Principal component analysis","brain-computer interfaces;electroencephalography;feature extraction;high-pass filters;medical signal processing;principal component analysis;signal classification","novel brain computer interface;fuzzy logic;principle component analysis;brain signal;BCI systems;EEG;electroencephalograms signal;high pass filter;discrete wavelet transform algorithms;feature extraction;statistical features;fuzzy Gaussian membership function;classification phase","","","","13","","19 May 2016","","","IEEE","IEEE Conferences"
"Logistic regression classifier for palmprint verification","D. Kostadinov; S. Bogdanova","Department of Electronics, Faculty of Electrical Engineering and Information Technologies, Ss. Cyril and Methodius University, Skopje, Macedonia; Department of Electronics, Faculty of Electrical Engineering and Information Technologies, Ss. Cyril and Methodius University, Skopje, Macedonia","2012 19th International Conference on Systems, Signals and Image Processing (IWSSIP)","31 May 2012","2012","","","413","416","We propose a supervised machine learning approach for automatic palmprint verification. In our approach a pair of palmprint images is represented and characterized using a vector of regional similarity features. Every regional similarity feature is computed using local modified complex wavelet structural similarity indexes (CW-SSIM). The logistic regression classifier verifies whether two palmprints described by the feature vector belong to same person or not. The aim of our classifier is to improve the matching accuracy and robustness of the verification, based on learned knowledge about: 1) the local and global characterization of the errors arising due to inaccurate image registration (translations, rotations, and distortions), and 2) the underlying vector patterns of the two palmprint images. Our experimental results show that the proposed approach achieves high verification accuracy.","2157-8702","978-3-200-02328-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6208164","Biometrics;palmprint;complex wavelet transform;machine learning","Indexes;Logistics;Accuracy;Feature extraction;Vectors;Support vector machine classification;Lighting","feature extraction;image classification;image matching;image representation;learning (artificial intelligence);palmprint recognition;regression analysis;wavelet transforms","logistic regression classifier;supervised machine learning approach;automatic palmprint verification;palmprint image representation;regional similarity features;local modified complex wavelet structural similarity indexes;CW-SSIM;feature vector;matching accuracy;verification robustness;global characterization;local characterization;vector patterns","","","","12","","31 May 2012","","","IEEE","IEEE Conferences"
"A robot control platform for motor impaired people","M. Will; T. Peter; M. Hanses; N. Elkmann; G. Rose; H. Hinrichs; C. Reichert","Research Campus STIMULATE,Institute for Medical Engineering, Otto-von-Guericke University,Magdeburg,Germany; Fraunhofer Institute for Factory Operation and Automation IFF,Robotic Systems,Magdeburg,Germany; Fraunhofer Institute for Factory Operation and Automation IFF,Robotic Systems,Magdeburg,Germany; Fraunhofer Institute for Factory Operation and Automation IFF,Robotic Systems,Magdeburg,Germany; Research Campus STIMULATE,Institute for Medical Engineering, Otto-von-Guericke University,Magdeburg,Germany; University Medical Center,Leibniz Institute for Neurobiology,Department of Neurology,Magdeburg,Germany; Research Campus STIMULATE,Leibniz Institute for Neurobiology,Magdeburg,Germany","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","2025","2030","Brain-machine interfaces (BMI) open new opportunities to control robotic devices as they provide the feasibility to translate brain signals into commands. Severely motor impaired people who have lost muscle control could benefit from this technique to control assistive devices, which support them in daily life. However, non-invasive BMIs can distinguish only a few different commands with relatively high error rates, which makes the asynchronous control of a robot with multiple degrees of freedom challenging. Here, we introduce a novel robotic grasping system, which combines scene recognition techniques and autonomous path planning with user interaction instantiated by a hybrid control system based on the electroencephalogram and the electrooculogram. The results show that healthy subjects can reliably perform a grasp-and-place task, arranging four objects at defined positions within 133-331s (193.6 ±61.5s), while they require only a few corrections. Our robot control platform proved to work solely with electrophysiological control signals and thus, constitutes a basis to perform various robot actions initiated by motor-impaired people.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283104","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283104","brain-machine interface;grasping;robot;path planning;BCI","Robot control;Grasping;Muscles;Brain-computer interfaces;Path planning;Reliability;Task analysis","brain-computer interfaces;electroencephalography;electro-oculography;handicapped aids;manipulators;medical robotics;medical signal processing;muscle;neurophysiology;path planning;user interfaces","motor-impaired people;robot control platform;brain-machine interfaces;robotic devices;brain signals;muscle control;assistive devices;noninvasive BMI;asynchronous control;robotic grasping system;scene recognition techniques;hybrid control system;electrophysiological control signals;robot actions;autonomous path planning;user interaction;electroencephalogram;electrooculogram;grasp-and-place task;time 133.0 s to 331.0 s","","","","28","","14 Dec 2020","","","IEEE","IEEE Conferences"
"CW-SSIM based image classification","Y. Gao; A. Rehman; Z. Wang","Dept. of Electrical and Computer Engineering, University of Waterloo, Waterloo, Ontario, Canada; Dept. of Electrical and Computer Engineering, University of Waterloo, Waterloo, Ontario, Canada; Dept. of Electrical and Computer Engineering, University of Waterloo, Waterloo, Ontario, Canada","2011 18th IEEE International Conference on Image Processing","29 Dec 2011","2011","","","1249","1252","Complex wavelet structural similarity (CW-SSIM) index has been proposed as a promising image similarity measure that is robust to small geometric distortions such as translation, scaling and rotation of images, but how to make the best use of it in image classification problems has not been deeply investigated. In this paper, we propose a novel “feature-extraction free” image classification algorithm based on CW-SSIM and use handwritten digit recognition as an example to demonstrate it. First, a CW-SSIM based unsupervised clustering method is used to divide the training images into clusters and to pick a representative image for each cluster. A supervised learning method based on support vector machines is then employed to maximize the classification accuracy based on CW-SSIM values between an input image and the representative images. Our experiments show that such a conceptually simple image classification method, which does not involve any registration, intensity normalization or sophisticated feature extraction processes, and does not rely on any modeling of the image patterns or distortion processes, achieves competitive performance with reduced computational complexity.","2381-8549","978-1-4577-1303-3","10.1109/ICIP.2011.6115659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115659","complex-wavelet structural similarity;image classification;handwritten digit recognition;clustering;support vector machine","Training;Support vector machines;Indexes;Image recognition;Error analysis;Distortion measurement","feature extraction;geometry;image classification;pattern clustering;support vector machines;unsupervised learning","CW-SSIM based image classification;complex wavelet structural similarity index;geometric distortions;feature-extraction free;handwritten digit recognition;CW-SSIM based unsupervised clustering method;supervised learning method;support vector machines;computational complexity","","9","","8","","29 Dec 2011","","","IEEE","IEEE Conferences"
"2001 IEEE Workshop on Automatic Speech Recognition and Understanding. ASRU 2001. Conference Proceedings (Cat. No.01EX544)","",,"IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU '01.","7 Nov 2002","2001","","","i","","Presents the front cover of the proceedings record.","","0-7803-7343-X","10.1109/ASRU.2001.1034574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1034574","","","speech recognition;feature extraction;information retrieval;language translation","automatic speech recognition;speech understanding;ASR robustness;feature extraction;acoustic modeling;wireless ASR;distributed speech recognition;hands free interaction;large vocabulary recognition;language modeling;dialogue systems;voice agents;audio-video information retrieval;digital archives;multilingual translation;speech-to-speech translation","","","","","","7 Nov 2002","","","IEEE","IEEE Conferences"
"2005 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat.05EX1206C)","",,"IEEE Workshop on Automatic Speech Recognition and Understanding, 2005.","3 Jan 2006","2005","","","0_1","0_1","The following topics were dealt with: automatic speech recognition; speech understanding; speech analysis; speech translation; auditory processing","","0-7803-9478-X","10.1109/ASRU.2005.1566448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1566448","","","audio signal processing;speech recognition;speech synthesis","automatic speech recognition;speech understanding;speech analysis;speech translation;auditory processing","","","","","","3 Jan 2006","","","IEEE","IEEE Conferences"
"Plenary talk I brain-machine interfaces: Theory and applications","S. Eldawlatly","Ain Shams University, Egypt","2015 Tenth International Conference on Computer Engineering & Systems (ICCES)","28 Jan 2016","2015","","","xvii","xviii","A Brain-Machine Interface (BMI) is an electronic device that interfaces directly with the brain to restore a lost function caused by an injury or disorder. BMIs currently provide new hope for people with disabilities, whose brain function is still intact. By recording brain activity, BMIs could translate recorded brain activity to actions that can interact with the external environment. There are two major types of BMIs: Non-invasive and invasive. Non-invasive BMIs use multiple electrodes to record electroencephalography (EEG) signals from the scalp. Such recording technique enables extracting several time-domain and frequency-domain features that correlate with the subjects' intended behavior. Invasive BMIs, which involve a brain surgery to implant electrodes into the brain, enable recording the activity of individual neurons in the brain in addition to the ability to electrically stimulating such neurons. This talk will give an overview of both the theoretical and practical aspects of non-invasive and invasive BMI research areas. For non-invasive BMIs, several time-domain and frequency-domain features have been the core of multiple applications. Time-domain features, such as the P300 signal which is an apparent increase in the recorded EEG post-presentation of a rare stimulus, and frequency-domain features, such as the steady-state visually evoked potentials (SSVEP), will be reviewed. On the other hand, invasive BMIs have some clinically successful applications, such as cochlear implants that restore hearing and deep brain stimulation that alleviates the symptoms of Parkinson's disease. Motor control BMIs also allow restoring motor functions impaired by a spinal cord injury. In this talk, such applications will be introduced. In addition, we will focus on using invasive BMIs to restore vision for the blind, which could be achieved by electrically stimulating the visual pathway to provide inputs that mimic what could come from an intact input to the visual pathway.","","978-1-4673-9971-5","10.1109/ICCES.2015.7393005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7393005","","","brain-computer interfaces;diseases;electroencephalography;handicapped aids;medical signal processing","brain-machine interface;people with disabilities;brain activity;noninvasive BMI;multiple electrodes;electroencephalography;EEG signal;time-domain feature;frequency-domain feature;brain surgery;Parkinson disease;motor control BMI","","","","","","28 Jan 2016","","","IEEE","IEEE Conferences"
