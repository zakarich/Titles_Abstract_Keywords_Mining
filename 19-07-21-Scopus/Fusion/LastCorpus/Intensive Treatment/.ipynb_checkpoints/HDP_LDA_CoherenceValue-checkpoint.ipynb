{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A structure-based model for Chinese organization name translation Alignment; Chunk; Hierarchical derivation; Machine translation; Named entity; Organization name; Rules extraction; Structural analysis; Synchronous context-free grammar'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import pprint\n",
    "\n",
    "#from gensim.models import Phrases\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "# NLTK Stop words\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','a','about', 'above', 'across'])\n",
    "\n",
    "NewCorpus = pd.read_excel('19-07-21-Scopus\\Fusion\\LastCorpus\\AllCorpus.xlsx',header=0)\n",
    "\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Title'] +' '+ NewCorpus['Author Keywords'] #+' '+ NewCorpus['Abstract'] #+' '+ NewCorpus['Year'].astype(str)\n",
    "NewCorpus['Titles & Keywords'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_19868/4159941197.py:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].str.replace('\\d+', '')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' a structure based model chinese organization alignment chunk hierarchical derivation named entity organization rule extraction structural analysis synchronous context free grammar'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS)\n",
    "import string\n",
    "\n",
    "#Words not must be included in the analysis\n",
    "removed = ['machine','translation','automatic','mt']\n",
    "#Define the function to remove the punctuation\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = str(text).replace(punctuation, ' ')\n",
    "    return text\n",
    "\n",
    "#Lemmatizaion of the keywords\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(str(text))]\n",
    "\n",
    "#merge words of a list into string \n",
    "def ListToString(lista):\n",
    "    text = ''\n",
    "    for word in lista:\n",
    "        text += ' '+word\n",
    "    return text\n",
    "\n",
    "#Replace Abbreviation\n",
    "def replace_Abbreviation(text):\n",
    "    text = str(text).replace('smt', 'statistical')\n",
    "    text = str(text).replace('nmt', 'neural')\n",
    "    text = str(text).replace('rbmt','rule based')\n",
    "    text = str(text).replace('hmt','Hybrid based')\n",
    "    text = str(text).replace('dmt','direct based')\n",
    "    text = str(text).replace('tbmt','transfer based')\n",
    "    text = str(text).replace('cbmt','corpus based')\n",
    "    text = str(text).replace('ebmt','example based')\n",
    "    return text\n",
    "\n",
    "#Apply Remove Punctuations\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].apply(remove_punctuations)\n",
    "#Remove Stop words:\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop_words))\n",
    "#Lowercase Author Keywords\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].str.lower()\n",
    "#Remove All words that could mess our classification\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in removed))\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].apply(replace_Abbreviation)\n",
    "#Apply the function lemmatization\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].apply(lemmatize_text)\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].apply(ListToString)\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].str.replace('\\d+', '')\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].str.replace('  ', ' ')\n",
    "NewCorpus['Titles & Keywords'] = NewCorpus['Titles & Keywords'].str.split('Â©').str[0]\n",
    "NewCorpus['Titles & Keywords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a structure based model chinese organization alignment chunk hierarchical derivation named entity organization rule extraction structural analysis synchronous context free grammar']\n",
      "[' a structure based model chinese organization alignment chunk hierarchical derivation named entity organization rule extraction structural analysis synchronous context free grammar', ' ancient modern chinese new large training dataset ancient modern chinese parallel corpus bilingual text alignment neural']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "df = NewCorpus['Titles & Keywords'].values.tolist()\n",
    "\n",
    "df = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in df]\n",
    "\n",
    "# Remove new line characters\n",
    "df = [re.sub('\\s+', ' ', sent) for sent in df]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "df = [re.sub(\"\\'\", \"\", sent) for sent in df]\n",
    "\n",
    "print(df[:1])\n",
    "\n",
    "df = [re.sub(\"-\", \" \", sent) for sent in df]\n",
    "df = [re.sub(\":\", \"\", sent) for sent in df]\n",
    "\n",
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2064"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "df_words = list(sent_to_words(df))\n",
    "len(df_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(df_words, min_count=5, threshold=10)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "\n",
    "data_words_nostops = remove_stopwords(df_words)\n",
    "\n",
    "# Form Bigrams\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "#_core_web_sm\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "#texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "#corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=6, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.068*\"disambiguation\" + 0.034*\"cross_lingual\" + 0.033*\"morphological\" + '\n",
      "  '0.033*\"word_sense\" + 0.030*\"language\" + 0.022*\"base\" + 0.021*\"learning\" + '\n",
      "  '0.020*\"processing\" + 0.019*\"information\" + 0.018*\"natural_language\"'),\n",
      " (1,\n",
      "  '0.239*\"neural\" + 0.042*\"learn\" + 0.022*\"deep_learne\" + 0.021*\"low_resource\" '\n",
      "  '+ 0.020*\"language\" + 0.019*\"datum\" + 0.016*\"neural_network\" + 0.016*\"user\" '\n",
      "  '+ 0.014*\"control\" + 0.013*\"use\"'),\n",
      " (2,\n",
      "  '0.052*\"human\" + 0.043*\"post_edite\" + 0.040*\"online\" + 0.040*\"document\" + '\n",
      "  '0.032*\"interactive\" + 0.027*\"tool\" + 0.022*\"adaptation\" + '\n",
      "  '0.018*\"performance\" + 0.017*\"quality\" + 0.016*\"pattern\"'),\n",
      " (3,\n",
      "  '0.096*\"neural\" + 0.055*\"attention\" + 0.048*\"alignment\" + 0.047*\"decode\" + '\n",
      "  '0.042*\"transformer\" + 0.028*\"base\" + 0.020*\"factor\" + 0.018*\"search\" + '\n",
      "  '0.017*\"processing\" + 0.016*\"unit\"'),\n",
      " (4,\n",
      "  '0.094*\"word\" + 0.076*\"text\" + 0.075*\"language\" + 0.046*\"natural_language\" + '\n",
      "  '0.032*\"processing\" + 0.032*\"process\" + 0.021*\"wordnet\" + 0.019*\"datum\" + '\n",
      "  '0.019*\"parallel\" + 0.019*\"word_segmentation\"'),\n",
      " (5,\n",
      "  '0.055*\"statistical\" + 0.038*\"rule\" + 0.036*\"domain_adaptation\" + '\n",
      "  '0.033*\"rule_base\" + 0.031*\"source\" + 0.030*\"extraction\" + 0.029*\"domain\" + '\n",
      "  '0.029*\"term\" + 0.028*\"similarity\" + 0.027*\"context\"'),\n",
      " (6,\n",
      "  '0.097*\"knowledge\" + 0.070*\"application\" + 0.066*\"translator\" + '\n",
      "  '0.052*\"representation\" + 0.034*\"mobile\" + 0.027*\"lexicon\" + 0.024*\"mapping\" '\n",
      "  '+ 0.023*\"research\" + 0.022*\"support\" + 0.019*\"map\"'),\n",
      " (7,\n",
      "  '0.064*\"web\" + 0.059*\"information_retrieval\" + 0.045*\"cross_language\" + '\n",
      "  '0.038*\"parallel_corpus\" + 0.032*\"patent\" + 0.028*\"learn\" + 0.026*\"ontology\" '\n",
      "  '+ 0.023*\"query\" + 0.022*\"multilingual\" + 0.021*\"dictionary\"'),\n",
      " (8,\n",
      "  '0.108*\"language\" + 0.068*\"low_resource\" + 0.044*\"pair\" + 0.041*\"tree\" + '\n",
      "  '0.031*\"morphology\" + 0.030*\"statistical\" + 0.028*\"processing\" + '\n",
      "  '0.028*\"neural\" + 0.020*\"english\" + 0.018*\"sentence_level\"'),\n",
      " (9,\n",
      "  '0.088*\"example_base\" + 0.034*\"parse\" + 0.031*\"sentence\" + 0.025*\"english\" + '\n",
      "  '0.025*\"syntactic\" + 0.024*\"grammar\" + 0.023*\"natural_language\" + '\n",
      "  '0.023*\"nlp\" + 0.022*\"lexical\" + 0.021*\"processing\"'),\n",
      " (10,\n",
      "  '0.103*\"arabic\" + 0.061*\"network\" + 0.041*\"language\" + '\n",
      "  '0.035*\"recurrent_neural\" + 0.025*\"transfer\" + 0.025*\"approach\" + '\n",
      "  '0.023*\"finite_state\" + 0.019*\"theory\" + 0.018*\"rule_base\" + '\n",
      "  '0.017*\"medical\"'),\n",
      " (11,\n",
      "  '0.075*\"language\" + 0.043*\"communication\" + 0.033*\"resource\" + '\n",
      "  '0.033*\"service\" + 0.028*\"network\" + 0.027*\"review\" + 0.024*\"technique\" + '\n",
      "  '0.023*\"prediction\" + 0.022*\"pivot\" + 0.019*\"use\"'),\n",
      " (12,\n",
      "  '0.146*\"semantic\" + 0.057*\"multilingual\" + 0.032*\"analysis\" + 0.027*\"base\" + '\n",
      "  '0.024*\"structure\" + 0.022*\"encoder\" + 0.021*\"dependency_parse\" + '\n",
      "  '0.020*\"efficient\" + 0.019*\"preposition\" + 0.018*\"sentiment_analysis\"'),\n",
      " (13,\n",
      "  '0.357*\"statistical\" + 0.076*\"model\" + 0.059*\"phrase_base\" + '\n",
      "  '0.032*\"language\" + 0.021*\"phrase_based\" + 0.019*\"english\" + 0.017*\"code\" + '\n",
      "  '0.016*\"improve\" + 0.015*\"algorithm\" + 0.014*\"segmentation\"'),\n",
      " (14,\n",
      "  '0.183*\"evaluation\" + 0.041*\"quality\" + 0.031*\"terminology\" + '\n",
      "  '0.026*\"language\" + 0.024*\"bleu\" + 0.021*\"linguistic\" + '\n",
      "  '0.020*\"data_selection\" + 0.019*\"metric\" + 0.017*\"human_evaluation\" + '\n",
      "  '0.015*\"analysis\"'),\n",
      " (15,\n",
      "  '0.070*\"word_alignment\" + 0.065*\"quality_estimation\" + 0.059*\"post_edite\" + '\n",
      "  '0.028*\"grammar\" + 0.027*\"neural\" + 0.025*\"correction\" + 0.020*\"fluency\" + '\n",
      "  '0.019*\"memory\" + 0.018*\"effort\" + 0.017*\"cognitive\"'),\n",
      " (16,\n",
      "  '0.048*\"bilingual\" + 0.042*\"base\" + 0.042*\"phrase\" + 0.034*\"unsupervised\" + '\n",
      "  '0.032*\"model\" + 0.026*\"recursive\" + 0.024*\"induction\" + 0.024*\"neural\" + '\n",
      "  '0.022*\"multi\" + 0.018*\"case\"'),\n",
      " (17,\n",
      "  '0.119*\"base\" + 0.104*\"model\" + 0.085*\"statistical\" + 0.055*\"language\" + '\n",
      "  '0.024*\"syntax\" + 0.022*\"structure\" + 0.020*\"english\" + 0.019*\"reordering\" + '\n",
      "  '0.019*\"word\" + 0.018*\"approach\"'),\n",
      " (18,\n",
      "  '0.061*\"feature\" + 0.051*\"neural_network\" + 0.036*\"model\" + 0.030*\"base\" + '\n",
      "  '0.029*\"semantic\" + 0.029*\"representation\" + 0.028*\"test\" + '\n",
      "  '0.028*\"processing\" + 0.025*\"testing\" + 0.025*\"natural_language\"'),\n",
      " (19,\n",
      "  '0.095*\"generation\" + 0.044*\"task\" + 0.034*\"translation\" + 0.029*\"database\" '\n",
      "  '+ 0.025*\"automate\" + 0.019*\"fuzzy\" + 0.019*\"error_analysis\" + '\n",
      "  '0.018*\"distribute\" + 0.018*\"sequence_sequence\" + 0.017*\"vietnamese\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3864835715794842\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 25\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)),corpus]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "print(str(len(corpus_sets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|âââââââ   | 1346/2064 [21:31:49<443:54:20, 2225.71s/it] "
     ]
    }
   ],
   "source": [
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=2064)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, k=k, a=a, b=b)\n",
    "                    \n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results1.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ldamodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-051c35144739>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#LDAvis_prepared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mlda_viz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mlda_viz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ldamodel' is not defined"
     ]
    }
   ],
   "source": [
    "#import pyLDAvis.gensim\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "#LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "#LDAvis_prepared\n",
    "\n",
    "lda_viz = gensimvis.prepare(ldamodel, corpus, dictionary)\n",
    "lda_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import gensim.models.wrappers.ldamallet as ldamallet\n",
    "#from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "import os\n",
    "#from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "os.environ['MALLET_HOME'] = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Equipe-ILC\\\\Automatisation\\\\LdaMallet\\\\mallet-2.0.8\"\n",
    "\n",
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "mallet_path = \"C:\\\\Users\\\\HP\\\\Desktop\\\\Equipe-ILC\\\\Automatisation\\\\LdaMallet\\\\mallet-2.0.8\\\\bin\\\\mallet\" # update this path\n",
    "#ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit=30, start=2, step=2):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    i = 0\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print('iteration : '+str(i)+' when the number of topics is : '+str(num_topics))\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        i+=1\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=30, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Show graph\n",
    "limit=30; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
